# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112QDqAUa_X5_NDLi8sGxI8Q76VL_Eea7
"""

# hla_predictor/features/hla_features.py
import pandas as pd
import numpy as np
import math
from tqdm import tqdm
import warnings
import os
from typing import Union, Optional

class HLAFeatureGenerator:
    """
    Generates HLA features using information content (IC) from peptide-HLA binding data.
    Handles both precomputed features (from training) and on-the-fly computation for new alleles.
    """

    def __init__(self, training_data_paths: Optional[Union[str, list]] = None):
        """
        Initialize feature generator.

        Args:
            training_data_paths: Path(s) to training data CSV file(s) for precomputing features.
                               If None, only on-the-fly computation will be available.
        """
        # Amino acid to index mapping
        self.aa_to_idx = {aa: i for i, aa in enumerate(
            ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I',
             'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']
        )}

        # Initialize feature storage
        self.precomputed_features = None
        self.feature_columns = ['HLA'] + [f"pos_{i}_aa_{j}"
                                         for i in range(1, 10)
                                         for j in range(20)]

        # Load precomputed features if training data provided
        if training_data_paths is not None:
            if isinstance(training_data_paths, str):
                training_data_paths = [training_data_paths]
            self._precompute_features(training_data_paths)

    def _precompute_features(self, data_paths: list):
        """Precompute features for all HLA alleles in training data"""
        warnings.filterwarnings('ignore', category=RuntimeWarning)

        # Load and concatenate all training data
        dfs = [pd.read_csv(path) for path in data_paths]
        alldata = pd.concat(dfs)
        alldata['HLA'] = alldata['HLA'].str.replace(r'\*', '', regex=True)

        # Initialize feature dataframe
        self.precomputed_features = pd.DataFrame(columns=self.feature_columns)

        # Compute features for each unique HLA
        hla_list = alldata['HLA'].unique()
        for hla in tqdm(hla_list, desc="Precomputing HLA features"):
            features = self._compute_single_hla_features(alldata, hla)
            self.precomputed_features.loc[len(self.precomputed_features)] = features

        # Set index and clean up
        self.precomputed_features = self.precomputed_features.set_index('HLA')
        warnings.resetwarnings()

    def _compute_single_hla_features(self, data: pd.DataFrame, hla_allele: str):
        """
        Compute IC-based features for a single HLA allele.

        Args:
            data: DataFrame containing peptide-HLA binding data
            hla_allele: HLA allele to compute features for

        Returns:
            List of features starting with HLA name followed by 180 IC values
        """
        # Filter data for this HLA
        hla_data = data[data['HLA'] == hla_allele]
        binders = hla_data[hla_data['label'] == 1]
        non_binders = hla_data[hla_data['label'] == 0]

        # Filter 9-mers only
        binders = binders[binders['length'] == 9]
        non_binders = non_binders[non_binders['length'] == 9]

        # Initialize probability profiles
        binder_profile = np.full((20, 9), 1e-10)  # Small epsilon to avoid log(0)
        non_binder_profile = np.full((20, 9), 1e-10)

        # Compute probability profiles
        for profile, df in [(binder_profile, binders), (non_binder_profile, non_binders)]:
            for _, row in df.iterrows():
                peptide = row['peptide']
                for pos in range(9):
                    if pos < len(peptide):
                        aa = peptide[pos]
                        if aa in self.aa_to_idx:
                            profile[self.aa_to_idx[aa], pos] += 1/len(df)

        # Compute information content matrix
        ic_matrix = np.zeros((20, 9))
        for i in range(20):
            for j in range(9):
                if binder_profile[i,j] > 0 and non_binder_profile[i,j] > 0:
                    ic_matrix[i,j] = binder_profile[i,j] * math.log(
                        binder_profile[i,j]/non_binder_profile[i,j], 20)

        # Flatten features and add HLA name
        features = [hla_allele] + ic_matrix.flatten().tolist()
        return features

    def get_features(self, hla_allele: str, peptide_data: Optional[pd.DataFrame] = None):
        """
        Get features for an HLA allele.

        Args:
            hla_allele: HLA allele name (e.g., 'HLA-A*02:01')
            peptide_data: Required for new alleles not in training data.
                         DataFrame must contain:
                         - 'peptide': peptide sequences
                         - 'label': binding labels (0/1)
                         - 'length': peptide lengths

        Returns:
            numpy array of 180 features
        """
        # Clean HLA name
        hla_allele = str(hla_allele).replace('*', '')

        # Try precomputed features first
        if self.precomputed_features is not None and hla_allele in self.precomputed_features.index:
            return self.precomputed_features.loc[hla_allele].values.astype(np.float32)

        # Compute on-the-fly for new alleles
        if peptide_data is None:
            raise ValueError(
                f"No precomputed features for {hla_allele} and no peptide data provided. "
                "Please provide peptide binding data for this HLA allele."
            )

        if not all(col in peptide_data.columns for col in ['peptide', 'label', 'length']):
            raise ValueError("peptide_data must contain 'peptide', 'label', and 'length' columns")

        return np.array(
            self._compute_single_hla_features(peptide_data, hla_allele)[1:],  # Exclude HLA name
            dtype=np.float32
        )

    def save_features(self, output_path: str):
        """Save precomputed features to CSV file"""
        if self.precomputed_features is not None:
            self.precomputed_features.reset_index().to_csv(output_path, index=False)
        else:
            raise ValueError("No precomputed features available to save")

    @classmethod
    def from_precomputed(cls, features_path: str):
        """Initialize with precomputed features from CSV"""
        instance = cls(training_data_paths=None)
        instance.precomputed_features = pd.read_csv(features_path).set_index('HLA')
        return instance

##example usage:
# # Initialize with training data
# train_files = ["fold0.csv", "fold1.csv", "fold2.csv"]
# hla_processor = HLAFeatureGenerator(train_files)

# # Get features (will use precomputed if available)
# features = hla_processor.get_features("HLA-A02:01")

# # For new HLA alleles
# new_hla_data = pd.DataFrame({
#     'peptide': ['AEFKEAASL', 'YLLPAIVHI'],
#     'label': [1, 0],
#     'length': [9, 9]
# })
# new_features = hla_processor.get_features("New-HLA", new_hla_data)

# # Save features for future use
# hla_processor.save_features("hla_features.csv")

# # Later load precomputed features
# hla_processor = HLAFeatureGenerator.from_precomputed("hla_features.csv")