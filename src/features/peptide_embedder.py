# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112QDqAUa_X5_NDLi8sGxI8Q76VL_Eea7
"""

# hla_predictor/features/peptide_embedder.py
import torch
import esm
import pandas as pd
import numpy as np
from typing import Union, List, Optional
import logging
import warnings
from tqdm import tqdm

# Set up logging
logger = logging.getLogger(__name__)

class PeptideEmbedder:
    """
    Generates ESM-2 embeddings for peptide sequences with support for multiple model sizes.
    Handles batch processing and automatic device detection.

    Example usage:
        embedder = PeptideEmbedder(model_name='esm2_t6_8M')
        embeddings = embedder.embed_peptides(["ACDEFGHIK", "YKLQPLTFL"])
    """

    # Available ESM-2 models and their dimensions
    MODEL_CONFIG = {
        'esm2_t6_8M': {
            'dim': 320,
            'loader': esm.pretrained.esm2_t6_8M_UR50D
        },
        'esm2_t12_35M': {
            'dim': 480,
            'loader': esm.pretrained.esm2_t12_35M_UR50D
        },
        'esm2_t30_150M': {
            'dim': 640,
            'loader': esm.pretrained.esm2_t30_150M_UR50D
        },
        'esm2_t33_650M': {
            'dim': 1280,
            'loader': esm.pretrained.esm2_t33_650M_UR50D
        },
        'esm2_t36_3B': {
            'dim': 2560,
            'loader': esm.pretrained.esm2_t36_3B_UR50D
        },
        'esm2_t48_15B': {
            'dim': 5120,
            'loader': esm.pretrained.esm2_t48_15B_UR50D
        }
    }

    def __init__(self, model_name: str = 'esm2_t6_8M', device: str = 'auto',
                 batch_size: int = 64, layer: int = 6):
        """
        Initialize the ESM embedder.

        Args:
            model_name: Name of ESM-2 model (default: 'esm2_t6_8M' - 320D)
            device: 'auto', 'cuda', or 'cpu'
            batch_size: Number of sequences to process simultaneously
            layer: Which transformer layer to extract features from (default: 6)
        """
        assert model_name in self.MODEL_CONFIG, \
            f"Invalid model name. Choose from: {list(self.MODEL_CONFIG.keys())}"
        assert layer > 0, "Layer must be a positive integer"

        self.model_name = model_name
        self.batch_size = batch_size
        self.layer = layer

        # Configure device
        self.device = self._get_device(device)
        logger.info(f"Using device: {self.device}")

        # Load model
        self.model, self.alphabet = self._load_model()
        self.batch_converter = self.alphabet.get_batch_converter()
        self.model.eval()  # Set to evaluation mode

    def _get_device(self, device_str: str) -> torch.device:
        """Determine the appropriate torch device"""
        if device_str == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        return torch.device(device_str)

    def _load_model(self):
        """Load the specified ESM model"""
        logger.info(f"Loading {self.model_name} model...")
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return self.MODEL_CONFIG[self.model_name]['loader']()

    @property
    def embedding_dim(self) -> int:
        """Get the dimension of embeddings produced by this model"""
        return self.MODEL_CONFIG[self.model_name]['dim']

    def _validate_peptides(self, peptides: List[str]) -> List[str]:
        """Validate and clean peptide sequences"""
        valid_peptides = []
        for seq in peptides:
            # Convert to uppercase and remove whitespace
            cleaned = str(seq).upper().strip()
            if not cleaned:
                logger.warning("Empty peptide sequence found")
                continue
            if any(c not in self.alphabet.all_toks for c in cleaned):
                logger.warning(f"Invalid characters in peptide: {seq}")
                continue
            valid_peptides.append(cleaned)
        return valid_peptides

    def _process_batch(self, batch: List[str]) -> np.ndarray:
        """Process a single batch of sequences"""
        batch = [("", seq) for seq in batch]
        _, _, batch_tokens = self.batch_converter(batch)
        batch_tokens = batch_tokens.to(self.device)

        with torch.no_grad():
            results = self.model(batch_tokens, repr_layers=[self.layer], return_contacts=False)
            token_representations = results["representations"][self.layer]

            # Mask for sequence positions (excluding special tokens)
            mask = batch_tokens != self.alphabet.padding_idx

            # Compute mean per-sequence representation
            seq_reps = []
            for i, tokens_len in enumerate(mask.sum(1)):
                # Extract sequence representations (excluding CLS and EOS tokens)
                seq_rep = token_representations[i, 1:tokens_len-1].mean(0)
                seq_reps.append(seq_rep.cpu().numpy())

        return np.stack(seq_reps)

    def embed_peptides(self, peptides: Union[List[str], pd.Series, pd.DataFrame],
                      peptide_col: Optional[str] = None) -> pd.DataFrame:
        """
        Generate embeddings for peptide sequences.

        Args:
            peptides: Input peptides as:
                      - List of strings
                      - pandas Series
                      - DataFrame (must specify peptide_col)
            peptide_col: Column name if peptides is a DataFrame

        Returns:
            DataFrame with embeddings (n_sequences Ã— embedding_dim)
        """
        # Convert input to list of sequences
        if isinstance(peptides, pd.DataFrame):
            assert peptide_col is not None, "Must specify peptide_col for DataFrame input"
            seq_list = peptides[peptide_col].astype(str).tolist()
        elif isinstance(peptides, pd.Series):
            seq_list = peptides.astype(str).tolist()
        else:
            seq_list = list(map(str, peptides))

        # Validate and clean sequences
        seq_list = self._validate_peptides(seq_list)
        if not seq_list:
            raise ValueError("No valid peptide sequences provided")

        # Process in batches
        all_embeddings = []
        for i in tqdm(range(0, len(seq_list), self.batch_size),
                      desc=f"Embedding peptides ({self.model_name})"):
            batch = seq_list[i:i+self.batch_size]
            batch_embeddings = self._process_batch(batch)
            all_embeddings.append(batch_embeddings)

        # Combine results
        embeddings = np.concatenate(all_embeddings)
        return pd.DataFrame(embeddings,
                          columns=[f"esm_{i}" for i in range(self.embedding_dim)])

    def save_embeddings(self, peptides: Union[List[str], pd.Series, pd.DataFrame],
                        output_path: str, peptide_col: Optional[str] = None):
        """
        Generate embeddings and save directly to file.

        Args:
            peptides: Input peptides (see embed_peptides())
            output_path: Path to save embeddings (CSV or NPZ)
            peptide_col: Column name if peptides is a DataFrame
        """
        embeddings = self.embed_peptides(peptides, peptide_col)

        if output_path.endswith('.csv'):
            embeddings.to_csv(output_path, index=False)
        elif output_path.endswith('.npy'):
            np.save(output_path, embeddings.values)
        elif output_path.endswith('.npz'):
            np.savez(output_path, embeddings=embeddings.values)
        else:
            raise ValueError("Unsupported file format. Use .csv, .npy, or .npz")

        logger.info(f"Saved embeddings to {output_path}")

##usage example:
# All these work:
# embedder.embed_peptides(["ACDEFGHIK", "YKLQPLTFL"])
# embedder.embed_peptides(df['peptide_sequence'])
# embedder.embed_peptides(df, peptide_col='sequence')
