{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68ad709",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2565c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix, matthews_corrcoef\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU, Concatenate, Multiply, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "# import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import log\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "import csv\n",
    "from urllib.request import urlretrieve\n",
    "import pickle as pickle\n",
    "import os\n",
    "import gzip\n",
    "from collections.abc import MutableMapping\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469c180-ff05-4cc7-93e3-9e1f6c8b3006",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# all 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e03d11-071f-4de5-944b-1c4369a53214",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "944bd4df-fc94-4bae-a449-d706123e15e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define Swish activation function\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load HLA features (drop non-feature columns)\n",
    "hla_df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\final_entropy_column.csv\")\n",
    "hla_df = hla_df.drop(columns=[\"Unnamed: 0\"])  # Correct way to drop a column\n",
    "hla_df = hla_df.set_index(\"HLA\")  # Set HLA column as index\n",
    "\n",
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\train_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings = pickle.load(f)\n",
    "\n",
    "# Load all folds\n",
    "folds = [\n",
    "    pd.read_csv(rf\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold{i}.csv\")\n",
    "    for i in range(5)\n",
    "]\n",
    "# Remove '*' from HLA names\n",
    "for fold in folds:\n",
    "    fold[\"HLA\"] = fold[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "# Assume df1, df2, df3, df4, df5 are your data frames\n",
    "# merged_df = pd.concat([folds[0], folds[1], folds[2], folds[3], folds[4]], axis=0)  # axis=0 for row-wise\n",
    "merged_df = folds[0]  # axis=0 for row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f988ff9a-5e9f-4387-940f-ef9b0b4553cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input shapes\n",
    "input_shape1 = (320,)  # Peptide\n",
    "input_shape2 = (180,)  # HLA\n",
    "\n",
    "# Define model\n",
    "left_input = Input(shape=input_shape1)\n",
    "right_input = Input(shape=input_shape2)\n",
    "\n",
    "encoded_l = Dense(256, activation=swish)(left_input)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "encoded_l = Dense(128, activation=swish)(encoded_l)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "\n",
    "encoded_r = Dense(128, activation=swish)(right_input)\n",
    "encoded_r = Dropout(0.5)(encoded_r)\n",
    "\n",
    "L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "concatenated = Dense(64, activation=swish)(L1_distance)\n",
    "concatenated = Dropout(0.2)(concatenated)\n",
    "concatenated = Dense(32, activation=swish)(concatenated)\n",
    "\n",
    "prediction = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f6f17c5-68b1-42fd-9531-cc6216653971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size = 512  # Fixed batch size\n",
    "initial_lr = 0.001  # Fixed learning rate\n",
    "epochs = 10  # Total epochs\n",
    "\n",
    "# Assume `fold` is your single fold of data\n",
    "# print(\"Training on the single fold...\")\n",
    "\n",
    "# Prepare the data\n",
    "peptides = np.array([peptide_embeddings[p] for p in merged_df[\"peptide\"]])\n",
    "hlas = np.array([hla_df.loc[h].values for h in merged_df[\"HLA\"]])\n",
    "labels = merged_df[\"label\"].values\n",
    "\n",
    "# Training Loop\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "#     # Train the model for one epoch\n",
    "#     history = siamese_net.fit(\n",
    "#         [peptides, hlas], labels,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=1,  # Train for one epoch at a time\n",
    "#         verbose=1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b96d707a-99c9-4194-a6dc-7e687562f211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 22ms/step - AUC: 0.9392 - Precision: 0.8599 - Recall: 0.8764 - accuracy: 0.8665 - loss: 0.3148\n",
      "Epoch 2/50\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9395 - Precision: 0.8606 - Recall: 0.8774 - accuracy: 0.8674 - loss: 0.3139\n",
      "Epoch 3/50\n",
      "\u001b[1m 243/1123\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - AUC: 0.9394 - Precision: 0.8615 - Recall: 0.8785 - accuracy: 0.8682 - loss: 0.3143"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m      2\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Monitor training loss\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Number of epochs with no improvement before stopping\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Restore model to best epoch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m,  \u001b[38;5;66;03m# Minimum change to qualify as an improvement\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeptides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhlas\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\users\\asus\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"loss\",  # Monitor training loss\n",
    "    patience=10,  # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True,  # Restore model to best epoch\n",
    "    min_delta=0.0001,  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "history = siamese_net.fit(\n",
    "        [peptides, hlas], labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50,  \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86ed472-b716-449a-a151-964877f17f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "siamese_net.save(r\"C:\\Users\\asus\\Desktop\\siamese_all_5_fold_new_feature.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25a4cbdd-d22a-4df5-bf4f-de5adbe566fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], 'r', label='train_loss')\n",
    "plt.plot(history.history['accuracy'], 'y', label='train_accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss and accuracy checking')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd5630-3295-4724-a927-11c82de06100",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa33dd3c-0245-462f-90d1-2a4ab6683201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define Swish activation function\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load HLA features (drop non-feature columns)\n",
    "hla_df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv\")\n",
    "hla_df = hla_df.drop(columns=[\"Unnamed: 0\"])  # Correct way to drop a column\n",
    "hla_df = hla_df.set_index(\"HLA\")  # Set HLA column as index\n",
    "\n",
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\train_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings = pickle.load(f)\n",
    "\n",
    "# Load all folds\n",
    "folds = [\n",
    "    pd.read_csv(rf\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold{i}.csv\")\n",
    "    for i in range(5)\n",
    "]\n",
    "# Remove '*' from HLA names\n",
    "for fold in folds:\n",
    "    fold[\"HLA\"] = fold[\"HLA\"].str.replace(\"*\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f7d13-07a9-41ed-ad71-35a68fe8ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input shapes\n",
    "input_shape1 = (320,)  # Peptide\n",
    "input_shape2 = (180,)  # HLA\n",
    "\n",
    "# Define model\n",
    "left_input = Input(shape=input_shape1)\n",
    "right_input = Input(shape=input_shape2)\n",
    "\n",
    "encoded_l = Dense(256, activation=swish)(left_input)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "encoded_l = Dense(128, activation=swish)(encoded_l)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "\n",
    "encoded_r = Dense(128, activation=swish)(right_input)\n",
    "encoded_r = Dropout(0.2)(encoded_r)\n",
    "\n",
    "L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "concatenated = Dense(64, activation=swish)(L1_distance)\n",
    "concatenated = Dropout(0.2)(concatenated)\n",
    "concatenated = Dense(32, activation=swish)(concatenated)\n",
    "\n",
    "prediction = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017c4a5e-c914-4cff-a203-133109011fa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Fold 1/5...\n",
      "\n",
      "\u001b[1m8980/8980\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 9ms/step - AUC: 0.8272 - Precision: 0.7345 - Recall: 0.7914 - accuracy: 0.7518 - loss: 0.5020\n",
      "\u001b[1m5321/5321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 11ms/step - AUC: 0.9044 - Precision: 0.8138 - Recall: 0.8502 - accuracy: 0.8275 - loss: 0.3899\n",
      "\u001b[1m3756/3756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 12ms/step - AUC: 0.9145 - Precision: 0.8266 - Recall: 0.8585 - accuracy: 0.8389 - loss: 0.3699\n",
      "\u001b[1m2903/2903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - AUC: 0.9198 - Precision: 0.8322 - Recall: 0.8622 - accuracy: 0.8439 - loss: 0.3591\n",
      "\u001b[1m2365/2365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 15ms/step - AUC: 0.9235 - Precision: 0.8374 - Recall: 0.8663 - accuracy: 0.8488 - loss: 0.3513\n",
      "\u001b[1m1996/1996\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 16ms/step - AUC: 0.9263 - Precision: 0.8411 - Recall: 0.8690 - accuracy: 0.8522 - loss: 0.3452\n",
      "\u001b[1m1731/1731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 18ms/step - AUC: 0.9280 - Precision: 0.8431 - Recall: 0.8714 - accuracy: 0.8544 - loss: 0.3410\n",
      "\u001b[1m1525/1525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 19ms/step - AUC: 0.9302 - Precision: 0.8464 - Recall: 0.8734 - accuracy: 0.8572 - loss: 0.3364\n",
      "\u001b[1m1362/1362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 20ms/step - AUC: 0.9319 - Precision: 0.8481 - Recall: 0.8747 - accuracy: 0.8588 - loss: 0.3322\n",
      "\u001b[1m1231/1231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - AUC: 0.9337 - Precision: 0.8501 - Recall: 0.8761 - accuracy: 0.8606 - loss: 0.3282\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9347 - Precision: 0.8521 - Recall: 0.8770 - accuracy: 0.8622 - loss: 0.3257\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - AUC: 0.9354 - Precision: 0.8534 - Recall: 0.8785 - accuracy: 0.8636 - loss: 0.3239\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - AUC: 0.9362 - Precision: 0.8558 - Recall: 0.8781 - accuracy: 0.8649 - loss: 0.3221\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9380 - Precision: 0.8584 - Recall: 0.8806 - accuracy: 0.8674 - loss: 0.3175\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - AUC: 0.9382 - Precision: 0.8574 - Recall: 0.8818 - accuracy: 0.8674 - loss: 0.3171\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - AUC: 0.9395 - Precision: 0.8593 - Recall: 0.8826 - accuracy: 0.8688 - loss: 0.3139\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - AUC: 0.9402 - Precision: 0.8604 - Recall: 0.8830 - accuracy: 0.8696 - loss: 0.3120\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - AUC: 0.9409 - Precision: 0.8605 - Recall: 0.8838 - accuracy: 0.8700 - loss: 0.3106\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - AUC: 0.9420 - Precision: 0.8630 - Recall: 0.8837 - accuracy: 0.8715 - loss: 0.3077\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - AUC: 0.9424 - Precision: 0.8633 - Recall: 0.8847 - accuracy: 0.8720 - loss: 0.3064\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9432 - Precision: 0.8649 - Recall: 0.8858 - accuracy: 0.8735 - loss: 0.3045\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9441 - Precision: 0.8641 - Recall: 0.8886 - accuracy: 0.8742 - loss: 0.3019\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9444 - Precision: 0.8660 - Recall: 0.8879 - accuracy: 0.8751 - loss: 0.3012\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9456 - Precision: 0.8685 - Recall: 0.8876 - accuracy: 0.8763 - loss: 0.2982\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9448 - Precision: 0.8666 - Recall: 0.8880 - accuracy: 0.8754 - loss: 0.3001\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9464 - Precision: 0.8694 - Recall: 0.8895 - accuracy: 0.8777 - loss: 0.2959\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9467 - Precision: 0.8681 - Recall: 0.8925 - accuracy: 0.8782 - loss: 0.2952\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9470 - Precision: 0.8683 - Recall: 0.8929 - accuracy: 0.8785 - loss: 0.2942\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - AUC: 0.9475 - Precision: 0.8714 - Recall: 0.8910 - accuracy: 0.8795 - loss: 0.2930\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9479 - Precision: 0.8698 - Recall: 0.8926 - accuracy: 0.8793 - loss: 0.2917\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9482 - Precision: 0.8720 - Recall: 0.8923 - accuracy: 0.8805 - loss: 0.2910\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9488 - Precision: 0.8716 - Recall: 0.8947 - accuracy: 0.8813 - loss: 0.2893\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9493 - Precision: 0.8724 - Recall: 0.8953 - accuracy: 0.8820 - loss: 0.2876\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9497 - Precision: 0.8714 - Recall: 0.8964 - accuracy: 0.8818 - loss: 0.2869\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9498 - Precision: 0.8724 - Recall: 0.8956 - accuracy: 0.8821 - loss: 0.2862\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9505 - Precision: 0.8725 - Recall: 0.8963 - accuracy: 0.8824 - loss: 0.2845\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9514 - Precision: 0.8751 - Recall: 0.8983 - accuracy: 0.8848 - loss: 0.2817\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9508 - Precision: 0.8750 - Recall: 0.8971 - accuracy: 0.8842 - loss: 0.2835\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9516 - Precision: 0.8754 - Recall: 0.8989 - accuracy: 0.8853 - loss: 0.2810\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9519 - Precision: 0.8753 - Recall: 0.8979 - accuracy: 0.8848 - loss: 0.2805\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9522 - Precision: 0.8747 - Recall: 0.8993 - accuracy: 0.8850 - loss: 0.2794\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9529 - Precision: 0.8779 - Recall: 0.8967 - accuracy: 0.8858 - loss: 0.2774\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9524 - Precision: 0.8747 - Recall: 0.9007 - accuracy: 0.8857 - loss: 0.2789\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9531 - Precision: 0.8767 - Recall: 0.8999 - accuracy: 0.8865 - loss: 0.2767\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9539 - Precision: 0.8792 - Recall: 0.8997 - accuracy: 0.8878 - loss: 0.2746\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9540 - Precision: 0.8800 - Recall: 0.8993 - accuracy: 0.8881 - loss: 0.2740\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9545 - Precision: 0.8795 - Recall: 0.9006 - accuracy: 0.8884 - loss: 0.2726\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9548 - Precision: 0.8811 - Recall: 0.8998 - accuracy: 0.8890 - loss: 0.2716\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9551 - Precision: 0.8824 - Recall: 0.8995 - accuracy: 0.8896 - loss: 0.2708\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9553 - Precision: 0.8811 - Recall: 0.9023 - accuracy: 0.8901 - loss: 0.2701\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9554 - Precision: 0.8826 - Recall: 0.9001 - accuracy: 0.8900 - loss: 0.2700\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9556 - Precision: 0.8823 - Recall: 0.9022 - accuracy: 0.8907 - loss: 0.2693\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9561 - Precision: 0.8827 - Recall: 0.9023 - accuracy: 0.8910 - loss: 0.2678\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9561 - Precision: 0.8826 - Recall: 0.9018 - accuracy: 0.8907 - loss: 0.2679\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9566 - Precision: 0.8839 - Recall: 0.9028 - accuracy: 0.8919 - loss: 0.2660\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9571 - Precision: 0.8812 - Recall: 0.9048 - accuracy: 0.8912 - loss: 0.2647\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9573 - Precision: 0.8849 - Recall: 0.9021 - accuracy: 0.8922 - loss: 0.2641\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9566 - Precision: 0.8833 - Recall: 0.9031 - accuracy: 0.8917 - loss: 0.2661\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9578 - Precision: 0.8854 - Recall: 0.9027 - accuracy: 0.8928 - loss: 0.2625\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9582 - Precision: 0.8873 - Recall: 0.9038 - accuracy: 0.8943 - loss: 0.2612\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9584 - Precision: 0.8867 - Recall: 0.9042 - accuracy: 0.8942 - loss: 0.2608\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9587 - Precision: 0.8872 - Recall: 0.9052 - accuracy: 0.8949 - loss: 0.2595\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9594 - Precision: 0.8888 - Recall: 0.9039 - accuracy: 0.8952 - loss: 0.2575\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9592 - Precision: 0.8875 - Recall: 0.9061 - accuracy: 0.8954 - loss: 0.2581\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9598 - Precision: 0.8905 - Recall: 0.9043 - accuracy: 0.8964 - loss: 0.2563\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9600 - Precision: 0.8908 - Recall: 0.9053 - accuracy: 0.8970 - loss: 0.2557\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9603 - Precision: 0.8905 - Recall: 0.9054 - accuracy: 0.8968 - loss: 0.2548\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9603 - Precision: 0.8899 - Recall: 0.9063 - accuracy: 0.8969 - loss: 0.2546\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9608 - Precision: 0.8910 - Recall: 0.9054 - accuracy: 0.8972 - loss: 0.2531\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9612 - Precision: 0.8926 - Recall: 0.9053 - accuracy: 0.8980 - loss: 0.2518\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9613 - Precision: 0.8912 - Recall: 0.9070 - accuracy: 0.8980 - loss: 0.2511\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9613 - Precision: 0.8921 - Recall: 0.9063 - accuracy: 0.8982 - loss: 0.2514\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9619 - Precision: 0.8919 - Recall: 0.9088 - accuracy: 0.8991 - loss: 0.2493\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9622 - Precision: 0.8949 - Recall: 0.9074 - accuracy: 0.9002 - loss: 0.2480\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9620 - Precision: 0.8937 - Recall: 0.9070 - accuracy: 0.8994 - loss: 0.2490\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9622 - Precision: 0.8935 - Recall: 0.9085 - accuracy: 0.8999 - loss: 0.2480\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9629 - Precision: 0.8958 - Recall: 0.9078 - accuracy: 0.9009 - loss: 0.2460\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9631 - Precision: 0.8960 - Recall: 0.9092 - accuracy: 0.9017 - loss: 0.2450\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9631 - Precision: 0.8953 - Recall: 0.9098 - accuracy: 0.9015 - loss: 0.2453\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9633 - Precision: 0.8965 - Recall: 0.9087 - accuracy: 0.9017 - loss: 0.2445\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9637 - Precision: 0.8957 - Recall: 0.9103 - accuracy: 0.9020 - loss: 0.2434\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9640 - Precision: 0.8969 - Recall: 0.9099 - accuracy: 0.9024 - loss: 0.2420\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9640 - Precision: 0.8987 - Recall: 0.9094 - accuracy: 0.9032 - loss: 0.2418\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9646 - Precision: 0.8976 - Recall: 0.9103 - accuracy: 0.9030 - loss: 0.2403\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9645 - Precision: 0.8981 - Recall: 0.9105 - accuracy: 0.9034 - loss: 0.2404\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9649 - Precision: 0.8985 - Recall: 0.9111 - accuracy: 0.9039 - loss: 0.2391\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9648 - Precision: 0.8985 - Recall: 0.9108 - accuracy: 0.9038 - loss: 0.2394\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9652 - Precision: 0.8995 - Recall: 0.9115 - accuracy: 0.9047 - loss: 0.2381\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9653 - Precision: 0.8999 - Recall: 0.9105 - accuracy: 0.9045 - loss: 0.2375\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9655 - Precision: 0.9001 - Recall: 0.9112 - accuracy: 0.9049 - loss: 0.2369\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9658 - Precision: 0.9011 - Recall: 0.9119 - accuracy: 0.9057 - loss: 0.2358\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9659 - Precision: 0.9004 - Recall: 0.9117 - accuracy: 0.9053 - loss: 0.2356\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9661 - Precision: 0.9002 - Recall: 0.9115 - accuracy: 0.9050 - loss: 0.2352\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9663 - Precision: 0.9019 - Recall: 0.9115 - accuracy: 0.9060 - loss: 0.2341\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9662 - Precision: 0.9006 - Recall: 0.9131 - accuracy: 0.9060 - loss: 0.2346\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9667 - Precision: 0.9027 - Recall: 0.9128 - accuracy: 0.9071 - loss: 0.2326\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9667 - Precision: 0.9028 - Recall: 0.9126 - accuracy: 0.9070 - loss: 0.2327\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9667 - Precision: 0.9026 - Recall: 0.9125 - accuracy: 0.9069 - loss: 0.2329\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9666 - Precision: 0.9020 - Recall: 0.9124 - accuracy: 0.9065 - loss: 0.2331\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9670 - Precision: 0.9026 - Recall: 0.9128 - accuracy: 0.9070 - loss: 0.2319\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - AUC: 0.9671 - Precision: 0.9022 - Recall: 0.9138 - accuracy: 0.9072 - loss: 0.2312\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9671 - Precision: 0.9017 - Recall: 0.9147 - accuracy: 0.9073 - loss: 0.2312\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9674 - Precision: 0.9031 - Recall: 0.9144 - accuracy: 0.9080 - loss: 0.2302\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9675 - Precision: 0.9038 - Recall: 0.9137 - accuracy: 0.9081 - loss: 0.2299\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9673 - Precision: 0.9034 - Recall: 0.9131 - accuracy: 0.9076 - loss: 0.2305\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9675 - Precision: 0.9031 - Recall: 0.9140 - accuracy: 0.9078 - loss: 0.2301\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9676 - Precision: 0.9039 - Recall: 0.9141 - accuracy: 0.9083 - loss: 0.2294\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9678 - Precision: 0.9045 - Recall: 0.9145 - accuracy: 0.9088 - loss: 0.2287\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9679 - Precision: 0.9042 - Recall: 0.9148 - accuracy: 0.9088 - loss: 0.2283\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9678 - Precision: 0.9035 - Recall: 0.9147 - accuracy: 0.9083 - loss: 0.2289\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9679 - Precision: 0.9034 - Recall: 0.9150 - accuracy: 0.9084 - loss: 0.2285\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - AUC: 0.9681 - Precision: 0.9046 - Recall: 0.9144 - accuracy: 0.9089 - loss: 0.2276\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9680 - Precision: 0.9040 - Recall: 0.9151 - accuracy: 0.9088 - loss: 0.2282\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - AUC: 0.9681 - Precision: 0.9050 - Recall: 0.9149 - accuracy: 0.9093 - loss: 0.2277\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9681 - Precision: 0.9039 - Recall: 0.9154 - accuracy: 0.9089 - loss: 0.2276\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9681 - Precision: 0.9041 - Recall: 0.9153 - accuracy: 0.9089 - loss: 0.2277\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9682 - Precision: 0.9056 - Recall: 0.9153 - accuracy: 0.9098 - loss: 0.2270\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9683 - Precision: 0.9040 - Recall: 0.9154 - accuracy: 0.9089 - loss: 0.2272\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9682 - Precision: 0.9045 - Recall: 0.9151 - accuracy: 0.9091 - loss: 0.2274\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9682 - Precision: 0.9047 - Recall: 0.9148 - accuracy: 0.9090 - loss: 0.2274\n",
      "\n",
      "Training on Fold 2/5...\n",
      "\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9627 - Precision: 0.8939 - Recall: 0.9078 - accuracy: 0.9002 - loss: 0.2468\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9634 - Precision: 0.8960 - Recall: 0.9080 - accuracy: 0.9014 - loss: 0.2441\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9639 - Precision: 0.8960 - Recall: 0.9094 - accuracy: 0.9020 - loss: 0.2424\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9640 - Precision: 0.8964 - Recall: 0.9103 - accuracy: 0.9027 - loss: 0.2421\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9643 - Precision: 0.8981 - Recall: 0.9087 - accuracy: 0.9029 - loss: 0.2410\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - AUC: 0.9651 - Precision: 0.8990 - Recall: 0.9106 - accuracy: 0.9043 - loss: 0.2384\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9653 - Precision: 0.8987 - Recall: 0.9112 - accuracy: 0.9044 - loss: 0.2378\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - AUC: 0.9655 - Precision: 0.8996 - Recall: 0.9109 - accuracy: 0.9047 - loss: 0.2370\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9653 - Precision: 0.9007 - Recall: 0.9108 - accuracy: 0.9053 - loss: 0.2375\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - AUC: 0.9655 - Precision: 0.9003 - Recall: 0.9101 - accuracy: 0.9047 - loss: 0.2369\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9654 - Precision: 0.9004 - Recall: 0.9105 - accuracy: 0.9050 - loss: 0.2371\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9657 - Precision: 0.9013 - Recall: 0.9109 - accuracy: 0.9057 - loss: 0.2359\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9658 - Precision: 0.9018 - Recall: 0.9111 - accuracy: 0.9061 - loss: 0.2358\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9662 - Precision: 0.9010 - Recall: 0.9121 - accuracy: 0.9061 - loss: 0.2344\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9659 - Precision: 0.9008 - Recall: 0.9121 - accuracy: 0.9059 - loss: 0.2354\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9660 - Precision: 0.9007 - Recall: 0.9114 - accuracy: 0.9056 - loss: 0.2349\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - AUC: 0.9665 - Precision: 0.9026 - Recall: 0.9114 - accuracy: 0.9066 - loss: 0.2335\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - AUC: 0.9664 - Precision: 0.9017 - Recall: 0.9121 - accuracy: 0.9064 - loss: 0.2338\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - AUC: 0.9661 - Precision: 0.9011 - Recall: 0.9126 - accuracy: 0.9063 - loss: 0.2346\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9666 - Precision: 0.9025 - Recall: 0.9123 - accuracy: 0.9070 - loss: 0.2330\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9669 - Precision: 0.9024 - Recall: 0.9129 - accuracy: 0.9072 - loss: 0.2319\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9670 - Precision: 0.9029 - Recall: 0.9131 - accuracy: 0.9076 - loss: 0.2313\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9665 - Precision: 0.9016 - Recall: 0.9124 - accuracy: 0.9065 - loss: 0.2335\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - AUC: 0.9669 - Precision: 0.9024 - Recall: 0.9130 - accuracy: 0.9072 - loss: 0.2319\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9667 - Precision: 0.9017 - Recall: 0.9120 - accuracy: 0.9064 - loss: 0.2328\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9670 - Precision: 0.9032 - Recall: 0.9133 - accuracy: 0.9078 - loss: 0.2314\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9669 - Precision: 0.9022 - Recall: 0.9130 - accuracy: 0.9071 - loss: 0.2320\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9672 - Precision: 0.9027 - Recall: 0.9132 - accuracy: 0.9075 - loss: 0.2307\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9668 - Precision: 0.9021 - Recall: 0.9129 - accuracy: 0.9070 - loss: 0.2321\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9668 - Precision: 0.9022 - Recall: 0.9128 - accuracy: 0.9070 - loss: 0.2324\n",
      "\n",
      "Training on Fold 3/5...\n",
      "\n",
      "\u001b[1m8980/8980\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 6ms/step - AUC: 0.9582 - Precision: 0.8879 - Recall: 0.9048 - accuracy: 0.8952 - loss: 0.2606\n",
      "\u001b[1m5321/5321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 7ms/step - AUC: 0.9589 - Precision: 0.8890 - Recall: 0.9056 - accuracy: 0.8963 - loss: 0.2583\n",
      "\u001b[1m3756/3756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - AUC: 0.9583 - Precision: 0.8880 - Recall: 0.9044 - accuracy: 0.8952 - loss: 0.2602\n",
      "\u001b[1m2903/2903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - AUC: 0.9579 - Precision: 0.8852 - Recall: 0.9058 - accuracy: 0.8941 - loss: 0.2617\n",
      "\u001b[1m2365/2365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - AUC: 0.9563 - Precision: 0.8813 - Recall: 0.9051 - accuracy: 0.8916 - loss: 0.2664\n",
      "\u001b[1m1996/1996\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - AUC: 0.9552 - Precision: 0.8814 - Recall: 0.9020 - accuracy: 0.8903 - loss: 0.2699\n",
      "\u001b[1m1731/1731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - AUC: 0.9555 - Precision: 0.8813 - Recall: 0.9032 - accuracy: 0.8908 - loss: 0.2692\n",
      "\u001b[1m1525/1525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - AUC: 0.9549 - Precision: 0.8821 - Recall: 0.9009 - accuracy: 0.8902 - loss: 0.2708\n",
      "\u001b[1m1362/1362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - AUC: 0.9545 - Precision: 0.8804 - Recall: 0.9011 - accuracy: 0.8893 - loss: 0.2721\n",
      "\u001b[1m1231/1231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - AUC: 0.9532 - Precision: 0.8779 - Recall: 0.8997 - accuracy: 0.8873 - loss: 0.2761\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - AUC: 0.9522 - Precision: 0.8766 - Recall: 0.8987 - accuracy: 0.8861 - loss: 0.2792\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - AUC: 0.9525 - Precision: 0.8761 - Recall: 0.9000 - accuracy: 0.8863 - loss: 0.2783\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - AUC: 0.9499 - Precision: 0.8706 - Recall: 0.8974 - accuracy: 0.8820 - loss: 0.2858\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - AUC: 0.9510 - Precision: 0.8736 - Recall: 0.8983 - accuracy: 0.8841 - loss: 0.2827\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - AUC: 0.9516 - Precision: 0.8776 - Recall: 0.8951 - accuracy: 0.8851 - loss: 0.2808\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9516 - Precision: 0.8763 - Recall: 0.8971 - accuracy: 0.8852 - loss: 0.2806\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - AUC: 0.9514 - Precision: 0.8764 - Recall: 0.8966 - accuracy: 0.8850 - loss: 0.2815\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9519 - Precision: 0.8776 - Recall: 0.8965 - accuracy: 0.8857 - loss: 0.2800\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9523 - Precision: 0.8784 - Recall: 0.8965 - accuracy: 0.8862 - loss: 0.2791\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9521 - Precision: 0.8760 - Recall: 0.8988 - accuracy: 0.8857 - loss: 0.2794\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - AUC: 0.9514 - Precision: 0.8753 - Recall: 0.8962 - accuracy: 0.8842 - loss: 0.2816\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9519 - Precision: 0.8760 - Recall: 0.8969 - accuracy: 0.8849 - loss: 0.2804\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - AUC: 0.9520 - Precision: 0.8765 - Recall: 0.8973 - accuracy: 0.8854 - loss: 0.2797\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - AUC: 0.9527 - Precision: 0.8765 - Recall: 0.8998 - accuracy: 0.8864 - loss: 0.2776\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9514 - Precision: 0.8754 - Recall: 0.8982 - accuracy: 0.8851 - loss: 0.2813\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9516 - Precision: 0.8743 - Recall: 0.8996 - accuracy: 0.8851 - loss: 0.2806\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9516 - Precision: 0.8758 - Recall: 0.8984 - accuracy: 0.8855 - loss: 0.2810\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9520 - Precision: 0.8770 - Recall: 0.8966 - accuracy: 0.8854 - loss: 0.2796\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9521 - Precision: 0.8763 - Recall: 0.8984 - accuracy: 0.8858 - loss: 0.2793\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9523 - Precision: 0.8766 - Recall: 0.8983 - accuracy: 0.8859 - loss: 0.2789\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9524 - Precision: 0.8779 - Recall: 0.8967 - accuracy: 0.8859 - loss: 0.2785\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9523 - Precision: 0.8764 - Recall: 0.8986 - accuracy: 0.8859 - loss: 0.2788\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - AUC: 0.9525 - Precision: 0.8796 - Recall: 0.8962 - accuracy: 0.8868 - loss: 0.2784\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9534 - Precision: 0.8785 - Recall: 0.8996 - accuracy: 0.8875 - loss: 0.2756\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9530 - Precision: 0.8780 - Recall: 0.8991 - accuracy: 0.8871 - loss: 0.2768\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9532 - Precision: 0.8781 - Recall: 0.9004 - accuracy: 0.8877 - loss: 0.2760\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9521 - Precision: 0.8736 - Recall: 0.9015 - accuracy: 0.8855 - loss: 0.2792\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9528 - Precision: 0.8767 - Recall: 0.8978 - accuracy: 0.8857 - loss: 0.2776\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9533 - Precision: 0.8787 - Recall: 0.8989 - accuracy: 0.8874 - loss: 0.2760\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9531 - Precision: 0.8778 - Recall: 0.9000 - accuracy: 0.8874 - loss: 0.2764\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9537 - Precision: 0.8787 - Recall: 0.9000 - accuracy: 0.8879 - loss: 0.2747\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9541 - Precision: 0.8792 - Recall: 0.8998 - accuracy: 0.8880 - loss: 0.2736\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9539 - Precision: 0.8792 - Recall: 0.9003 - accuracy: 0.8883 - loss: 0.2738\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9543 - Precision: 0.8815 - Recall: 0.8986 - accuracy: 0.8889 - loss: 0.2727\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9544 - Precision: 0.8810 - Recall: 0.8991 - accuracy: 0.8888 - loss: 0.2725\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9548 - Precision: 0.8797 - Recall: 0.9023 - accuracy: 0.8894 - loss: 0.2714\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9554 - Precision: 0.8820 - Recall: 0.9018 - accuracy: 0.8905 - loss: 0.2697\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9554 - Precision: 0.8826 - Recall: 0.9003 - accuracy: 0.8902 - loss: 0.2697\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9555 - Precision: 0.8820 - Recall: 0.9021 - accuracy: 0.8907 - loss: 0.2693\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9557 - Precision: 0.8835 - Recall: 0.9005 - accuracy: 0.8909 - loss: 0.2684\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9557 - Precision: 0.8839 - Recall: 0.8999 - accuracy: 0.8908 - loss: 0.2684\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9561 - Precision: 0.8844 - Recall: 0.9018 - accuracy: 0.8919 - loss: 0.2672\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9556 - Precision: 0.8820 - Recall: 0.9024 - accuracy: 0.8908 - loss: 0.2689\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9551 - Precision: 0.8820 - Recall: 0.9001 - accuracy: 0.8898 - loss: 0.2705\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9553 - Precision: 0.8816 - Recall: 0.9009 - accuracy: 0.8899 - loss: 0.2699\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9558 - Precision: 0.8829 - Recall: 0.9011 - accuracy: 0.8907 - loss: 0.2685\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9555 - Precision: 0.8844 - Recall: 0.8989 - accuracy: 0.8907 - loss: 0.2694\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9555 - Precision: 0.8830 - Recall: 0.9011 - accuracy: 0.8908 - loss: 0.2692\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9556 - Precision: 0.8825 - Recall: 0.9016 - accuracy: 0.8908 - loss: 0.2688\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9562 - Precision: 0.8821 - Recall: 0.9031 - accuracy: 0.8912 - loss: 0.2670\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9556 - Precision: 0.8830 - Recall: 0.8998 - accuracy: 0.8902 - loss: 0.2688\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9559 - Precision: 0.8827 - Recall: 0.9011 - accuracy: 0.8907 - loss: 0.2679\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9563 - Precision: 0.8829 - Recall: 0.9011 - accuracy: 0.8907 - loss: 0.2670\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9565 - Precision: 0.8839 - Recall: 0.9024 - accuracy: 0.8919 - loss: 0.2661\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9568 - Precision: 0.8853 - Recall: 0.9020 - accuracy: 0.8925 - loss: 0.2649\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9565 - Precision: 0.8852 - Recall: 0.9010 - accuracy: 0.8921 - loss: 0.2661\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9568 - Precision: 0.8843 - Recall: 0.9029 - accuracy: 0.8924 - loss: 0.2652\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9573 - Precision: 0.8847 - Recall: 0.9034 - accuracy: 0.8928 - loss: 0.2635\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9572 - Precision: 0.8852 - Recall: 0.9027 - accuracy: 0.8928 - loss: 0.2638\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9579 - Precision: 0.8857 - Recall: 0.9047 - accuracy: 0.8939 - loss: 0.2619\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9578 - Precision: 0.8867 - Recall: 0.9028 - accuracy: 0.8937 - loss: 0.2621\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - AUC: 0.9581 - Precision: 0.8871 - Recall: 0.9036 - accuracy: 0.8943 - loss: 0.2611\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9581 - Precision: 0.8865 - Recall: 0.9034 - accuracy: 0.8939 - loss: 0.2610\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9583 - Precision: 0.8874 - Recall: 0.9032 - accuracy: 0.8943 - loss: 0.2604\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9581 - Precision: 0.8889 - Recall: 0.9022 - accuracy: 0.8947 - loss: 0.2609\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9589 - Precision: 0.8900 - Recall: 0.9035 - accuracy: 0.8959 - loss: 0.2584\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9589 - Precision: 0.8889 - Recall: 0.9039 - accuracy: 0.8955 - loss: 0.2582\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9591 - Precision: 0.8892 - Recall: 0.9039 - accuracy: 0.8956 - loss: 0.2576\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9592 - Precision: 0.8882 - Recall: 0.9046 - accuracy: 0.8953 - loss: 0.2576\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9593 - Precision: 0.8890 - Recall: 0.9051 - accuracy: 0.8960 - loss: 0.2572\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9593 - Precision: 0.8887 - Recall: 0.9048 - accuracy: 0.8957 - loss: 0.2572\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9598 - Precision: 0.8899 - Recall: 0.9059 - accuracy: 0.8969 - loss: 0.2556\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9596 - Precision: 0.8903 - Recall: 0.9051 - accuracy: 0.8968 - loss: 0.2560\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9601 - Precision: 0.8899 - Recall: 0.9066 - accuracy: 0.8972 - loss: 0.2544\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9600 - Precision: 0.8912 - Recall: 0.9053 - accuracy: 0.8973 - loss: 0.2548\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9602 - Precision: 0.8916 - Recall: 0.9036 - accuracy: 0.8968 - loss: 0.2545\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9606 - Precision: 0.8919 - Recall: 0.9053 - accuracy: 0.8978 - loss: 0.2532\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9606 - Precision: 0.8910 - Recall: 0.9058 - accuracy: 0.8975 - loss: 0.2530\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9607 - Precision: 0.8923 - Recall: 0.9059 - accuracy: 0.8983 - loss: 0.2527\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9605 - Precision: 0.8924 - Recall: 0.9048 - accuracy: 0.8978 - loss: 0.2533\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9610 - Precision: 0.8917 - Recall: 0.9058 - accuracy: 0.8979 - loss: 0.2517\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9608 - Precision: 0.8923 - Recall: 0.9055 - accuracy: 0.8981 - loss: 0.2522\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9609 - Precision: 0.8924 - Recall: 0.9056 - accuracy: 0.8982 - loss: 0.2520\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9610 - Precision: 0.8929 - Recall: 0.9062 - accuracy: 0.8987 - loss: 0.2515\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9615 - Precision: 0.8930 - Recall: 0.9072 - accuracy: 0.8993 - loss: 0.2501\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9616 - Precision: 0.8929 - Recall: 0.9069 - accuracy: 0.8990 - loss: 0.2498\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9616 - Precision: 0.8928 - Recall: 0.9062 - accuracy: 0.8987 - loss: 0.2498\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9613 - Precision: 0.8941 - Recall: 0.9056 - accuracy: 0.8991 - loss: 0.2505\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9617 - Precision: 0.8935 - Recall: 0.9062 - accuracy: 0.8990 - loss: 0.2495\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9621 - Precision: 0.8944 - Recall: 0.9062 - accuracy: 0.8996 - loss: 0.2483\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9622 - Precision: 0.8944 - Recall: 0.9071 - accuracy: 0.9000 - loss: 0.2477\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9618 - Precision: 0.8933 - Recall: 0.9076 - accuracy: 0.8995 - loss: 0.2491\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9619 - Precision: 0.8941 - Recall: 0.9065 - accuracy: 0.8995 - loss: 0.2486\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9620 - Precision: 0.8938 - Recall: 0.9076 - accuracy: 0.8999 - loss: 0.2483\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9621 - Precision: 0.8938 - Recall: 0.9070 - accuracy: 0.8996 - loss: 0.2482\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9621 - Precision: 0.8941 - Recall: 0.9077 - accuracy: 0.9001 - loss: 0.2479\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9623 - Precision: 0.8950 - Recall: 0.9071 - accuracy: 0.9003 - loss: 0.2474\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9618 - Precision: 0.8940 - Recall: 0.9068 - accuracy: 0.8996 - loss: 0.2490\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9621 - Precision: 0.8943 - Recall: 0.9085 - accuracy: 0.9005 - loss: 0.2478\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9624 - Precision: 0.8942 - Recall: 0.9078 - accuracy: 0.9002 - loss: 0.2473\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9622 - Precision: 0.8938 - Recall: 0.9078 - accuracy: 0.8999 - loss: 0.2478\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9625 - Precision: 0.8956 - Recall: 0.9070 - accuracy: 0.9006 - loss: 0.2467\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9624 - Precision: 0.8945 - Recall: 0.9078 - accuracy: 0.9004 - loss: 0.2473\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9627 - Precision: 0.8959 - Recall: 0.9070 - accuracy: 0.9008 - loss: 0.2461\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9623 - Precision: 0.8945 - Recall: 0.9069 - accuracy: 0.8999 - loss: 0.2474\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9626 - Precision: 0.8948 - Recall: 0.9082 - accuracy: 0.9007 - loss: 0.2464\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9627 - Precision: 0.8949 - Recall: 0.9080 - accuracy: 0.9007 - loss: 0.2462\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9625 - Precision: 0.8954 - Recall: 0.9071 - accuracy: 0.9006 - loss: 0.2469\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9627 - Precision: 0.8950 - Recall: 0.9077 - accuracy: 0.9006 - loss: 0.2462\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9631 - Precision: 0.8953 - Recall: 0.9088 - accuracy: 0.9013 - loss: 0.2449\n",
      "\n",
      "Training on Fold 4/5...\n",
      "\n",
      "\u001b[1m8980/8980\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 8ms/step - AUC: 0.9554 - Precision: 0.8821 - Recall: 0.9022 - accuracy: 0.8908 - loss: 0.2708\n",
      "\u001b[1m5321/5321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 9ms/step - AUC: 0.9558 - Precision: 0.8836 - Recall: 0.9019 - accuracy: 0.8916 - loss: 0.2685\n",
      "\u001b[1m3756/3756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - AUC: 0.9549 - Precision: 0.8804 - Recall: 0.9026 - accuracy: 0.8900 - loss: 0.2706\n",
      "\u001b[1m2903/2903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 12ms/step - AUC: 0.9547 - Precision: 0.8829 - Recall: 0.8978 - accuracy: 0.8894 - loss: 0.2715\n",
      "\u001b[1m2365/2365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - AUC: 0.9545 - Precision: 0.8816 - Recall: 0.8998 - accuracy: 0.8895 - loss: 0.2726\n",
      "\u001b[1m1996/1996\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 14ms/step - AUC: 0.9543 - Precision: 0.8795 - Recall: 0.8995 - accuracy: 0.8882 - loss: 0.2729\n",
      "\u001b[1m1731/1731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 15ms/step - AUC: 0.9539 - Precision: 0.8788 - Recall: 0.9003 - accuracy: 0.8881 - loss: 0.2740\n",
      "\u001b[1m1525/1525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 15ms/step - AUC: 0.9529 - Precision: 0.8773 - Recall: 0.9001 - accuracy: 0.8872 - loss: 0.2769\n",
      "\u001b[1m1362/1362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 17ms/step - AUC: 0.9516 - Precision: 0.8755 - Recall: 0.8960 - accuracy: 0.8844 - loss: 0.2809\n",
      "\u001b[1m1231/1231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - AUC: 0.9516 - Precision: 0.8779 - Recall: 0.8957 - accuracy: 0.8856 - loss: 0.2807\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9514 - Precision: 0.8765 - Recall: 0.8951 - accuracy: 0.8846 - loss: 0.2814\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9515 - Precision: 0.8754 - Recall: 0.8973 - accuracy: 0.8849 - loss: 0.2813\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9513 - Precision: 0.8753 - Recall: 0.8977 - accuracy: 0.8850 - loss: 0.2815\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9484 - Precision: 0.8715 - Recall: 0.8942 - accuracy: 0.8812 - loss: 0.2900\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9509 - Precision: 0.8736 - Recall: 0.8966 - accuracy: 0.8835 - loss: 0.2830\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9512 - Precision: 0.8755 - Recall: 0.8975 - accuracy: 0.8850 - loss: 0.2819\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9514 - Precision: 0.8750 - Recall: 0.8963 - accuracy: 0.8842 - loss: 0.2818\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9509 - Precision: 0.8722 - Recall: 0.8990 - accuracy: 0.8837 - loss: 0.2829\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9506 - Precision: 0.8740 - Recall: 0.8974 - accuracy: 0.8841 - loss: 0.2836\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9508 - Precision: 0.8740 - Recall: 0.8960 - accuracy: 0.8834 - loss: 0.2834\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9508 - Precision: 0.8755 - Recall: 0.8958 - accuracy: 0.8843 - loss: 0.2831\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9516 - Precision: 0.8770 - Recall: 0.8954 - accuracy: 0.8849 - loss: 0.2810\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9513 - Precision: 0.8774 - Recall: 0.8951 - accuracy: 0.8851 - loss: 0.2815\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9513 - Precision: 0.8763 - Recall: 0.8956 - accuracy: 0.8846 - loss: 0.2817\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9521 - Precision: 0.8767 - Recall: 0.8966 - accuracy: 0.8853 - loss: 0.2795\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9525 - Precision: 0.8780 - Recall: 0.8975 - accuracy: 0.8865 - loss: 0.2783\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9520 - Precision: 0.8777 - Recall: 0.8961 - accuracy: 0.8856 - loss: 0.2797\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9526 - Precision: 0.8781 - Recall: 0.8963 - accuracy: 0.8860 - loss: 0.2784\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9523 - Precision: 0.8783 - Recall: 0.8953 - accuracy: 0.8857 - loss: 0.2788\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9527 - Precision: 0.8795 - Recall: 0.8971 - accuracy: 0.8872 - loss: 0.2774\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9530 - Precision: 0.8796 - Recall: 0.8962 - accuracy: 0.8868 - loss: 0.2769\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9527 - Precision: 0.8786 - Recall: 0.8972 - accuracy: 0.8867 - loss: 0.2778\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9525 - Precision: 0.8788 - Recall: 0.8970 - accuracy: 0.8867 - loss: 0.2784\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9521 - Precision: 0.8766 - Recall: 0.8982 - accuracy: 0.8859 - loss: 0.2792\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9524 - Precision: 0.8781 - Recall: 0.8973 - accuracy: 0.8864 - loss: 0.2784\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9523 - Precision: 0.8782 - Recall: 0.8967 - accuracy: 0.8862 - loss: 0.2786\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9527 - Precision: 0.8780 - Recall: 0.8975 - accuracy: 0.8865 - loss: 0.2774\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9529 - Precision: 0.8787 - Recall: 0.8980 - accuracy: 0.8870 - loss: 0.2772\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9533 - Precision: 0.8800 - Recall: 0.8978 - accuracy: 0.8877 - loss: 0.2756\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9540 - Precision: 0.8809 - Recall: 0.8986 - accuracy: 0.8886 - loss: 0.2736\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9539 - Precision: 0.8800 - Recall: 0.8991 - accuracy: 0.8883 - loss: 0.2739\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9542 - Precision: 0.8791 - Recall: 0.9004 - accuracy: 0.8884 - loss: 0.2732\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9545 - Precision: 0.8812 - Recall: 0.8988 - accuracy: 0.8889 - loss: 0.2724\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9545 - Precision: 0.8819 - Recall: 0.8984 - accuracy: 0.8891 - loss: 0.2724\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9543 - Precision: 0.8817 - Recall: 0.8983 - accuracy: 0.8890 - loss: 0.2727\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9539 - Precision: 0.8802 - Recall: 0.9006 - accuracy: 0.8891 - loss: 0.2738\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9543 - Precision: 0.8805 - Recall: 0.8993 - accuracy: 0.8887 - loss: 0.2728\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9548 - Precision: 0.8809 - Recall: 0.9015 - accuracy: 0.8899 - loss: 0.2710\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9539 - Precision: 0.8806 - Recall: 0.8982 - accuracy: 0.8882 - loss: 0.2745\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9550 - Precision: 0.8827 - Recall: 0.8981 - accuracy: 0.8895 - loss: 0.2705\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9550 - Precision: 0.8835 - Recall: 0.8983 - accuracy: 0.8900 - loss: 0.2705\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9554 - Precision: 0.8847 - Recall: 0.8977 - accuracy: 0.8904 - loss: 0.2694\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9554 - Precision: 0.8841 - Recall: 0.8995 - accuracy: 0.8908 - loss: 0.2694\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9549 - Precision: 0.8818 - Recall: 0.8988 - accuracy: 0.8892 - loss: 0.2710\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9561 - Precision: 0.8834 - Recall: 0.9012 - accuracy: 0.8912 - loss: 0.2673\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9557 - Precision: 0.8812 - Recall: 0.9020 - accuracy: 0.8902 - loss: 0.2686\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9565 - Precision: 0.8836 - Recall: 0.9029 - accuracy: 0.8920 - loss: 0.2657\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - AUC: 0.9564 - Precision: 0.8831 - Recall: 0.9027 - accuracy: 0.8917 - loss: 0.2662\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9565 - Precision: 0.8844 - Recall: 0.9012 - accuracy: 0.8918 - loss: 0.2659\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9567 - Precision: 0.8854 - Recall: 0.9007 - accuracy: 0.8921 - loss: 0.2652\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9569 - Precision: 0.8843 - Recall: 0.9024 - accuracy: 0.8922 - loss: 0.2647\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9573 - Precision: 0.8851 - Recall: 0.9030 - accuracy: 0.8929 - loss: 0.2636\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9572 - Precision: 0.8861 - Recall: 0.9005 - accuracy: 0.8924 - loss: 0.2641\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9574 - Precision: 0.8858 - Recall: 0.9020 - accuracy: 0.8929 - loss: 0.2631\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9577 - Precision: 0.8862 - Recall: 0.9024 - accuracy: 0.8933 - loss: 0.2623\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9574 - Precision: 0.8866 - Recall: 0.9022 - accuracy: 0.8935 - loss: 0.2630\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9569 - Precision: 0.8856 - Recall: 0.9021 - accuracy: 0.8928 - loss: 0.2647\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9575 - Precision: 0.8856 - Recall: 0.9033 - accuracy: 0.8933 - loss: 0.2628\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9578 - Precision: 0.8868 - Recall: 0.9026 - accuracy: 0.8937 - loss: 0.2620\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9581 - Precision: 0.8867 - Recall: 0.9030 - accuracy: 0.8938 - loss: 0.2611\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9579 - Precision: 0.8869 - Recall: 0.9044 - accuracy: 0.8946 - loss: 0.2613\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9578 - Precision: 0.8866 - Recall: 0.9033 - accuracy: 0.8940 - loss: 0.2619\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9580 - Precision: 0.8863 - Recall: 0.9049 - accuracy: 0.8945 - loss: 0.2611\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9584 - Precision: 0.8876 - Recall: 0.9036 - accuracy: 0.8946 - loss: 0.2599\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9582 - Precision: 0.8868 - Recall: 0.9036 - accuracy: 0.8942 - loss: 0.2606\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9584 - Precision: 0.8873 - Recall: 0.9035 - accuracy: 0.8944 - loss: 0.2598\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9586 - Precision: 0.8878 - Recall: 0.9037 - accuracy: 0.8948 - loss: 0.2592\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9586 - Precision: 0.8857 - Recall: 0.9058 - accuracy: 0.8945 - loss: 0.2591\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9588 - Precision: 0.8879 - Recall: 0.9042 - accuracy: 0.8951 - loss: 0.2586\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9591 - Precision: 0.8885 - Recall: 0.9051 - accuracy: 0.8958 - loss: 0.2577\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9591 - Precision: 0.8889 - Recall: 0.9048 - accuracy: 0.8959 - loss: 0.2577\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9594 - Precision: 0.8893 - Recall: 0.9050 - accuracy: 0.8962 - loss: 0.2568\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9592 - Precision: 0.8892 - Recall: 0.9044 - accuracy: 0.8959 - loss: 0.2573\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9596 - Precision: 0.8883 - Recall: 0.9047 - accuracy: 0.8955 - loss: 0.2564\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9600 - Precision: 0.8893 - Recall: 0.9062 - accuracy: 0.8968 - loss: 0.2549\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9599 - Precision: 0.8900 - Recall: 0.9064 - accuracy: 0.8972 - loss: 0.2550\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9598 - Precision: 0.8900 - Recall: 0.9056 - accuracy: 0.8969 - loss: 0.2554\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9601 - Precision: 0.8891 - Recall: 0.9063 - accuracy: 0.8967 - loss: 0.2546\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9599 - Precision: 0.8892 - Recall: 0.9061 - accuracy: 0.8966 - loss: 0.2551\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9604 - Precision: 0.8903 - Recall: 0.9054 - accuracy: 0.8970 - loss: 0.2537\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9605 - Precision: 0.8904 - Recall: 0.9076 - accuracy: 0.8980 - loss: 0.2530\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9604 - Precision: 0.8898 - Recall: 0.9065 - accuracy: 0.8972 - loss: 0.2534\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9607 - Precision: 0.8908 - Recall: 0.9058 - accuracy: 0.8975 - loss: 0.2525\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9608 - Precision: 0.8920 - Recall: 0.9064 - accuracy: 0.8984 - loss: 0.2522\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9610 - Precision: 0.8917 - Recall: 0.9072 - accuracy: 0.8985 - loss: 0.2515\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9612 - Precision: 0.8919 - Recall: 0.9059 - accuracy: 0.8981 - loss: 0.2512\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9610 - Precision: 0.8912 - Recall: 0.9068 - accuracy: 0.8981 - loss: 0.2515\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9612 - Precision: 0.8917 - Recall: 0.9075 - accuracy: 0.8987 - loss: 0.2509\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9614 - Precision: 0.8926 - Recall: 0.9064 - accuracy: 0.8988 - loss: 0.2503\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9615 - Precision: 0.8926 - Recall: 0.9074 - accuracy: 0.8992 - loss: 0.2499\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9618 - Precision: 0.8936 - Recall: 0.9062 - accuracy: 0.8992 - loss: 0.2492\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9614 - Precision: 0.8921 - Recall: 0.9067 - accuracy: 0.8986 - loss: 0.2502\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9618 - Precision: 0.8931 - Recall: 0.9071 - accuracy: 0.8993 - loss: 0.2491\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9616 - Precision: 0.8914 - Recall: 0.9082 - accuracy: 0.8988 - loss: 0.2497\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9617 - Precision: 0.8917 - Recall: 0.9082 - accuracy: 0.8990 - loss: 0.2491\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9618 - Precision: 0.8932 - Recall: 0.9071 - accuracy: 0.8993 - loss: 0.2492\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9617 - Precision: 0.8921 - Recall: 0.9078 - accuracy: 0.8990 - loss: 0.2491\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9616 - Precision: 0.8923 - Recall: 0.9077 - accuracy: 0.8991 - loss: 0.2497\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9620 - Precision: 0.8932 - Recall: 0.9079 - accuracy: 0.8997 - loss: 0.2483\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9619 - Precision: 0.8934 - Recall: 0.9070 - accuracy: 0.8995 - loss: 0.2488\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9619 - Precision: 0.8926 - Recall: 0.9076 - accuracy: 0.8992 - loss: 0.2489\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9622 - Precision: 0.8934 - Recall: 0.9080 - accuracy: 0.8999 - loss: 0.2478\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9623 - Precision: 0.8935 - Recall: 0.9068 - accuracy: 0.8994 - loss: 0.2477\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9621 - Precision: 0.8929 - Recall: 0.9070 - accuracy: 0.8992 - loss: 0.2480\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - AUC: 0.9621 - Precision: 0.8936 - Recall: 0.9078 - accuracy: 0.8999 - loss: 0.2479\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9624 - Precision: 0.8943 - Recall: 0.9077 - accuracy: 0.9002 - loss: 0.2472\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9624 - Precision: 0.8940 - Recall: 0.9074 - accuracy: 0.9000 - loss: 0.2469\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9623 - Precision: 0.8938 - Recall: 0.9070 - accuracy: 0.8997 - loss: 0.2475\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9622 - Precision: 0.8935 - Recall: 0.9079 - accuracy: 0.8999 - loss: 0.2476\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - AUC: 0.9622 - Precision: 0.8930 - Recall: 0.9076 - accuracy: 0.8994 - loss: 0.2477\n",
      "\n",
      "Training on Fold 5/5...\n",
      "\n",
      "\u001b[1m8980/8980\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - AUC: 0.9560 - Precision: 0.8837 - Recall: 0.9027 - accuracy: 0.8918 - loss: 0.2683\n",
      "\u001b[1m5322/5322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - AUC: 0.9560 - Precision: 0.8836 - Recall: 0.9040 - accuracy: 0.8923 - loss: 0.2679\n",
      "\u001b[1m3757/3757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 9ms/step - AUC: 0.9560 - Precision: 0.8835 - Recall: 0.9019 - accuracy: 0.8913 - loss: 0.2677\n",
      "\u001b[1m2903/2903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - AUC: 0.9547 - Precision: 0.8808 - Recall: 0.9005 - accuracy: 0.8891 - loss: 0.2717\n",
      "\u001b[1m2366/2366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - AUC: 0.9520 - Precision: 0.8763 - Recall: 0.8985 - accuracy: 0.8857 - loss: 0.2800\n",
      "\u001b[1m1996/1996\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 12ms/step - AUC: 0.9498 - Precision: 0.8746 - Recall: 0.8927 - accuracy: 0.8822 - loss: 0.2859\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 14ms/step - AUC: 0.9523 - Precision: 0.8772 - Recall: 0.8991 - accuracy: 0.8864 - loss: 0.2785\n",
      "\u001b[1m1525/1525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 15ms/step - AUC: 0.9512 - Precision: 0.8746 - Recall: 0.8991 - accuracy: 0.8849 - loss: 0.2818\n",
      "\u001b[1m1362/1362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - AUC: 0.9505 - Precision: 0.8744 - Recall: 0.8960 - accuracy: 0.8834 - loss: 0.2838\n",
      "\u001b[1m1231/1231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - AUC: 0.9502 - Precision: 0.8736 - Recall: 0.8956 - accuracy: 0.8828 - loss: 0.2845\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9499 - Precision: 0.8755 - Recall: 0.8956 - accuracy: 0.8839 - loss: 0.2856\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9502 - Precision: 0.8728 - Recall: 0.8962 - accuracy: 0.8826 - loss: 0.2847\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9502 - Precision: 0.8756 - Recall: 0.8956 - accuracy: 0.8840 - loss: 0.2848\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9493 - Precision: 0.8747 - Recall: 0.8932 - accuracy: 0.8824 - loss: 0.2873\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9494 - Precision: 0.8719 - Recall: 0.8963 - accuracy: 0.8821 - loss: 0.2870\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9503 - Precision: 0.8734 - Recall: 0.8976 - accuracy: 0.8835 - loss: 0.2843\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9507 - Precision: 0.8743 - Recall: 0.8974 - accuracy: 0.8840 - loss: 0.2832\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9510 - Precision: 0.8752 - Recall: 0.8973 - accuracy: 0.8845 - loss: 0.2824\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9504 - Precision: 0.8750 - Recall: 0.8967 - accuracy: 0.8840 - loss: 0.2841\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9510 - Precision: 0.8763 - Recall: 0.8957 - accuracy: 0.8844 - loss: 0.2822\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9512 - Precision: 0.8754 - Recall: 0.8981 - accuracy: 0.8849 - loss: 0.2822\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9514 - Precision: 0.8743 - Recall: 0.8978 - accuracy: 0.8842 - loss: 0.2816\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9518 - Precision: 0.8771 - Recall: 0.8966 - accuracy: 0.8853 - loss: 0.2804\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9505 - Precision: 0.8736 - Recall: 0.8969 - accuracy: 0.8833 - loss: 0.2839\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9512 - Precision: 0.8763 - Recall: 0.8960 - accuracy: 0.8846 - loss: 0.2819\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9512 - Precision: 0.8749 - Recall: 0.8973 - accuracy: 0.8843 - loss: 0.2820\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9519 - Precision: 0.8769 - Recall: 0.8976 - accuracy: 0.8856 - loss: 0.2796\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9516 - Precision: 0.8769 - Recall: 0.8973 - accuracy: 0.8854 - loss: 0.2808\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9519 - Precision: 0.8789 - Recall: 0.8961 - accuracy: 0.8861 - loss: 0.2797\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9520 - Precision: 0.8777 - Recall: 0.8965 - accuracy: 0.8856 - loss: 0.2794\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9515 - Precision: 0.8777 - Recall: 0.8950 - accuracy: 0.8850 - loss: 0.2810\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9514 - Precision: 0.8763 - Recall: 0.8981 - accuracy: 0.8854 - loss: 0.2813\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9515 - Precision: 0.8778 - Recall: 0.8965 - accuracy: 0.8856 - loss: 0.2808\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9520 - Precision: 0.8781 - Recall: 0.8963 - accuracy: 0.8857 - loss: 0.2794\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9519 - Precision: 0.8756 - Recall: 0.8990 - accuracy: 0.8854 - loss: 0.2801\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9522 - Precision: 0.8766 - Recall: 0.8985 - accuracy: 0.8858 - loss: 0.2790\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9525 - Precision: 0.8789 - Recall: 0.8968 - accuracy: 0.8865 - loss: 0.2782\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9528 - Precision: 0.8775 - Recall: 0.8996 - accuracy: 0.8868 - loss: 0.2772\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9531 - Precision: 0.8786 - Recall: 0.8984 - accuracy: 0.8869 - loss: 0.2761\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9532 - Precision: 0.8782 - Recall: 0.9008 - accuracy: 0.8877 - loss: 0.2760\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9532 - Precision: 0.8794 - Recall: 0.9000 - accuracy: 0.8881 - loss: 0.2759\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9533 - Precision: 0.8800 - Recall: 0.8982 - accuracy: 0.8876 - loss: 0.2755\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9535 - Precision: 0.8784 - Recall: 0.8999 - accuracy: 0.8875 - loss: 0.2753\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9539 - Precision: 0.8812 - Recall: 0.8991 - accuracy: 0.8887 - loss: 0.2741\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9535 - Precision: 0.8796 - Recall: 0.8978 - accuracy: 0.8872 - loss: 0.2754\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9541 - Precision: 0.8818 - Recall: 0.8981 - accuracy: 0.8887 - loss: 0.2735\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9543 - Precision: 0.8821 - Recall: 0.8985 - accuracy: 0.8890 - loss: 0.2728\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9545 - Precision: 0.8812 - Recall: 0.9001 - accuracy: 0.8892 - loss: 0.2722\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9544 - Precision: 0.8814 - Recall: 0.8996 - accuracy: 0.8891 - loss: 0.2724\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9547 - Precision: 0.8827 - Recall: 0.9004 - accuracy: 0.8901 - loss: 0.2713\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9552 - Precision: 0.8839 - Recall: 0.8994 - accuracy: 0.8905 - loss: 0.2699\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9550 - Precision: 0.8830 - Recall: 0.8999 - accuracy: 0.8901 - loss: 0.2704\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9546 - Precision: 0.8833 - Recall: 0.8981 - accuracy: 0.8895 - loss: 0.2718\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9553 - Precision: 0.8834 - Recall: 0.8992 - accuracy: 0.8900 - loss: 0.2697\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9551 - Precision: 0.8851 - Recall: 0.8965 - accuracy: 0.8899 - loss: 0.2707\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9550 - Precision: 0.8841 - Recall: 0.8978 - accuracy: 0.8898 - loss: 0.2707\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9559 - Precision: 0.8840 - Recall: 0.9006 - accuracy: 0.8910 - loss: 0.2680\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9556 - Precision: 0.8837 - Recall: 0.9004 - accuracy: 0.8908 - loss: 0.2687\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9560 - Precision: 0.8838 - Recall: 0.9009 - accuracy: 0.8911 - loss: 0.2675\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9561 - Precision: 0.8859 - Recall: 0.8994 - accuracy: 0.8916 - loss: 0.2672\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9560 - Precision: 0.8843 - Recall: 0.9007 - accuracy: 0.8912 - loss: 0.2676\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9562 - Precision: 0.8832 - Recall: 0.9013 - accuracy: 0.8908 - loss: 0.2669\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9567 - Precision: 0.8860 - Recall: 0.9006 - accuracy: 0.8921 - loss: 0.2655\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9567 - Precision: 0.8855 - Recall: 0.9016 - accuracy: 0.8923 - loss: 0.2653\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9567 - Precision: 0.8839 - Recall: 0.9020 - accuracy: 0.8916 - loss: 0.2655\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9571 - Precision: 0.8862 - Recall: 0.9017 - accuracy: 0.8927 - loss: 0.2640\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9573 - Precision: 0.8863 - Recall: 0.9018 - accuracy: 0.8929 - loss: 0.2638\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9575 - Precision: 0.8861 - Recall: 0.9017 - accuracy: 0.8927 - loss: 0.2629\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9578 - Precision: 0.8876 - Recall: 0.9024 - accuracy: 0.8939 - loss: 0.2618\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9578 - Precision: 0.8876 - Recall: 0.9024 - accuracy: 0.8939 - loss: 0.2619\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9577 - Precision: 0.8874 - Recall: 0.9026 - accuracy: 0.8939 - loss: 0.2620\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9580 - Precision: 0.8874 - Recall: 0.9031 - accuracy: 0.8940 - loss: 0.2616\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9578 - Precision: 0.8865 - Recall: 0.9032 - accuracy: 0.8936 - loss: 0.2620\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9581 - Precision: 0.8874 - Recall: 0.9034 - accuracy: 0.8942 - loss: 0.2611\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9583 - Precision: 0.8882 - Recall: 0.9043 - accuracy: 0.8951 - loss: 0.2601\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9584 - Precision: 0.8877 - Recall: 0.9040 - accuracy: 0.8946 - loss: 0.2601\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9587 - Precision: 0.8894 - Recall: 0.9027 - accuracy: 0.8950 - loss: 0.2591\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9588 - Precision: 0.8887 - Recall: 0.9038 - accuracy: 0.8951 - loss: 0.2586\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9588 - Precision: 0.8891 - Recall: 0.9046 - accuracy: 0.8957 - loss: 0.2587\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - AUC: 0.9587 - Precision: 0.8890 - Recall: 0.9029 - accuracy: 0.8949 - loss: 0.2591\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9589 - Precision: 0.8892 - Recall: 0.9034 - accuracy: 0.8952 - loss: 0.2584\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9588 - Precision: 0.8887 - Recall: 0.9035 - accuracy: 0.8950 - loss: 0.2587\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9593 - Precision: 0.8884 - Recall: 0.9059 - accuracy: 0.8959 - loss: 0.2568\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9593 - Precision: 0.8891 - Recall: 0.9047 - accuracy: 0.8957 - loss: 0.2570\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9595 - Precision: 0.8894 - Recall: 0.9044 - accuracy: 0.8957 - loss: 0.2565\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9596 - Precision: 0.8894 - Recall: 0.9051 - accuracy: 0.8961 - loss: 0.2561\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9597 - Precision: 0.8900 - Recall: 0.9051 - accuracy: 0.8964 - loss: 0.2560\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9598 - Precision: 0.8904 - Recall: 0.9058 - accuracy: 0.8970 - loss: 0.2553\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9602 - Precision: 0.8917 - Recall: 0.9062 - accuracy: 0.8978 - loss: 0.2542\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9600 - Precision: 0.8905 - Recall: 0.9053 - accuracy: 0.8968 - loss: 0.2547\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9601 - Precision: 0.8910 - Recall: 0.9049 - accuracy: 0.8969 - loss: 0.2548\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9603 - Precision: 0.8912 - Recall: 0.9065 - accuracy: 0.8977 - loss: 0.2535\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9604 - Precision: 0.8910 - Recall: 0.9052 - accuracy: 0.8971 - loss: 0.2535\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9604 - Precision: 0.8913 - Recall: 0.9061 - accuracy: 0.8977 - loss: 0.2536\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9606 - Precision: 0.8910 - Recall: 0.9054 - accuracy: 0.8971 - loss: 0.2533\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9605 - Precision: 0.8914 - Recall: 0.9065 - accuracy: 0.8978 - loss: 0.2530\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9609 - Precision: 0.8907 - Recall: 0.9081 - accuracy: 0.8982 - loss: 0.2517\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9606 - Precision: 0.8915 - Recall: 0.9059 - accuracy: 0.8977 - loss: 0.2530\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9610 - Precision: 0.8917 - Recall: 0.9067 - accuracy: 0.8981 - loss: 0.2518\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9611 - Precision: 0.8925 - Recall: 0.9072 - accuracy: 0.8988 - loss: 0.2513\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9610 - Precision: 0.8919 - Recall: 0.9067 - accuracy: 0.8982 - loss: 0.2517\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9611 - Precision: 0.8917 - Recall: 0.9068 - accuracy: 0.8981 - loss: 0.2515\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9609 - Precision: 0.8915 - Recall: 0.9062 - accuracy: 0.8978 - loss: 0.2520\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9613 - Precision: 0.8921 - Recall: 0.9078 - accuracy: 0.8988 - loss: 0.2508\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9613 - Precision: 0.8923 - Recall: 0.9065 - accuracy: 0.8984 - loss: 0.2508\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9611 - Precision: 0.8917 - Recall: 0.9063 - accuracy: 0.8979 - loss: 0.2516\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9611 - Precision: 0.8921 - Recall: 0.9071 - accuracy: 0.8985 - loss: 0.2513\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - AUC: 0.9610 - Precision: 0.8919 - Recall: 0.9070 - accuracy: 0.8983 - loss: 0.2516\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9613 - Precision: 0.8919 - Recall: 0.9081 - accuracy: 0.8988 - loss: 0.2505\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9613 - Precision: 0.8925 - Recall: 0.9068 - accuracy: 0.8986 - loss: 0.2509\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9614 - Precision: 0.8932 - Recall: 0.9068 - accuracy: 0.8990 - loss: 0.2505\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9615 - Precision: 0.8936 - Recall: 0.9069 - accuracy: 0.8993 - loss: 0.2502\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9612 - Precision: 0.8919 - Recall: 0.9066 - accuracy: 0.8982 - loss: 0.2512\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9615 - Precision: 0.8927 - Recall: 0.9069 - accuracy: 0.8987 - loss: 0.2499\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9616 - Precision: 0.8923 - Recall: 0.9080 - accuracy: 0.8990 - loss: 0.2497\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9617 - Precision: 0.8929 - Recall: 0.9081 - accuracy: 0.8994 - loss: 0.2493\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - AUC: 0.9614 - Precision: 0.8942 - Recall: 0.9066 - accuracy: 0.8994 - loss: 0.2501\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - AUC: 0.9617 - Precision: 0.8929 - Recall: 0.9067 - accuracy: 0.8988 - loss: 0.2496\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9615 - Precision: 0.8938 - Recall: 0.9065 - accuracy: 0.8992 - loss: 0.2499\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - AUC: 0.9616 - Precision: 0.8934 - Recall: 0.9073 - accuracy: 0.8993 - loss: 0.2496\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import numpy as np\n",
    "\n",
    "# Define early stopping based on training loss\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"loss\",  # Monitor training loss\n",
    "    patience=10,  # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True,  # Restore model to best epoch\n",
    "    min_delta=0.0001,  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Function to compute the current batch size based on the warmup schedule\n",
    "def get_current_batch_size(epoch, initial_batch_size, final_batch_size, warmup_epochs):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        return initial_batch_size + (final_batch_size - initial_batch_size) * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        return final_batch_size\n",
    "\n",
    "# Function to compute the current learning rate based on the batch size\n",
    "def get_current_learning_rate(initial_lr, current_batch_size, initial_batch_size):\n",
    "    return initial_lr * (current_batch_size / initial_batch_size)\n",
    "\n",
    "# Training with Transfer Learning\n",
    "initial_batch_size = 64  # Start with a smaller batch size\n",
    "final_batch_size = 512  # Target batch size\n",
    "warmup_epochs = 10  # Number of epochs for warmup\n",
    "initial_lr = 0.001  # Initial learning rate\n",
    "\n",
    "# Define a learning rate scheduler after warmup\n",
    "lr_schedule = CosineDecay(\n",
    "    initial_learning_rate=initial_lr * (final_batch_size / initial_batch_size),\n",
    "    decay_steps=120 - warmup_epochs\n",
    ")\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"\\nTraining on Fold {i+1}/5...\\n\")\n",
    "    \n",
    "    # Match peptides & HLAs\n",
    "    peptides = np.array([peptide_embeddings[p] for p in fold[\"peptide\"]])\n",
    "    hlas = np.array([hla_df.loc[h].values for h in fold[\"HLA\"]])\n",
    "    labels = fold[\"label\"].values\n",
    "\n",
    "    # Custom training loop with batch size warmup, learning rate adjustment, and early stopping\n",
    "    for epoch in range(120):  # Total epochs\n",
    "        # Compute the current batch size\n",
    "        current_batch_size = int(get_current_batch_size(epoch, initial_batch_size, final_batch_size, warmup_epochs))\n",
    "        \n",
    "        # Compute the current learning rate\n",
    "        if epoch < warmup_epochs:\n",
    "            # During warmup, scale the learning rate linearly with the batch size\n",
    "            current_lr = get_current_learning_rate(initial_lr, current_batch_size, initial_batch_size)\n",
    "        else:\n",
    "            # After warmup, use the learning rate scheduler\n",
    "            current_lr = lr_schedule(epoch - warmup_epochs)\n",
    "        \n",
    "        # Update the learning rate in the optimizer\n",
    "        siamese_net.optimizer.learning_rate.assign(current_lr)\n",
    "        \n",
    "        # Train the model for one epoch with the current batch size\n",
    "        history = siamese_net.fit(\n",
    "            [peptides, hlas], labels,\n",
    "            epochs=1,  # Train for one epoch at a time\n",
    "            batch_size=current_batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]  # Add early stopping callback\n",
    "        )\n",
    "        \n",
    "        # Check if early stopping was triggered\n",
    "        if early_stopping.stopped_epoch > 0:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n",
    "            break\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611280b2-1a70-440e-a91c-9ba872d28cb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d383c-f74c-49e2-8c08-0f8f1b958cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define Swish activation function\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load HLA features (drop non-feature columns)\n",
    "hla_df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv\")\n",
    "hla_df = hla_df.drop(columns=[\"Unnamed: 0\"])  # Correct way to drop a column\n",
    "hla_df = hla_df.set_index(\"HLA\")  # Set HLA column as index\n",
    "\n",
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\train_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings = pickle.load(f)\n",
    "\n",
    "# Load all folds\n",
    "folds = [\n",
    "    pd.read_csv(rf\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold{i}.csv\")\n",
    "    for i in range(5)\n",
    "]\n",
    "# Remove '*' from HLA names\n",
    "for fold in folds:\n",
    "    fold[\"HLA\"] = fold[\"HLA\"].str.replace(\"*\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b57587f-0798-4029-943a-113efd72d785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\asus\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model input shapes\n",
    "input_shape1 = (320,)  # Peptide\n",
    "input_shape2 = (180,)  # HLA\n",
    "\n",
    "# Define model\n",
    "left_input = Input(shape=input_shape1)\n",
    "right_input = Input(shape=input_shape2)\n",
    "\n",
    "encoded_l = Dense(256, activation=swish)(left_input)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "encoded_l = Dense(128, activation=swish)(encoded_l)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "\n",
    "encoded_r = Dense(128, activation=swish)(right_input)\n",
    "encoded_r = Dropout(0.2)(encoded_r)\n",
    "\n",
    "L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "concatenated = Dense(64, activation=swish)(L1_distance)\n",
    "concatenated = Dropout(0.2)(concatenated)\n",
    "concatenated = Dense(32, activation=swish)(concatenated)\n",
    "\n",
    "prediction = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0dc6d-80d9-44c1-8a3c-bb40ade51415",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "=== Training on 4 folds, Testing on Fold 1 ===\n",
      "==================================================\n",
      "\n",
      "Training samples: 2298670, Test samples: 574658\n",
      "Epoch 1/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 17ms/step - accuracy: 0.7881 - loss: 0.4467\n",
      "Epoch 2/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 17ms/step - accuracy: 0.8565 - loss: 0.3358\n",
      "Epoch 3/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 17ms/step - accuracy: 0.8646 - loss: 0.3190\n",
      "Epoch 4/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.8693 - loss: 0.3097\n",
      "Epoch 5/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 10ms/step - accuracy: 0.8720 - loss: 0.3033\n",
      "Epoch 6/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 10ms/step - accuracy: 0.8742 - loss: 0.2988\n",
      "Epoch 7/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8762 - loss: 0.2952\n",
      "Epoch 8/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8781 - loss: 0.2911\n",
      "Epoch 9/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8794 - loss: 0.2882\n",
      "Epoch 10/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8807 - loss: 0.2853\n",
      "Epoch 11/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8824 - loss: 0.2826\n",
      "Epoch 12/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8836 - loss: 0.2805\n",
      "Epoch 13/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8845 - loss: 0.2778\n",
      "Epoch 14/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8851 - loss: 0.2763\n",
      "Epoch 15/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8863 - loss: 0.2742\n",
      "Epoch 16/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8871 - loss: 0.2727\n",
      "Epoch 17/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 11ms/step - accuracy: 0.8880 - loss: 0.2711\n",
      "Epoch 18/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8884 - loss: 0.2699\n",
      "Epoch 19/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8892 - loss: 0.2680\n",
      "Epoch 20/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8898 - loss: 0.2669\n",
      "Epoch 21/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8902 - loss: 0.2659\n",
      "Epoch 22/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8908 - loss: 0.2648\n",
      "Epoch 23/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8917 - loss: 0.2632\n",
      "Epoch 24/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8918 - loss: 0.2625\n",
      "Epoch 25/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8924 - loss: 0.2614\n",
      "Epoch 26/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8931 - loss: 0.2601\n",
      "Epoch 27/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8930 - loss: 0.2597\n",
      "Epoch 28/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8936 - loss: 0.2586\n",
      "Epoch 29/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8941 - loss: 0.2579\n",
      "Epoch 30/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8945 - loss: 0.2570\n",
      "Epoch 31/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8951 - loss: 0.2559\n",
      "Epoch 32/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8953 - loss: 0.2551\n",
      "Epoch 33/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8958 - loss: 0.2542\n",
      "Epoch 34/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8959 - loss: 0.2540\n",
      "Epoch 35/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8964 - loss: 0.2535\n",
      "Epoch 36/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8966 - loss: 0.2529\n",
      "Epoch 37/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8970 - loss: 0.2522\n",
      "Epoch 38/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8970 - loss: 0.2516\n",
      "Epoch 39/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8976 - loss: 0.2507\n",
      "Epoch 40/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8980 - loss: 0.2503\n",
      "Epoch 41/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.8984 - loss: 0.2492\n",
      "Epoch 42/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.8983 - loss: 0.2492\n",
      "Epoch 43/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8986 - loss: 0.2487\n",
      "Epoch 44/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8989 - loss: 0.2481\n",
      "Epoch 45/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8991 - loss: 0.2474\n",
      "Epoch 46/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8996 - loss: 0.2471\n",
      "Epoch 47/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.8995 - loss: 0.2466\n",
      "Epoch 48/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8997 - loss: 0.2463\n",
      "Epoch 49/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9001 - loss: 0.2452\n",
      "Epoch 50/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9003 - loss: 0.2447\n",
      "Epoch 51/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9006 - loss: 0.2445\n",
      "Epoch 52/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9009 - loss: 0.2441\n",
      "Epoch 53/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9010 - loss: 0.2434\n",
      "Epoch 54/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9012 - loss: 0.2432\n",
      "Epoch 55/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9016 - loss: 0.2427\n",
      "Epoch 56/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9015 - loss: 0.2425\n",
      "Epoch 57/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9016 - loss: 0.2421\n",
      "Epoch 58/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9021 - loss: 0.2415\n",
      "Epoch 59/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9025 - loss: 0.2407\n",
      "Epoch 60/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9024 - loss: 0.2406\n",
      "Epoch 61/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9027 - loss: 0.2401\n",
      "Epoch 62/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9024 - loss: 0.2402\n",
      "Epoch 63/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9029 - loss: 0.2397\n",
      "Epoch 64/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9032 - loss: 0.2393\n",
      "Epoch 65/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9032 - loss: 0.2389\n",
      "Epoch 66/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9033 - loss: 0.2386\n",
      "Epoch 67/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9033 - loss: 0.2385\n",
      "Epoch 68/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9035 - loss: 0.2380\n",
      "Epoch 69/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9035 - loss: 0.2381\n",
      "Epoch 70/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9040 - loss: 0.2373\n",
      "Epoch 71/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9041 - loss: 0.2370\n",
      "Epoch 72/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.9041 - loss: 0.2370\n",
      "Epoch 73/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9044 - loss: 0.2364\n",
      "Epoch 74/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9044 - loss: 0.2360\n",
      "Epoch 75/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9048 - loss: 0.2357\n",
      "Epoch 76/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9048 - loss: 0.2358\n",
      "Epoch 77/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9050 - loss: 0.2354\n",
      "Epoch 78/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9051 - loss: 0.2347\n",
      "Epoch 79/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9053 - loss: 0.2345\n",
      "Epoch 80/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9052 - loss: 0.2346\n",
      "Epoch 81/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9057 - loss: 0.2338\n",
      "Epoch 82/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9057 - loss: 0.2335\n",
      "Epoch 83/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9056 - loss: 0.2335\n",
      "Epoch 84/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9058 - loss: 0.2331\n",
      "Epoch 85/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9058 - loss: 0.2332\n",
      "Epoch 86/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9060 - loss: 0.2327\n",
      "Epoch 87/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9059 - loss: 0.2326\n",
      "Epoch 88/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9062 - loss: 0.2325\n",
      "Epoch 89/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9067 - loss: 0.2316\n",
      "Epoch 90/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9066 - loss: 0.2319\n",
      "Epoch 91/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9068 - loss: 0.2312\n",
      "Epoch 92/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9067 - loss: 0.2312\n",
      "Epoch 93/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9067 - loss: 0.2311\n",
      "Epoch 94/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9068 - loss: 0.2309\n",
      "Epoch 95/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9070 - loss: 0.2310\n",
      "Epoch 96/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9073 - loss: 0.2300\n",
      "Epoch 97/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.9073 - loss: 0.2301\n",
      "Epoch 98/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9071 - loss: 0.2301\n",
      "Epoch 99/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9075 - loss: 0.2295\n",
      "Epoch 100/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9076 - loss: 0.2296\n",
      "\u001b[1m17959/17959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step\n",
      "\n",
      "==============================\n",
      "Fold 1 Detailed Results:\n",
      "==============================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9505    0.8680    0.9074    287329\n",
      "           1     0.8786    0.9548    0.9151    287329\n",
      "\n",
      "    accuracy                         0.9114    574658\n",
      "   macro avg     0.9145    0.9114    0.9112    574658\n",
      "weighted avg     0.9145    0.9114    0.9112    574658\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives: 249410\n",
      "False Positives: 37919\n",
      "False Negatives: 12996\n",
      "True Positives: 274333\n",
      "\n",
      "Additional Metrics:\n",
      "AUC: 0.9734\n",
      "AUPR: 0.9712\n",
      "MCC: 0.8259\n",
      "Accuracy: 0.9114\n",
      "Specificity: 0.8680\n",
      "Sensitivity: 0.9548\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Training on 4 folds, Testing on Fold 2 ===\n",
      "==================================================\n",
      "\n",
      "Training samples: 2298670, Test samples: 574658\n",
      "Epoch 1/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12ms/step - accuracy: 0.9070 - loss: 0.2308\n",
      "Epoch 2/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9070 - loss: 0.2304\n",
      "Epoch 3/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9074 - loss: 0.2295\n",
      "Epoch 4/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9074 - loss: 0.2295\n",
      "Epoch 5/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9073 - loss: 0.2298\n",
      "Epoch 6/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9079 - loss: 0.2288\n",
      "Epoch 7/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9076 - loss: 0.2293\n",
      "Epoch 8/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9079 - loss: 0.2285\n",
      "Epoch 9/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9078 - loss: 0.2285\n",
      "Epoch 10/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9081 - loss: 0.2281\n",
      "Epoch 11/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9083 - loss: 0.2276\n",
      "Epoch 12/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9083 - loss: 0.2277\n",
      "Epoch 13/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9086 - loss: 0.2275\n",
      "Epoch 14/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9082 - loss: 0.2275\n",
      "Epoch 15/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9085 - loss: 0.2269\n",
      "Epoch 16/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9085 - loss: 0.2268\n",
      "Epoch 17/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9088 - loss: 0.2264\n",
      "Epoch 18/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9088 - loss: 0.2261\n",
      "Epoch 19/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9090 - loss: 0.2263\n",
      "Epoch 20/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9089 - loss: 0.2261\n",
      "Epoch 21/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9089 - loss: 0.2261\n",
      "Epoch 22/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9091 - loss: 0.2257\n",
      "Epoch 23/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9090 - loss: 0.2255\n",
      "Epoch 24/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9091 - loss: 0.2256\n",
      "Epoch 25/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9097 - loss: 0.2249\n",
      "Epoch 26/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9093 - loss: 0.2252\n",
      "Epoch 27/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9097 - loss: 0.2246\n",
      "Epoch 28/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9098 - loss: 0.2242\n",
      "Epoch 29/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9099 - loss: 0.2244\n",
      "Epoch 30/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9098 - loss: 0.2243\n",
      "Epoch 31/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.9097 - loss: 0.2242\n",
      "Epoch 32/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9101 - loss: 0.2237\n",
      "Epoch 33/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9102 - loss: 0.2235\n",
      "Epoch 34/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9099 - loss: 0.2240\n",
      "Epoch 35/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9103 - loss: 0.2233\n",
      "Epoch 36/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.9103 - loss: 0.2228\n",
      "Epoch 37/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9106 - loss: 0.2229\n",
      "Epoch 38/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9102 - loss: 0.2230\n",
      "Epoch 39/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9106 - loss: 0.2226\n",
      "Epoch 40/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9107 - loss: 0.2224\n",
      "Epoch 41/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9104 - loss: 0.2222\n",
      "Epoch 42/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9108 - loss: 0.2219\n",
      "Epoch 43/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9108 - loss: 0.2218\n",
      "Epoch 44/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9107 - loss: 0.2216\n",
      "Epoch 45/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9111 - loss: 0.2214\n",
      "Epoch 46/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9109 - loss: 0.2214\n",
      "Epoch 47/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9111 - loss: 0.2214\n",
      "Epoch 48/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9112 - loss: 0.2214\n",
      "Epoch 49/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9112 - loss: 0.2214\n",
      "Epoch 50/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9114 - loss: 0.2212\n",
      "Epoch 51/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9116 - loss: 0.2207\n",
      "Epoch 52/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12ms/step - accuracy: 0.9116 - loss: 0.2205\n",
      "Epoch 53/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9113 - loss: 0.2206\n",
      "Epoch 54/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9118 - loss: 0.2201\n",
      "Epoch 55/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9116 - loss: 0.2203\n",
      "Epoch 56/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9115 - loss: 0.2203\n",
      "Epoch 57/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9114 - loss: 0.2202\n",
      "Epoch 58/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9118 - loss: 0.2201\n",
      "Epoch 59/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9119 - loss: 0.2197\n",
      "Epoch 60/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9119 - loss: 0.2195\n",
      "Epoch 61/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9121 - loss: 0.2192\n",
      "Epoch 62/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9117 - loss: 0.2196\n",
      "Epoch 63/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9119 - loss: 0.2194\n",
      "Epoch 64/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9121 - loss: 0.2193\n",
      "Epoch 65/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9123 - loss: 0.2191\n",
      "Epoch 66/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9124 - loss: 0.2186\n",
      "Epoch 67/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9123 - loss: 0.2187\n",
      "Epoch 68/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9122 - loss: 0.2185\n",
      "Epoch 69/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9122 - loss: 0.2188\n",
      "Epoch 70/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9126 - loss: 0.2182\n",
      "Epoch 71/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9128 - loss: 0.2177\n",
      "Epoch 72/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.9128 - loss: 0.2177\n",
      "Epoch 73/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9128 - loss: 0.2179\n",
      "Epoch 74/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9127 - loss: 0.2176\n",
      "Epoch 75/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9126 - loss: 0.2177\n",
      "Epoch 76/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9130 - loss: 0.2174\n",
      "Epoch 77/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9130 - loss: 0.2173\n",
      "Epoch 78/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9129 - loss: 0.2173\n",
      "Epoch 79/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9132 - loss: 0.2173\n",
      "Epoch 80/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9130 - loss: 0.2171\n",
      "Epoch 81/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9133 - loss: 0.2166\n",
      "Epoch 82/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9133 - loss: 0.2167\n",
      "Epoch 83/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.9132 - loss: 0.2166\n",
      "Epoch 84/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9131 - loss: 0.2169\n",
      "Epoch 85/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9133 - loss: 0.2163\n",
      "Epoch 86/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9134 - loss: 0.2164\n",
      "Epoch 87/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9134 - loss: 0.2162\n",
      "Epoch 88/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9136 - loss: 0.2159\n",
      "Epoch 89/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9133 - loss: 0.2162\n",
      "Epoch 90/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9136 - loss: 0.2158\n",
      "Epoch 91/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9136 - loss: 0.2155\n",
      "Epoch 92/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9141 - loss: 0.2156\n",
      "Epoch 93/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9138 - loss: 0.2155\n",
      "Epoch 94/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9139 - loss: 0.2153\n",
      "Epoch 95/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9139 - loss: 0.2154\n",
      "Epoch 96/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9139 - loss: 0.2153\n",
      "Epoch 97/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9137 - loss: 0.2151\n",
      "Epoch 98/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9142 - loss: 0.2146\n",
      "Epoch 99/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.9142 - loss: 0.2148\n",
      "Epoch 100/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9140 - loss: 0.2147\n",
      "\u001b[1m17959/17959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3ms/step\n",
      "\n",
      "==============================\n",
      "Fold 2 Detailed Results:\n",
      "==============================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9557    0.8775    0.9149    287329\n",
      "           1     0.8867    0.9593    0.9216    287329\n",
      "\n",
      "    accuracy                         0.9184    574658\n",
      "   macro avg     0.9212    0.9184    0.9183    574658\n",
      "weighted avg     0.9212    0.9184    0.9183    574658\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives: 252120\n",
      "False Positives: 35209\n",
      "False Negatives: 11681\n",
      "True Positives: 275648\n",
      "\n",
      "Additional Metrics:\n",
      "AUC: 0.9772\n",
      "AUPR: 0.9750\n",
      "MCC: 0.8396\n",
      "Accuracy: 0.9184\n",
      "Specificity: 0.8775\n",
      "Sensitivity: 0.9593\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Training on 4 folds, Testing on Fold 3 ===\n",
      "==================================================\n",
      "\n",
      "Training samples: 2298670, Test samples: 574658\n",
      "Epoch 1/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.9135 - loss: 0.2165\n",
      "Epoch 2/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 14ms/step - accuracy: 0.9135 - loss: 0.2160\n",
      "Epoch 3/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9136 - loss: 0.2159\n",
      "Epoch 4/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9137 - loss: 0.2158\n",
      "Epoch 5/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9138 - loss: 0.2156\n",
      "Epoch 6/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9143 - loss: 0.2149\n",
      "Epoch 7/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9143 - loss: 0.2147\n",
      "Epoch 8/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9143 - loss: 0.2145\n",
      "Epoch 9/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9140 - loss: 0.2149\n",
      "Epoch 10/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9140 - loss: 0.2151\n",
      "Epoch 11/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.2143\n",
      "Epoch 12/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.2144\n",
      "Epoch 13/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.2144\n",
      "Epoch 14/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9142 - loss: 0.2143\n",
      "Epoch 15/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9146 - loss: 0.2139\n",
      "Epoch 16/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9145 - loss: 0.2141\n",
      "Epoch 17/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.2138\n",
      "Epoch 18/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9146 - loss: 0.2136\n",
      "Epoch 19/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9147 - loss: 0.2136\n",
      "Epoch 20/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9147 - loss: 0.2136\n",
      "Epoch 21/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9146 - loss: 0.2136\n",
      "Epoch 22/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9145 - loss: 0.2135\n",
      "Epoch 23/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9148 - loss: 0.2132\n",
      "Epoch 24/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9148 - loss: 0.2128\n",
      "Epoch 25/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9150 - loss: 0.2129\n",
      "Epoch 26/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9151 - loss: 0.2127\n",
      "Epoch 27/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9151 - loss: 0.2125\n",
      "Epoch 28/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9148 - loss: 0.2130\n",
      "Epoch 29/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9151 - loss: 0.2127\n",
      "Epoch 30/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9150 - loss: 0.2126\n",
      "Epoch 31/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9150 - loss: 0.2123\n",
      "Epoch 32/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9152 - loss: 0.2125\n",
      "Epoch 33/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9154 - loss: 0.2121\n",
      "Epoch 34/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9152 - loss: 0.2126\n",
      "Epoch 35/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9152 - loss: 0.2121\n",
      "Epoch 36/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9152 - loss: 0.2124\n",
      "Epoch 37/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 14ms/step - accuracy: 0.9155 - loss: 0.2119\n",
      "Epoch 38/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9154 - loss: 0.2116\n",
      "Epoch 39/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9155 - loss: 0.2118\n",
      "Epoch 40/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9156 - loss: 0.2113\n",
      "Epoch 41/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9156 - loss: 0.2114\n",
      "Epoch 42/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14ms/step - accuracy: 0.9157 - loss: 0.2117\n",
      "Epoch 43/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9156 - loss: 0.2113\n",
      "Epoch 44/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9158 - loss: 0.2112\n",
      "Epoch 45/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9159 - loss: 0.2112\n",
      "Epoch 46/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9161 - loss: 0.2110\n",
      "Epoch 47/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9159 - loss: 0.2111\n",
      "Epoch 48/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9157 - loss: 0.2111\n",
      "Epoch 49/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9162 - loss: 0.2105\n",
      "Epoch 50/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9163 - loss: 0.2107\n",
      "Epoch 51/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9160 - loss: 0.2108\n",
      "Epoch 52/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9160 - loss: 0.2105\n",
      "Epoch 53/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9161 - loss: 0.2105\n",
      "Epoch 54/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9160 - loss: 0.2105\n",
      "Epoch 55/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9159 - loss: 0.2104\n",
      "Epoch 56/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9160 - loss: 0.2105\n",
      "Epoch 57/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9162 - loss: 0.2099\n",
      "Epoch 58/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9161 - loss: 0.2099\n",
      "Epoch 59/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9162 - loss: 0.2100\n",
      "Epoch 60/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9168 - loss: 0.2096\n",
      "Epoch 61/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9165 - loss: 0.2103\n",
      "Epoch 62/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9164 - loss: 0.2097\n",
      "Epoch 63/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9163 - loss: 0.2096\n",
      "Epoch 64/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9166 - loss: 0.2094\n",
      "Epoch 65/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9166 - loss: 0.2092\n",
      "Epoch 66/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9167 - loss: 0.2092\n",
      "Epoch 67/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9165 - loss: 0.2092\n",
      "Epoch 68/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9165 - loss: 0.2095\n",
      "Epoch 69/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9165 - loss: 0.2094\n",
      "Epoch 70/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9167 - loss: 0.2092\n",
      "Epoch 71/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9168 - loss: 0.2091\n",
      "Epoch 72/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9167 - loss: 0.2089\n",
      "Epoch 73/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9171 - loss: 0.2084\n",
      "Epoch 74/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9169 - loss: 0.2088\n",
      "Epoch 75/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9168 - loss: 0.2090\n",
      "Epoch 76/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9168 - loss: 0.2085\n",
      "Epoch 77/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9169 - loss: 0.2085\n",
      "Epoch 78/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9167 - loss: 0.2085\n",
      "Epoch 79/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2081\n",
      "Epoch 80/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2081\n",
      "Epoch 81/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14ms/step - accuracy: 0.9167 - loss: 0.2086\n",
      "Epoch 82/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.9169 - loss: 0.2084\n",
      "Epoch 83/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9168 - loss: 0.2085\n",
      "Epoch 84/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2080\n",
      "Epoch 85/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2080\n",
      "Epoch 86/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9172 - loss: 0.2077\n",
      "Epoch 87/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2080\n",
      "Epoch 88/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2081\n",
      "Epoch 89/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9171 - loss: 0.2081\n",
      "Epoch 90/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9175 - loss: 0.2075\n",
      "Epoch 91/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9176 - loss: 0.2075\n",
      "Epoch 92/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9171 - loss: 0.2078\n",
      "Epoch 93/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2072\n",
      "Epoch 94/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9172 - loss: 0.2076\n",
      "Epoch 95/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9176 - loss: 0.2072\n",
      "Epoch 96/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9174 - loss: 0.2074\n",
      "Epoch 97/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9175 - loss: 0.2071\n",
      "Epoch 98/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9175 - loss: 0.2072\n",
      "Epoch 99/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9178 - loss: 0.2069\n",
      "Epoch 100/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 15ms/step - accuracy: 0.9169 - loss: 0.2086\n",
      "Epoch 2/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 14ms/step - accuracy: 0.9171 - loss: 0.2084\n",
      "Epoch 4/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9175 - loss: 0.2077\n",
      "Epoch 5/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9172 - loss: 0.2078\n",
      "Epoch 6/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9176 - loss: 0.2074\n",
      "Epoch 7/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9177 - loss: 0.2075\n",
      "Epoch 8/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2077\n",
      "Epoch 9/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9175 - loss: 0.2072\n",
      "Epoch 10/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9173 - loss: 0.2073\n",
      "Epoch 11/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9177 - loss: 0.2070\n",
      "Epoch 12/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9176 - loss: 0.2069\n",
      "Epoch 13/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.9178 - loss: 0.2072\n",
      "Epoch 14/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9176 - loss: 0.2070\n",
      "Epoch 15/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9179 - loss: 0.2066\n",
      "Epoch 16/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9178 - loss: 0.2065\n",
      "Epoch 17/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9180 - loss: 0.2061\n",
      "Epoch 18/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9181 - loss: 0.2068\n",
      "Epoch 19/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9179 - loss: 0.2069\n",
      "Epoch 20/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9179 - loss: 0.2068\n",
      "Epoch 21/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9179 - loss: 0.2065\n",
      "Epoch 22/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9181 - loss: 0.2062\n",
      "Epoch 23/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9180 - loss: 0.2062\n",
      "Epoch 24/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9183 - loss: 0.2058\n",
      "Epoch 25/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9180 - loss: 0.2061\n",
      "Epoch 26/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9181 - loss: 0.2059\n",
      "Epoch 27/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9181 - loss: 0.2061\n",
      "Epoch 28/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9181 - loss: 0.2059\n",
      "Epoch 29/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9181 - loss: 0.2056\n",
      "Epoch 30/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9184 - loss: 0.2057\n",
      "Epoch 31/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9181 - loss: 0.2059\n",
      "Epoch 32/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9186 - loss: 0.2052\n",
      "Epoch 33/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9184 - loss: 0.2055\n",
      "Epoch 34/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9185 - loss: 0.2052\n",
      "Epoch 35/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9183 - loss: 0.2055\n",
      "Epoch 36/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9183 - loss: 0.2058\n",
      "Epoch 37/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9182 - loss: 0.2058\n",
      "Epoch 38/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9186 - loss: 0.2051\n",
      "Epoch 39/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9183 - loss: 0.2051\n",
      "Epoch 40/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9186 - loss: 0.2052\n",
      "Epoch 41/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9185 - loss: 0.2054\n",
      "Epoch 42/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9189 - loss: 0.2043\n",
      "Epoch 43/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9187 - loss: 0.2046\n",
      "Epoch 44/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9186 - loss: 0.2050\n",
      "Epoch 45/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9186 - loss: 0.2049\n",
      "Epoch 46/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9188 - loss: 0.2050\n",
      "Epoch 47/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9184 - loss: 0.2051\n",
      "Epoch 48/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9187 - loss: 0.2046\n",
      "Epoch 49/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9189 - loss: 0.2043\n",
      "Epoch 50/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9188 - loss: 0.2049\n",
      "Epoch 51/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9188 - loss: 0.2048\n",
      "Epoch 52/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9189 - loss: 0.2045\n",
      "Epoch 53/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9188 - loss: 0.2045\n",
      "Epoch 54/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9190 - loss: 0.2045\n",
      "Epoch 55/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9187 - loss: 0.2043\n",
      "Epoch 56/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9190 - loss: 0.2043\n",
      "Epoch 57/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9190 - loss: 0.2042\n",
      "Epoch 58/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9192 - loss: 0.2040\n",
      "Epoch 59/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.9193 - loss: 0.2038\n",
      "Epoch 60/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9189 - loss: 0.2041\n",
      "Epoch 61/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9191 - loss: 0.2037\n",
      "Epoch 62/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - accuracy: 0.9192 - loss: 0.2038\n",
      "Epoch 63/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9190 - loss: 0.2039\n",
      "Epoch 64/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9193 - loss: 0.2036\n",
      "Epoch 65/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9193 - loss: 0.2035\n",
      "Epoch 66/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9192 - loss: 0.2036\n",
      "Epoch 67/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 13ms/step - accuracy: 0.9192 - loss: 0.2037\n",
      "Epoch 68/100\n",
      "\u001b[1m4490/4490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 13ms/step - accuracy: 0.9192 - loss: 0.2034\n",
      "Epoch 69/100\n",
      "\u001b[1m2745/4490\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9190 - loss: 0.2037"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, \n",
    "                            matthews_corrcoef, accuracy_score, \n",
    "                            confusion_matrix, classification_report)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Initialize metric storage\n",
    "metrics = {\n",
    "    'AUC': [],\n",
    "    'AUPR': [],\n",
    "    'MCC': [],\n",
    "    'ACC': [],\n",
    "    'Specificity': [],\n",
    "    'Sensitivity': []\n",
    "}\n",
    "\n",
    "for test_fold_idx in range(5):\n",
    "    print(f\"\\n\\n{'='*50}\")\n",
    "    print(f\"=== Training on 4 folds, Testing on Fold {test_fold_idx + 1} ===\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Combine 4 folds for training\n",
    "    train_peptides = []\n",
    "    train_hlas = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for fold_idx in range(5):\n",
    "        if fold_idx != test_fold_idx:\n",
    "            fold = folds[fold_idx]\n",
    "            train_peptides.extend(fold['peptide'])\n",
    "            train_hlas.extend(fold['HLA'])\n",
    "            train_labels.extend(fold['label'])\n",
    "    \n",
    "    # Use current fold for testing\n",
    "    test_fold = folds[test_fold_idx]\n",
    "    test_peptides = test_fold['peptide']\n",
    "    test_hlas = test_fold['HLA']\n",
    "    test_labels = test_fold['label']\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    X_train_pep = np.array([peptide_embeddings[p] for p in train_peptides])\n",
    "    X_train_hla = np.array([hla_df.loc[h].values for h in train_hlas])\n",
    "    y_train = np.array(train_labels)\n",
    "    \n",
    "    X_test_pep = np.array([peptide_embeddings[p] for p in test_peptides])\n",
    "    X_test_hla = np.array([hla_df.loc[h].values for h in test_hlas])\n",
    "    y_test = np.array(test_labels)\n",
    "    \n",
    "    # Recompile model for fresh training\n",
    "    siamese_net.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training samples: {len(X_train_pep)}, Test samples: {len(X_test_pep)}\")\n",
    "    siamese_net.fit(\n",
    "        [X_train_pep, X_train_hla], y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict probabilities and classes\n",
    "    y_pred_probs = siamese_net.predict([X_test_pep, X_test_hla]).flatten()\n",
    "    y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel()\n",
    "    \n",
    "    metrics['AUC'].append(roc_auc_score(y_test, y_pred_probs))\n",
    "    metrics['AUPR'].append(average_precision_score(y_test, y_pred_probs))\n",
    "    metrics['MCC'].append(matthews_corrcoef(y_test, y_pred_classes))\n",
    "    metrics['ACC'].append(accuracy_score(y_test, y_pred_classes))\n",
    "    metrics['Specificity'].append(tn / (tn + fp))\n",
    "    metrics['Sensitivity'].append(tp / (tp + fn))\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"Fold {test_fold_idx + 1} Detailed Results:\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes, digits=4))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    # Additional Metrics\n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"AUC: {metrics['AUC'][-1]:.4f}\")\n",
    "    print(f\"AUPR: {metrics['AUPR'][-1]:.4f}\")\n",
    "    print(f\"MCC: {metrics['MCC'][-1]:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['ACC'][-1]:.4f}\")\n",
    "    print(f\"Specificity: {metrics['Specificity'][-1]:.4f}\")\n",
    "    print(f\"Sensitivity: {metrics['Sensitivity'][-1]:.4f}\")\n",
    "\n",
    "# Calculate and display final results\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"=== Final 5-Fold Cross-Validation Results ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nIndividual Fold Results:\")\n",
    "print(\"Fold\\tAUC\\tAUPR\\tMCC\\tACC\\tSpec\\tSens\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}\\t\" + \"\\t\".join(f\"{metrics[metric][i]:.4f}\" \n",
    "          for metric in ['AUC', 'AUPR', 'MCC', 'ACC', 'Specificity', 'Sensitivity']))\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(\"Metric\\tMean\\tStd\")\n",
    "for metric in metrics:\n",
    "    print(f\"{metric}\\t{np.mean(metrics[metric]):.4f}\\t{np.std(metrics[metric]):.4f}\")\n",
    "\n",
    "# Save all results to a dictionary for further analysis\n",
    "results = {\n",
    "    'per_fold_metrics': metrics,\n",
    "    'average_metrics': {metric: np.mean(values) for metric, values in metrics.items()},\n",
    "    'std_metrics': {metric: np.std(values) for metric, values in metrics.items()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e9bbb1-73b9-47d7-a698-f582aec230d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': np.float64(0.9783921078641467),\n",
       " 'AUPR': np.float64(0.976240480221686),\n",
       " 'MCC': np.float64(0.8482882228162769),\n",
       " 'ACC': np.float64(0.9231287140918056),\n",
       " 'Specificity': np.float64(0.8895096629564143),\n",
       " 'Sensitivity': np.float64(0.9567477652271968)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['average_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8404fe4a-951d-4c41-91ba-1b4422b9a4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': np.float64(0.002975898493658373),\n",
       " 'AUPR': np.float64(0.002993959373452749),\n",
       " 'MCC': np.float64(0.014694229296108894),\n",
       " 'ACC': np.float64(0.007777530130314855),\n",
       " 'Specificity': np.float64(0.016215746754368557),\n",
       " 'Sensitivity': np.float64(0.0025139005343834237)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['std_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97edc84a-ed65-48a0-b57f-adfc958d0e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.9733621979103274),\n",
       " np.float64(0.9772357251790202),\n",
       " np.float64(0.978969645184864),\n",
       " np.float64(0.9803076585721329),\n",
       " np.float64(0.9820853124743887)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['per_fold_metrics']['AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0679f05f-c71b-4ac0-8e27-ff34a6906c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.9712104499011122),\n",
       " np.float64(0.9750325365966928),\n",
       " np.float64(0.9767818614539365),\n",
       " np.float64(0.9782112935548505),\n",
       " np.float64(0.9799662596018375)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['per_fold_metrics']['AUPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88426ccd-eff6-4c5b-881f-2482419f0bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.8259118477991703),\n",
       " np.float64(0.8396269473626313),\n",
       " np.float64(0.8499640963652425),\n",
       " np.float64(0.857052334389012),\n",
       " np.float64(0.8688858881653289)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['per_fold_metrics']['MCC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b53b9b-ecf9-41f5-b991-a4a2edb160ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.911399475862165,\n",
       " 0.9184036418182641,\n",
       " 0.9240382975613322,\n",
       " 0.9276682827003191,\n",
       " 0.9341338725169481]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['per_fold_metrics']['ACC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b23a2db0-dd4e-49ac-bc80-346aabf5f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.8680293322289083),\n",
       " np.float64(0.8774610289946368),\n",
       " np.float64(0.890735011084854),\n",
       " np.float64(0.8960459960533047),\n",
       " np.float64(0.9152769464203684)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['per_fold_metrics']['Specificity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c8bd122-a952-423f-9dc9-d46c2dfd69d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.9547696194954216),\n",
       " np.float64(0.9593462546418914),\n",
       " np.float64(0.9573415840378103),\n",
       " np.float64(0.9592905693473336),\n",
       " np.float64(0.9529907986135279)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['per_fold_metrics']['Sensitivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91450b-2537-4a2a-9380-985880db4941",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39d73c67-ca92-4362-9d99-7da78ea16b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\external_set.csv\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "peptides_test = np.array([peptide_embeddings_test[p] for p in test[\"peptide\"]])\n",
    "hlas_test = np.array([hla_df.loc[h].values for h in test[\"HLA\"]])\n",
    "labels_test = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b34a1e2-3c85-487b-abee-091e93638099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\independent_set.csv\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "peptides_test = np.array([peptide_embeddings_test[p] for p in test[\"peptide\"]])\n",
    "hlas_test = np.array([hla_df.loc[h].values for h in test[\"HLA\"]])\n",
    "labels_test = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3efc1aa-4431-48be-8992-8b0fa5b9d4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103865"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hlas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a290ff66-97b1-4402-bb74-f82192c68f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peptides shape: (103865, 320)\n",
      "HLAs shape: (103865, 180)\n",
      "\u001b[1m3246/3246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Remove the last dimension (1) since your model expects 2D inputs\n",
    "l_test = np.array(peptides_test).reshape(-1, 320)  # Shape: (103865, 320)\n",
    "r_test = np.array(hlas_test).reshape(-1, 180)      # Shape: (103865, 180)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Peptides shape:\", l_test.shape)  # Should be (103865, 320)\n",
    "print(\"HLAs shape:\", r_test.shape)      # Should be (103865, 180)\n",
    "\n",
    "# Now predict\n",
    "pred = siamese_net.predict([l_test, r_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60e83676-b467-4977-a16b-87cfc30b7af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABK2UlEQVR4nO3deXgUVdo28Lv3Tiednc6+sIMIRALEgIyiGaMiyKifvDACMqjjyLyj5HVDNscNdBBhFMVhEZ1RQR1FRxCQSAYRlC1REAiEsJOVkHS23s/3RyctgQTSId2VTt+/6+rLUF1V/dQhdt2cOnVKJoQQICIiIpKIXOoCiIiIyL8xjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJSSl1AazgcDpw9exZ6vR4ymUzqcoiIiKgVhBCorq5GbGws5PKW+z98IoycPXsWCQkJUpdBREREbXDq1CnEx8e3+L5PhBG9Xg/AeTDBwcESV0NEREStYTQakZCQ4DqPt8QnwkjjpZng4GCGESIiIh9zpSEWHMBKREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJyO4xs3boVo0ePRmxsLGQyGdauXXvFbXJycjBo0CBoNBr06NEDq1atakOpRERE1Bm5HUZqa2sxcOBALFmypFXrHzt2DKNGjcLIkSORl5eHxx9/HA8++CA2btzodrFERETU+bj9bJrbb78dt99+e6vXX7p0Kbp27YrXXnsNANC3b19s27YNr7/+OjIzM939eCIiIupkPP6gvB07diAjI6PJsszMTDz++OMtbmM2m2E2m11/NhqNniqPiIjIaxwOAavDAatdwGpzwGSzw2YXsNgdsNodqDXbYbM74BCAzeGAzS5gcwiUVZsRpFXC4RCwOwTsQjh/FgIOAdfyU+frYNBrISAgGpY7BOAQ4oKX889CAHaHcP089YauSAjXSdIuHg8jxcXFiIqKarIsKioKRqMR9fX1CAgIuGSbefPm4a9//aunSyMiIoLN7kCd1Y4akw1mmwMmqx3VJhssNgcsdjvqLHbUW+yw2B2w2Bw4Vl6L0AAVLHYBk9UOk9WOWosd1SYr6szO9WwOB6w2AZPNua3JaofRZJP6UC9rTEps5w0jbTFjxgxkZWW5/mw0GpGQkCBhRURE1FFZbA5Um6wwmmyoNTtfBWU1AIB6ix3GeiuqGl41ZhuqTTacq7U4/2yyod5ql/gIgJAAFVQKOdQKGc5WmRAXGgC9VgmlQgalXA6VQoaDRdVISQiFXC6DQgbIZbKGn2VQyGWQyQCFXIaC0hr0jQmGouF9eeO6MkAmk7l+dr736/tRwVrJjt/jYSQ6OholJSVNlpWUlCA4OLjZXhEA0Gg00Gg0ni6NiIi8TAiBeqsdNWYbzFYHKuusqLPYUFpthtnmQHFVPdRKOQpKa6DXqmC1O1DXECj2nDiP2NAA7DtThcggDcw2O8xWByx2R7vUJpcBDgFEBmmgUcpxprIePQxBCA9UQ6dWQK2QQ62UQ62Q43RlPfrFBiNApYBWpUCQRolAjQKBGiU0SgVUChlUCjlUCjl0auc6WpUcGqXzv43vKeSydqnd13k8jKSnp2P9+vVNln3zzTdIT0/39EcTEVE7aAwE1SarKxjUmG0wmmwoLKuBQiaD1SFwpKQaeq0SB4qMCNOpcaSkBiqlDCq5HPVW5+WOGrMNdodocy3nai0AgPIa8yXvNQYCnVqJyCA1zlaaMDg5DEEaJcJ0agQHKBGkUSFIq0RkoBqhOrVrG71WBbWSU29Jxe0wUlNTg4KCAtefjx07hry8PISHhyMxMREzZszAmTNn8P777wMAHnnkEbz55pt46qmn8Ic//AHffvstPv74Y6xbt679joKIiJpwOARqLc7AUGOyoare6ryEYbGhvNqM4+fqEKBWoK7hskXj2IYas80VHEqNZtSYPTPOQSYDAlQKyADUWuxIjtDBahfo1iUQtWYb+sQE41yNGd26BCFY6wwQeo0SDiEQH6aDUiFDSIAKWpUCOpUCwQEq9jL4MLfDyO7duzFy5EjXnxvHdkyePBmrVq1CUVERTp486Xq/a9euWLduHaZPn47FixcjPj4ey5cv5229RETNsNodzssYDeMfymssDXdZ2FxjHowmG0xWO34+XYnwQDUOnDXCoNfCZHOGiPO1FlTUWSDa3gHRIpXCObYgWKtCcIASwVoVKuutiA8LQGSQBsZ6KxLCdRBCIDkyEHKZDBGBagSoFQhoCA1BGiV0agVkMoYHcpIJ4Ylf1/ZlNBoREhKCqqoqBAcHS10OEVGL7A6BOosN9RY7quqtOFdrQUFpDeQyGY6V18BkdaDYaIJGKUdVvRXGeiuqzTYUltV6pB61Qg69VokgrRIhASoEqJyXJHRqBeosdvSN0SOwIRw0jn0IUCugVSoQ0hAc9FoldBrnmAkGCHJHa8/fHfJuGiIiqQghUGuxo6LGgvN1zh6Go6U1UMhlKK4yodpsg1ohh8lqR1m1GbmnKqFRylFUZWrXOhRyGYK1SujUSpyprMeQ5DDotb+Gg0CNElqVAsZ6K3pGBUEIICZEC61KgTCdGpFBagQHqKBRMkBQx8cwQkR+oc5iw9nKepyvs6Ki1oLztRacraxHabUZJqsdRVUmFFWZcLKirl0+Ty4D9FoVwnQqRAZpcLayHkO7hqPGbENsaAB0aiW6dwlEeKAagRrn5Q691vlfrVrOXgjyKwwjROTThBAoqzHjxLk6nKux4FRFHc5W1eNURT2KjfU4V+OcT6LO0ra5JLp1CURsSABKjCb0jwuBTCaDQg4kRQRC33DpQ6WQIypYg5AANUICnKGCPRJErccwQkQdhsMhYDQ5ey7Kayw4V2NGZb0VJyvqcLy8FsfKa9FFr0G1yea6vbSy3gqLrXXzTARrlQgLVCM80BkawnVqqBRyaFRyDEoMQ3SIFnGhAa55JRgmiLyDYYSIPE4IgYpaC4qqTDhbWY/DJdWobxhzUVptRnGVCcVGEyrrrFfc16Hi6maX67VKxIYEoLshEPFhOsSHBSA2JACReg3CdCqE6pwBhIg6HoYRIroqjUHjTGU9yqrNKKoy4cS5WpysqEOx0YwyownlNRa3Z8lMitAhIlCNMJ2zJ0OvVUEhB8IDNejWJdA1xiIkQAVDsAYapcJDR0hEnsYwQkQtsjsETlXUodhoQlm1GWXVZhwoMqLaZMWW/DIY9BqUNUzj3RqRQWpEh2gREaiBQwikdQ1HZJAGMaEBiArWIDJIg6CGu0SIyH8wjBD5MSEESoxmnD5fh1Pn63DiXB2KKk04U1mPY+W1KKqqx+Vm7j59vt71s0GvgSFYA4Nei8RwHZIjdIgJDWhYrkWXIA2n2yaiZjGMEHVyZpsdJ87V4XBJNY6X1+JURT3OVtWjxGhCcZXpio811yjliA0NQESgGpFBGkSHaBEcoEK/2GAEqpVICA9AVLCWvRlE1GYMI0SdgBACp8/X45ezVTh9vh4FpTU4cc7Z21FUZbrsg8kan1Q6MD4EPaP0SAjTIS4swNW7ERGk4TM/iMijGEaIfIzRZMX+M1XYf6YK+cU1+Hp/Eeqt9ss+h0SnVqB7lyD0NAQhMUKH+DAdooO1iAhSo2tkIHs1iEhSDCNEHZTN7sCh4mrknqpEfrERBaU1KCitbfbR6Y2UchliQrX43XXxSG4IHUkROhj0Gs6ZQUQdFsMIUQdQWm1CQUkNjp+rQ2FZDY6W1WD3ifOobmE8R2yIFgMTQtEnOhi9o4PQw6BHYriOA0SJyCcxjBB5kRACR8tqsf1oOfadrsK5WgsOnDWi2Nj8Q9aCNEr0jwvBwIRQ9DQEobshCN27BEKv5eRdRNR5MIwQeZDDIVy9HLknz2NLfhnKqi+9zCKXAYnhOnSNDERyZCC6RQbi2rgQ9I8LgVLB3g4i6twYRojaiRACx8/VIb/YiNxTlcg7WYkDZ42oNje91KJRynFtXAj6xugREqDCjb0MzttkNfzfkYj8E7/9iK7C8fJa7DxWge1Hy/HjsQoUVV16uUWtkCM1KQz940Pwm55dMDg5jHevEBFdgGGEyA0nztUi+2Ap9p+pwvdHy1FibHrJRa2Qo7shCP3jgp0BJC4UPaOCoOKlFiKiFjGMEF1GUVU9cvLLsOtYBX46XYmjZbVN3lfIZQjSKDEhLREjekQiJTEUOjX/tyIicge/NYkuYLU78Ome08g7WYldxytQWH5p+EjrGo4hyeFISQxFWtdwhg8ioqvEb1Hye0II7DxWgW8OlGDdvqIm4z7kMmBgQihu6BGJAfGhSE0KQ3igWsJqiYg6H4YR8lsHzhrx8e5TWL+vCKUX3G4bplPhtmtjMKx7BG7oEYkwhg8iIo9iGCG/4XAIbD1ShnU/F2H70XM4U1nvei9ApcBt10bjpt5d8NtronjphYjIi/iNS51ancWGbw+V4t97TiP3VCUq66yu95RyGW7qbcDY62Jxcx8DAwgRkUT47UudjsXmwNbDZfjPz2ex8ZdimKwO13tyGTB+aCJu7ReNQYmhnFadiKgDYBihTuNYeS0+/PEEPtt7BudqLa7lcaEByOwXjZv7GDCkaxg0Sk44RkTUkTCMkE87X2vBun1FzttxT1W6lkcEqnFH/xiMSYnF4KQwyGQy6YokIqLLYhghn+NwCOw8XoFPdp/Gf346C4vdeRlGKZdhWI9I3J+WiJt6G6BWctZTIiJfwDBCPuNURR0+3n0KH/x4EhUXXIbpGhmIsSlxmJCWiC56jYQVEhFRWzCMUIdmdwjk5Jfi3e+PY1tBuWu5RinHmIGx+H+DEzC0a7iEFRIR0dViGKEO67sjZViw6TB+umAsyJDkMNybGo+x18VxICoRUSfBMEIdzvaCcizdWoith8sAACqFDOOHJuL+65PQK0ovcXVERNTeGEaoQ7A7BDYfLMGyrYXYfeI8AOeA1HFDEvC/N/dEdIhW4gqJiMhTGEZIUrVmGz7efQrLvzvmmp5dKZfh3tR4/PHG7ugaGShxhURE5GkMIyQJi82Bf/1wAm/lFKC8xnlnTJBGifsGJ2DK8GQkhOskrpCIiLyFYYS8ymJz4PPc03gr5yhOnKsD4Jwh9cERXXHf4AQEavgrSUTkb/jNT15httnxye7TWLT5CMprzACAyCA1Hr2pByakJUKr4p0xRET+imGEPMpmd+Cz3DN4bVM+SozOEBIRqMYfb+yGCWlJCGJPCBGR3+OZgDxCCIH1+4qx8Jt8HC2rBQCE6lR4aEQ3TL2hK3tCiIjIhWGE2l1FrQWPr8lzzRMSrFXijzd2ZwghIqJmMYxQuzHb7Fj1/XG8uaUA1SYbZDJg2k098NCIbgjRqaQuj4iIOiiGEbpqVrsDX+8vxuLNh12XZHoagjD/nv5ITeJzY4iI6PIYRuiq7D5egRmf7cOR0hoAQEiACk/f1gfjhiRAIZdJXB0REfkChhFqkxqzDa98fQj//OEEAOeEZVNv6IrJw5IRHqiWuDoiIvIlDCPkts0HSjD7i/0oqjIBAO4ZFI+Zo/oyhBARUZswjFCrVdRa8OJXB/BZ7hkAQGK4Dn8d0w8j+xgkroyIiHwZwwi1yob9xZj5+T6cq3U+R+Z/hiRgzuhroFPzV4iIiK4OzyR0WQ6HwPNfHcCq7ccBAPFhAVg0LgWDk3mXDBERtQ+GEWpRtcmKJz/5GRt+KQYAPDAsGf93ay/otZwzhIiI2g/DCDUrv7gaD7y7E0VVJijkMsy/uz/+3+AEqcsiIqJOiGGEmnA4BP7xXSEWbMyHzSEQE6LF6+NScH23CKlLIyKiTophhFzMNjv+98NcbDpQAgAY0TMSC+9LQRe9RuLKiIioM2MYIQBAVZ0VU1btxN6TlZDLgJmjrsEfhidDJuMsqkRE5FkMI4SzlfWY8u4u5JdUQymX4Y3x1+H2/jFSl0VERH6CYcTPHSuvxf3Lf8SZynqEB6qxasoQDIgPlbosIiLyIwwjfizvVCWmrtqFc7UWxIZo8cFD16NrZKDUZRERkZ9hGPFTG/YXY/qaPNRb7ehhCML7fxiK2NAAqcsiIiI/JG/LRkuWLEFycjK0Wi3S0tKwc+fOy66/aNEi9O7dGwEBAUhISMD06dNhMpnaVDBdvY93ncKjH+xBvdWOlIRQfPboMAYRIiKSjNthZM2aNcjKysLcuXOxd+9eDBw4EJmZmSgtLW12/Q8//BDPPPMM5s6di4MHD2LFihVYs2YNnn322asuntz35U9n8dS/f4ZDAHcPisMnj6QjmDOqEhGRhNwOIwsXLsRDDz2EKVOm4JprrsHSpUuh0+mwcuXKZtffvn07hg8fjgkTJiA5ORm33norxo8ff8XeFGp/Ww+XYfqaPADAPYPiseDegVAp2tQ5RkRE1G7cOhNZLBbs2bMHGRkZv+5ALkdGRgZ27NjR7DbDhg3Dnj17XOGjsLAQ69evxx133NHi55jNZhiNxiYvujrbj5Zj6nu7YHcI3NYvGq/eOwByOecQISIi6bk1gLW8vBx2ux1RUVFNlkdFReHQoUPNbjNhwgSUl5fjhhtugBACNpsNjzzyyGUv08ybNw9//etf3SmNLuO7I2WYumo3rHaBtK7hWDw+BQoGESIi6iA83kefk5ODl19+GW+99Rb27t2Lzz77DOvWrcMLL7zQ4jYzZsxAVVWV63Xq1ClPl9lp7T9ThT/+cw8sdgeG94jAygeGQKNUSF0WERGRi1s9I5GRkVAoFCgpKWmyvKSkBNHR0c1uM3v2bEycOBEPPvggAKB///6ora3Fww8/jJkzZ0IuvzQPaTQaaDR8HsrVOlhkxMQVP6LOYkda13AGESIi6pDc6hlRq9VITU1Fdna2a5nD4UB2djbS09Ob3aauru6SwKFQOE+IQgh366VWOn2+Dg+8uxPn66zoGxOMpfenMogQEVGH5PakZ1lZWZg8eTIGDx6MoUOHYtGiRaitrcWUKVMAAJMmTUJcXBzmzZsHABg9ejQWLlyI6667DmlpaSgoKMDs2bMxevRoVyih9nWqog7j3tmBEqMZCeEB+ODBNIQFqqUui4iIqFluh5Fx48ahrKwMc+bMQXFxMVJSUrBhwwbXoNaTJ0826QmZNWsWZDIZZs2ahTNnzqBLly4YPXo0XnrppfY7CnLZc6IC//thLs5WmRAXGoCVk4cgnEGEiIg6MJnwgWslRqMRISEhqKqqQnBwsNTldFhHy2ow6u/fwWR1IC40AKsfvh4J4TqpyyIiIj/V2vM3n03TSZRWmzB55U6YrA4MTgrDu1OGQM+ZVYmIyAdw+s1OQAiBJz75GafP1yMmRIvF469jECEiIp/BMNIJfPnTWWw9XAalXIZ3JqYijg+9IyIiH8Iw4uNOn6/DzM/3AwCmDE/GgPhQaQsiIiJyE8OID7M7BJ769GfUmG3oaQjCE5m9pS6JiIjIbQwjPmzpf49i+9FzCFAp8Pb9gzipGRER+SSGER+Vd6oSf9uYDwB4dlRf9DDoJa6IiIiobRhGfFCt2YYnPvkJAHBzHwPuT0uUuCIiIqK2YxjxQX/bmI+C0hqEB6rx8u/6QyaTSV0SERFRmzGM+Jj/Hi7Dqu3HAQAL7xuI6BCttAURERFdJYYRH2Ky2vHcl78AAO4ZFI+behskroiIiOjqMYz4CCEEZq3dj2Plteii12DO6GukLomIiKhdMIz4iA93nsSne04DAF69ZwBCAjjdOxERdQ4MIz6guMqE+V8fAgBMz+iFkX14eYaIiDoPhpEOzmZ34H8/2otqkw39YoMxbWR3qUsiIiJqVwwjHdz7O05g1/HzCFAp8Mo9A6BU8K+MiIg6F57ZOrBTFXV4daPz8syzo/ri2rgQiSsiIiJqfwwjHdgLXx2AyerA4KQwTBjKWVaJiKhzYhjpoLYfLcemAyWQyYDn77oWCjlnWSUios6JYaQDMlntmLV2PwDg7uvicU1ssMQVEREReQ7DSAe0YtsxFJbVIjJIg9l39pW6HCIiIo9iGOlg6iw2vPv9cQDAM7f3QahOLW1BREREHsYw0sGs+O4YymvMiAsNwJiBsVKXQ0RE5HEMIx1IabUJS/97FADweEZPqJX86yEios6PZ7sO5K0tR1FrsWNAfAjuHhQvdTlERERewTDSQZw4V4tV248DcPaK8FZeIiLyFwwjHcSb3xYAAAbEh2Bkbz4Ij4iI/AfDSAfw06lKfLr3NADgzyN7QCZjrwgREfkPhhGJORwCz36+D0IAd6XE4tZ+0VKXRERE5FUMIxLbfLAEv5w1QquS45nb+0hdDhERkdcxjEhICIEFm/IBAA8M64qYkACJKyIiIvI+hhEJZR8sxeGSGqgUMky9oavU5RAREUmCYURCjbfyTkpPRhe9RtpiiIiIJMIwIpGfT1diW0E5lHIZJl6fJHU5REREkmEYkchHO08CAG7pa0ByZKDE1RAREUmHYUQC52st+PeeMwCAPwznWBEiIvJvDCMS2PBLMSx2B/pE6zG0a7jU5RAREUmKYcTLHA6BlduOAQBGD4zlbKtEROT3GEa8LPtQKY6U1kCnVmDckASpyyEiIpIcw4iX/euHEwCAuwfFITKIt/MSERExjHjRyXN1+O5IGQBgcnqytMUQERF1EAwjXvTRrpNwCGBocjh6RumlLoeIiKhDYBjxEpvdgY93nQIA/P76RImrISIi6jgYRrzkgx9P4lytBWE6FW6/NkbqcoiIiDoMhhEvaZxx9aHfdINayWYnIiJqxLOiFxSU1uBQcTUUchnGD+ElGiIiogsxjHjBlz+dBQCkd4tAWKBa4mqIiIg6FoYRD7PaHfjwR+fcImMGxkpcDRERUcfDMOJhPxZWoLzGOXD1d4PipC6HiIiow2EY8bC3/1sAAMjsFw2Vgs1NRER0MZ4dPchktSP3ZCUAYOx17BUhIiJqDsOIB209XIY6ix2hOhWGJodLXQ4REVGHxDDiQdkHSwEAY1PiIJfLJK6GiIioY2IY8RAhBNbmnQEA3Niri8TVEBERdVwMIx5y4lwdzDYHAGBwcpjE1RAREXVcDCMesut4BQBAq5JDr1VJXA0REVHH1aYwsmTJEiQnJ0Or1SItLQ07d+687PqVlZWYNm0aYmJioNFo0KtXL6xfv75NBfuKTQdKAAAPDOsqcSVEREQdm9LdDdasWYOsrCwsXboUaWlpWLRoETIzM5Gfnw+DwXDJ+haLBb/97W9hMBjw6aefIi4uDidOnEBoaGh71N8hGU1WfHvIOXh19EA+oZeIiOhy3A4jCxcuxEMPPYQpU6YAAJYuXYp169Zh5cqVeOaZZy5Zf+XKlaioqMD27duhUjkvVyQnJ19d1R1c9sES2B0CCeEBuCYmWOpyiIiIOjS3LtNYLBbs2bMHGRkZv+5ALkdGRgZ27NjR7DZffvkl0tPTMW3aNERFReHaa6/Fyy+/DLvd3uLnmM1mGI3GJi9fsu7nYgDA6AGxkMl4Sy8REdHluBVGysvLYbfbERUV1WR5VFQUiouLm92msLAQn376Kex2O9avX4/Zs2fjtddew4svvtji58ybNw8hISGuV0JCgjtlSkoIgX1nKgEAI3ryll4iIqIr8fjdNA6HAwaDAf/4xz+QmpqKcePGYebMmVi6dGmL28yYMQNVVVWu16lTpzxdZrspLK9FidEMpVyGAfEhUpdDRETU4bk1ZiQyMhIKhQIlJSVNlpeUlCA6OrrZbWJiYqBSqaBQKFzL+vbti+LiYlgsFqjV6ku20Wg00Gg07pTWYfx8uhIA0C82GIEat4fkEBER+R23ekbUajVSU1ORnZ3tWuZwOJCdnY309PRmtxk+fDgKCgrgcDhcyw4fPoyYmJhmg4iv23q4HAAwhM+iISIiahW3L9NkZWVh2bJleO+993Dw4EH86U9/Qm1trevumkmTJmHGjBmu9f/0pz+hoqICjz32GA4fPox169bh5ZdfxrRp09rvKDoIm92BzQ3zi9zc59LbnImIiOhSbl9HGDduHMrKyjBnzhwUFxcjJSUFGzZscA1qPXnyJOTyXzNOQkICNm7ciOnTp2PAgAGIi4vDY489hqeffrr9jqKD+Ol0JarNNgRrlRjSlT0jRERErSETQgipi7gSo9GIkJAQVFVVITi4487b8ea3R7Bg02Fk9DVg+eQhUpdDREQkqdaev/lsmna08/h5AMD13SIkroSIiMh3MIy0E5PVjp3HzgFgGCEiInIHw0g72XKoFCarAwa9hlPAExERuYFhpJ189XMRAOCWvgbI5ZwCnoiIqLUYRtqByWrHNwedt/T+7rp4iashIiLyLQwj7WD38fOw2BzootdgSHKY1OUQERH5FIaRdtA4cHVocjif0ktEROQmhpF28MOxCgDADT0jJa6EiIjI9zCMXCWLzYH9Z6oAAKlJvERDRETkLoaRq7T7eAXqLHZEBmnQo0uQ1OUQERH5HIaRq7TnhHPW1aFdw3hLLxERURswjFylHxvGiwxK5CUaIiKitmAYuQoWmwO/nHWOF7kuMVTaYoiIiHwUw8hV2PhLMc7XWdFFr8GA+FCpyyEiIvJJDCNX4WhZDQCgT7QeKgWbkoiIqC14Br0KPxT+OtkZERERtQ3DSBuVVpuws2Hw6tjr4iSuhoiIyHcxjLTRnuPn4RDOSzQJ4TqpyyEiIvJZDCNtlHeqEgCQkhAqaR1ERES+jmGkjRonOxvM8SJERERXhWGkDRwOgd0NYaRfbLDE1RAREfk2hpE2OHau1vVzryi9hJUQERH5PoaRNtheUA4AMOg1UPB5NERERFeFYaQN/nu4DAAwbkiCxJUQERH5PoaRNigsd16m6RPN8SJERERXi2HETfUWOwrLnGFkcDKf1EtERHS1GEbcdLDYCAAI1iph0GskroaIiMj3MYy46VBRNQAgKSIQMhkHrxIREV0thhE3nWi4rTckQCVxJURERJ0Dw4ibPt1zGgDw22uiJK6EiIioc2AYcYMQAqLh5x6GIElrISIi6iwYRtxQVm1GRa0FMhmQmsQ7aYiIiNoDw4gbDhY7B692jQiEVqWQuBoiIqLOgWHEDQeLnLf1dusSKHElREREnQfDiBuOlNQAAFISQqUthIiIqBNhGHHDD4XnAAAJ4TqJKyEiIuo8GEbcUGuxAQC6cOZVIiKidsMw0ko2uwO1ZmcYSWTPCBERUbthGGmlExV1sNoFtCo5YkMCpC6HiIio02AYaaXGwavdIoMgl/OZNERERO2FYaSVvt5fBIAzrxIREbU3hpFWstgcUpdARETUKTGMtNL5OgsAoF9ssMSVEBERdS4MI60ghEDuyUoAQHr3CGmLISIi6mQYRlrh9Pl6mG0OKOUy9IrSS10OERFRp8Iw0gqnztcBAGwOwQfkERERtTOGkVZovK33lj4GiSshIiLqfBhGWqHYaAIAxIdxsjMiIqL2xjDSClsPlwEAojnzKhERUbtjGGmFqnorACBIq5S4EiIios6HYaQVTp+vBwD0jwuRuBIiIqLOh2HkCuosNtfPCRwzQkRE1O4YRq7gbKWzVyRQrUBEkEbiaoiIiDofhpEr+OWsEQDQrQsfkEdEROQJDCNX0DjHSP94jhchIiLyBIaRKzha5gwjyRE6iSshIiLqnNoURpYsWYLk5GRotVqkpaVh586drdpu9erVkMlkGDt2bFs+VhL7z1YBAPrG8Gm9REREnuB2GFmzZg2ysrIwd+5c7N27FwMHDkRmZiZKS0svu93x48fxxBNPYMSIEW0u1ttsdgdOVTgHsPY08AF5REREnuB2GFm4cCEeeughTJkyBddccw2WLl0KnU6HlStXtriN3W7H73//e/z1r39Ft27drqpgbyqrMbt+Nuh5Jw0REZEnuBVGLBYL9uzZg4yMjF93IJcjIyMDO3bsaHG7559/HgaDAVOnTm3V55jNZhiNxiYvKZQYfw0jcrlMkhqIiIg6O7fCSHl5Oex2O6Kioposj4qKQnFxcbPbbNu2DStWrMCyZcta/Tnz5s1DSEiI65WQkOBOme3mfK0FAKBSMIgQERF5ikfvpqmursbEiROxbNkyREZGtnq7GTNmoKqqyvU6deqUB6tsWWm182m9w7q3vnYiIiJyj1tPfouMjIRCoUBJSUmT5SUlJYiOjr5k/aNHj+L48eMYPXq0a5nD4XB+sFKJ/Px8dO/e/ZLtNBoNNBrpx2j8eKwCABAVLH0tREREnZVbPSNqtRqpqanIzs52LXM4HMjOzkZ6evol6/fp0wf79u1DXl6e6zVmzBiMHDkSeXl5kl1+aS27QwAAFHJOx0JEROQpbvWMAEBWVhYmT56MwYMHY+jQoVi0aBFqa2sxZcoUAMCkSZMQFxeHefPmQavV4tprr22yfWhoKABcsrwjqmgYM9ItMlDiSoiIiDovt8PIuHHjUFZWhjlz5qC4uBgpKSnYsGGDa1DryZMnIe8kPQnfHSkHAPSK5hwjREREniITQgipi7gSo9GIkJAQVFVVITjYOzOhOhwC3Z5dDwD47qmRSAjndPBERETuaO35u3N0YXhARZ3F9XNUsFbCSoiIiDo3hpEWnK2sd/2sVrKZiIiIPIVn2RYcKqoGAAxKDJW2ECIiok6OYaQF/z1cBgDowmfSEBEReRTDSAsEnON6VQo2ERERkSfxTNuCOosdAJDePULiSoiIiDo3hpEWGOutAIAwnVriSoiIiDo3hpEWlNc4b+2NCGQYISIi8iSGkWZY7Q7Xrb3xnOyMiIjIoxhGmnHmfD1sDgG1Qo4YTnhGRETkUQwjzTjX8IA8i90BuVwmcTVERESdG8NIM840XKIZEB8icSVERESdH8NIM06eqwUA9DTwab1ERESexjDSjDOVJgBAXCjHixAREXkaw0gzTjT0jMSEBkhcCRERUefHMNKMwyU1AIBeUUESV0JERNT5MYxcxGJzoLzGDABIjgiUuBoiIqLOj2HkIo130mhVcoRz9lUiIiKPYxi5SFm1s1ckKlgLmYxzjBAREXkaw8hFGh+QF8oH5BEREXkFw8hFDpdWAwCCtUqJKyEiIvIPDCMXMVsdAACHEBJXQkRE5B8YRi5iNDkv03D2VSIiIu9gGLlIqdE5gDU+jBOeEREReQPDyEWyD5UAAOLDdBJXQkRE5B8YRi7iaBgqEqThAFYiIiJvYBi5iMXmHMAay4fkEREReQXDyAWq6qyun6NDGEaIiIi8gWHkAkfLnQ/IiwnRQqfmZRoiIiJvYBi5QHnDVPAGvUbiSoiIiPwHw8gFShvCSBeGESIiIq9hGLlAeQ3DCBERkbcxjFygsmEAa3CASuJKiIiI/AfDyAXKGnpGIgPZM0JEROQtDCMX2JpfBoC39RIREXkTw8gFQgOdl2d4mYaIiMh7GEYaCCFQVGkCAHSLDJS4GiIiIv/BMNKgxmyDreHBNBFBaomrISIi8h8MIw3O1VgAAAEqBWdfJSIi8iKGkQaNE56xV4SIiMi7GEYaVNQ6e0Y44RkREZF3MYw0OH6uFgAQpmPPCBERkTcxjDRwCOfgVavdIXElRERE/oVhpEGd2Q4AiA/TSVwJERGRf2EYaVBV73wuTSQHsBIREXkVw0iDc7XOu2k4ZoSIiMi7GEYaHC+vAwDEhQVIXAkREZF/YRhpcKDICACIDWEYISIi8iaGETifS9MoLJAPySMiIvImhhEAJuuvt/OG8Im9REREXsUwAqDabHX9HMjn0hAREXkVwwiAapMNAKDXKiGXyySuhoiIyL8wjODXOUZ4iYaIiMj7GEYAFFWaAABqBZuDiIjI23j2BSDgvJumst56hTWJiIiovTGMADhe7nxib/+4EIkrISIi8j9tCiNLlixBcnIytFot0tLSsHPnzhbXXbZsGUaMGIGwsDCEhYUhIyPjsutLweZw9oyUVZslroSIiMj/uB1G1qxZg6ysLMydOxd79+7FwIEDkZmZidLS0mbXz8nJwfjx47Flyxbs2LEDCQkJuPXWW3HmzJmrLr69lNc4Q0ifGL3ElRAREfkft8PIwoUL8dBDD2HKlCm45pprsHTpUuh0OqxcubLZ9T/44AM8+uijSElJQZ8+fbB8+XI4HA5kZ2dfdfHtRa1QOH8Ql1+PiIiI2p9bYcRisWDPnj3IyMj4dQdyOTIyMrBjx45W7aOurg5WqxXh4eEtrmM2m2E0Gpu8PGn3iQoAQNfIQI9+DhEREV3KrTBSXl4Ou92OqKioJsujoqJQXFzcqn08/fTTiI2NbRJoLjZv3jyEhIS4XgkJCe6U6bb4hif11lrsHv0cIiIiupRX76aZP38+Vq9ejc8//xxarbbF9WbMmIGqqirX69SpUx6tq6LWAgDoyzEjREREXufWg1giIyOhUChQUlLSZHlJSQmio6Mvu+2CBQswf/58bN68GQMGDLjsuhqNBhqNxp3Srsqu4+cBcAZWIiIiKbjVM6JWq5Gamtpk8GnjYNT09PQWt3v11VfxwgsvYMOGDRg8eHDbq/WQLkHO4KNWctoVIiIib3P7EbVZWVmYPHkyBg8ejKFDh2LRokWora3FlClTAACTJk1CXFwc5s2bBwB45ZVXMGfOHHz44YdITk52jS0JCgpCUFBQOx5K25ltzrEijaGEiIiIvMftMDJu3DiUlZVhzpw5KC4uRkpKCjZs2OAa1Hry5EnI5b/2MLz99tuwWCy49957m+xn7ty5eO65566u+nYghHA9tTdI63ZzEBER0VWSCSE6/OwaRqMRISEhqKqqQnBwcLvu22S1o8/sDQCAfc/dCr2W40aIiIjaQ2vP334/SKLugtt5A1QKCSshIiLyT34fRmoaLtEAgFLh981BRETkdX5/9jWarACAYI4XISIikoTfh5Hzdc4Jz6JDWp6EjYiIiDzH78PIyYo6AJzwjIiISCp+H0ZUDbchnzhXJ3ElRERE/snvw8iZynoAwJDklp8iTERERJ7j92HE0TDNSrXZdoU1iYiIyBP8PoyoGm7nDVRzjhEiIiIp+H0YsdkdAIAuej6XhoiISAoMIw7nZRql3O+bgoiISBJ+fwa2N4YRhUziSoiIiPyT34eRqnrnDKwKOcMIERGRFPw+jJRVmwEADkeHf3gxERFRp+T3YSQsUA0AMNscEldCRETkn/w+jNRb7QCA5AidxJUQERH5J78PIzUm52RngRo+tZeIiEgKfh9GGntGdGqGESIiIin4fRgxN4QRrcrvm4KIiEgSfn8GrrM4w4hGyengiYiIpOD3YcTaMB08e0aIiIik4fdnYKu9cQZWv28KIiIiSfj9GdjmcPaMKDkDKxERkSQYRhp6RlTsGSEiIpKE35+BG8eM8Nk0RERE0vD7MNJ4N02ghnfTEBERScGvw4jN7oCt4QF5ASqGESIiIin4dRi58OF4nGeEiIhIGn4dRhov0QCARunXTUFERCQZvz4DW+y/9ozIOYCViIhIEv4dRhou0wTxib1ERESS8eswYrY5L9OoeYmGiIhIMn59Fm4cM8I7aYiIiKTj12HEbHVepglQM4wQERFJxa/DSOMAVjWngiciIpKMX5+FS40mAICKY0aIiIgk49dnYW3DWJHC0hqJKyEiIvJffh1GGm/tvS4pTOJKiIiI/Jd/hxGOGSEiIpKcX5+FTVbnrb2cCp6IiEg6fn0WNjXc2qtR+XUzEBERScqvz8K2hss07BkhIiKSjl+fha0NYUTFMSNERESS8euz8OnKegAMI0RERFLy68fVNt5FU1lnlbgSIvfY7XZYrfy9JSJpqVQqKBRX/0gVvw4jjZOeBWn4bBryDUIIFBcXo7KyUupSiIgAAKGhoYiOjoZMJmvzPvw6jNgdAgAQolNLXAlR6zQGEYPBAJ1Od1X/8xMRXQ0hBOrq6lBaWgoAiImJafO+/DqM2BrCiFLOL3Tq+Ox2uyuIRERESF0OERECAgIAAKWlpTAYDG2+ZOPXIzcdDWFEwTBCPqBxjIhOp5O4EiKiXzV+J13NODa/DiN24QwjcnZ1kw/hpRki6kja4zvJr8PIrz0jEhdCRETkx/z6NMyeESLp3HTTTXj88celLoOIOgC/DiP7z1QB4JgRIl+Qk5MDmUzWLrc1b926FaNHj0ZsbCxkMhnWrl171fvsKHJycjBo0CBoNBr06NEDq1atuuI2H3/8MVJSUqDT6ZCUlIS//e1vbu+3uroajz/+OJKSkhAQEIBhw4Zh165drvetViuefvpp9O/fH4GBgYiNjcWkSZNw9uzZSz5r3bp1SEtLQ0BAAMLCwjB27Ngm7+/atQu33HILQkNDERYWhszMTPz0009N1hFCYMGCBejVqxc0Gg3i4uLw0ksvud4vKirChAkT0KtXL8jl8isG49WrV0Mmk11SS0lJCR544AHExsZCp9Phtttuw5EjR5qsU1xcjIkTJyI6OhqBgYEYNGgQ/v3vfzdZ5/Dhw7jrrrsQGRmJ4OBg3HDDDdiyZYtbx52Tk4O77roLMTExCAwMREpKCj744IMm+/jss88wePBghIaGutb55z//2eJxP/LII5DJZFi0aNFl2+dq+XUY6WEIAgCUVZslroSIvKm2thYDBw7EkiVLpC6lXR07dgyjRo3CyJEjkZeXh8cffxwPPvggNm7c2OI2X3/9NX7/+9/jkUcewf79+/HWW2/h9ddfx5tvvunWfh988EF88803+Oc//4l9+/bh1ltvRUZGBs6cOQMAqKurw969ezF79mzs3bsXn332GfLz8zFmzJgm9fz73//GxIkTMWXKFPz000/4/vvvMWHCBNf7NTU1uO2225CYmIgff/wR27Ztg16vR2ZmZpMBlI899hiWL1+OBQsW4NChQ/jyyy8xdOhQ1/tmsxldunTBrFmzMHDgwMu26/Hjx/HEE09gxIgRTZYLITB27FgUFhbiiy++QG5uLpKSkpCRkYHa2lrXepMmTUJ+fj6+/PJL7Nu3D3fffTfuu+8+5Obmuta58847YbPZ8O2332LPnj0YOHAg7rzzThQXF7f6uLdv344BAwbg3//+N37++WdMmTIFkyZNwldffeX6nPDwcMycORM7duxwrTNlypRmf0c+//xz/PDDD4iNjb1s+7QL4QOqqqoEAFFVVdWu+526apdIevor8dGPJ9p1v0SeUF9fLw4cOCDq6+ulLsVtNTU1YuLEiSIwMFBER0eLBQsWiBtvvFE89thjrnXef/99kZqaKoKCgkRUVJQYP368KCkpEUIIcezYMQGgyWvy5MlCCCG+/vprMXz4cBESEiLCw8PFqFGjREFBQatrAyA+//zzNh3XU089JXr27CkCAgJE165dxaxZs4TFYnG9P3nyZHHXXXc12eaxxx4TN954o+vPdrtdvPLKK6J79+5CrVaLhIQE8eKLL7a5nn79+jVZNm7cOJGZmdniNuPHjxf33ntvk2V///vfRXx8vHA4HK3ab11dnVAoFOKrr75qss6gQYPEzJkzW/zsnTt3CgDixAnnd7DVahVxcXFi+fLlLW6za9cuAUCcPHnSteznn38WAMSRI0eEEEIcOHBAKJVKcejQoRb3c6GLfxcvZLPZxLBhw8Ty5csv+fvMz88XAMT+/ftdy+x2u+jSpYtYtmyZa1lgYKB4//33m+w3PDzctU5ZWZkAILZu3ep632g0CgDim2++afVxN+eOO+4QU6ZMuezxX3fddWLWrFlNlp0+fVrExcWJ/fv3i6SkJPH666+3uP3lvptae/72654Ru8P5oDw5L9OQjxJCoM5i8/pLNIy3aq0nn3wS//3vf/HFF19g06ZNyMnJwd69e5usY7Va8cILL+Cnn37C2rVrcfz4cTzwwAMAgISEBFe3dn5+PoqKirB48WIAzl6OrKws7N69G9nZ2ZDL5fjd734HR8P/356k1+uxatUqHDhwAIsXL8ayZcvw+uuvu7WPGTNmYP78+Zg9ezYOHDiADz/8EFFRUa73+/Xrh6CgoBZft99+u2vdHTt2ICMjo8n+MzMzsWPHjhY/32w2Q6vVNlkWEBCA06dP48SJE63ar81mg91ub3Y/27Zta/Gzq6qqIJPJEBoaCgDYu3cvzpw5A7lcjuuuuw4xMTG4/fbbsX//ftc2vXv3RkREBFasWAGLxYL6+nqsWLECffv2RXJyMgDgP//5D7p164avvvoKXbt2RXJyMh588EFUVFS0WEtLnn/+eRgMBkydOvWS98xmZ6/6hcctl8uh0WiaHPewYcOwZs0aVFRUwOFwYPXq1TCZTLjpppsAABEREejduzfef/991NbWwmaz4Z133oHBYEBqamqrj7ulNg4PD2/2PSEEsrOzkZ+fj9/85jeu5Q6HAxMnTsSTTz6Jfv36tbqtrkabJj1bsmQJ/va3v6G4uBgDBw7EG2+80aT762KffPIJZs+ejePHj6Nnz5545ZVXcMcdd7S56PZib/g+5aRn5KvqrXZcM6flLnhPOfB8JnTq1n191NTUYMWKFfjXv/6FW265BQDw3nvvIT4+vsl6f/jDH1w/d+vWDX//+98xZMgQ1NTUICgoyPWFajAYXCcvALjnnnua7GflypXo0qULDhw4gGuvvbYth9dqs2bNcv2cnJyMJ554AqtXr8ZTTz3Vqu2rq6uxePFivPnmm5g8eTIAoHv37rjhhhtc66xfv/6y8zc0TjoFOMcmXBhkACAqKgpGoxH19fVN1m2UmZmJ6dOn44EHHsDIkSNRUFCA1157DYBzXEVycvIV96vX65Geno4XXngBffv2RVRUFD766CPs2LEDPXr0aLZuk8mEp59+GuPHj0dwcDAAoLCwEADw3HPPYeHChUhOTsZrr72Gm266CYcPH0Z4eDj0ej1ycnIwduxYvPDCCwCAnj17YuPGjVAqla79nDhxAp988gnef/992O12TJ8+Hffeey++/fbbFtvyYtu2bcOKFSuQl5fX7Pt9+vRBYmIiZsyYgXfeeQeBgYF4/fXXcfr0aRQVFbnW+/jjjzFu3DhERERAqVRCp9Ph888/d7WNTCbD5s2bMXbsWOj1esjlchgMBmzYsAFhYWEA0KrjvtjHH3+MXbt24Z133mmyvKqqCnFxcTCbzVAoFHjrrbfw29/+1vX+K6+8AqVSib/85S+tbqur5XbPyJo1a5CVlYW5c+di7969GDhwIDIzM13TwV5s+/btGD9+PKZOnYrc3FyMHTsWY8eObZJ0pdLYM8IBrESec/ToUVgsFqSlpbmWhYeHo3fv3k3W27NnD0aPHo3ExETo9XrceOONAICTJ09edv9HjhzB+PHj0a1bNwQHB7v+lXil7drDmjVrMHz4cERHRyMoKAizZs1y63MPHjwIs9nsCmnNSUpKQo8ePVp8xcXFXdUxPPTQQ/jzn/+MO++8E2q1Gtdffz3+53/+B4DzX/mt9c9//hNCCMTFxUGj0eDvf/87xo8f3+w+rFYr7rvvPggh8Pbbb7uWN/ZmzZw5E/fccw9SU1Px7rvvQiaT4ZNPPgEA1NfXY+rUqRg+fDh++OEHfP/997j22msxatQo1NfXu/ZjNpvx/vvvY8SIEbjpppuwYsUKbNmyBfn5+a06nurqakycOBHLli1DZGRks+uoVCp89tlnrqCk0+mwZcsW3H777U2Oe/bs2aisrMTmzZuxe/duZGVl4b777sO+ffsAOHsopk2bBoPBgO+++w47d+7E2LFjMXr0aFeoac1xX2jLli2YMmUKli1bdknvhl6vR15eHnbt2oWXXnoJWVlZyMnJAeD8/3Dx4sVYtWqVd+c0uuxFnGYMHTpUTJs2zfVnu90uYmNjxbx585pd/7777hOjRo1qsiwtLU388Y9/bPVnemrMyH1Lt4ukp78S//npTLvul8gTmrsu63A4RK3Z6vVX41iC1sjLy2syLqBRSkqK6zp9TU2NiIiIEBMmTBBbt24VBw8eFBs3bhQARG5urhBCiC1btggA4vz5803207t3b3HrrbeKzZs3iwMHDoj9+/e7NQ7EnXUvtH37dqFQKMSLL74odu3aJQ4fPiyef/55ERIS4lpnypQpYsyYMU22e/TRR11jRhqv+RcWFrb4Oddcc40IDAxs8XXbbbe51h0xYsQlYx9WrlwpgoODr3g8NptNnD59WpjNZrF+/XoBQJSWlrq935qaGnH27FkhhPP7/4477mjyvsViEWPHjhUDBgwQ5eXlTd779ttvBQDx3XffNVk+dOhQ8eyzzwohhFi+fLkwGAzCbre73jebzUKn04mPPvpICCHEnDlzhFKpbLKPuro6AUBs2rTpkpqbGzOSm5srAAiFQuF6yWQyIZPJhEKhuGRcUmVlpau9hg4dKh599FEhhBAFBQWXjCsRQohbbrnFdR7cvHmzkMvll5zjevTo4Tq3tua4G+Xk5IjAwEDxzjvvXHKszZk6daq49dZbhRBCvP76665jbHwBEHK5XCQlJTW7fXuMGXHrMo3FYsGePXswY8YM1zK5XI6MjIwWr0nu2LEDWVlZTZZlZmZe9lY6s9nsuhYHAEaj0Z0yW80h+Gwa8m0ymazVl0uk0r17d6hUKvz4449ITEwEAJw/fx6HDx929X4cOnQI586dw/z585GQkAAA2L17d5P9qNXOB1ra7XbXsnPnziE/Px/Lli1z3elwuTEK7Wn79u1ISkrCzJkzXcsax1g06tKlyyW9wHl5eVCpVACc3ewBAQHIzs7Ggw8+2OznuHOZJj09HevXr2/y/jfffIP09PQrHo9CoXD1snz00UdIT09Hly5d3N5vYGAgAgMDcf78eWzcuBGvvvqq673GHpEjR45gy5YtlzxjKTU1FRqNBvn5+a5LVVarFcePH0dSUhIA5105crm8yb/aG//c2LMyfPhw2Gw2HD16FN27dwfgvHUWgGs/V9KnTx9Xz0WjWbNmuS6tNf6eNgoJCQHg7KnbvXu361JKXV2dq8YLKRQKV70trSOXy5usc6XjBpy3995555145ZVX8PDDD7fqWBt7kgBg4sSJzY4ParzDyWNaFZsanDlzRgAQ27dvb7L8ySefFEOHDm12G5VKJT788MMmy5YsWSIMBkOLnzN37txLRs7DAz0jY5dsE0lPfyU2/VLcrvsl8gRfvpvmkUceEUlJSSI7O1vs27dPjBkzRgQFBbn+NVpaWirUarV48sknxdGjR8UXX3whevXq1aRn5PTp00Imk4lVq1aJ0tJSUV1dLex2u4iIiBD333+/OHLkiMjOzhZDhgy5Ym9HdXW1yM3Ndf3rd+HChSI3N/eS3pvL+eKLL4RSqRQfffSRKCgoEIsXLxbh4eFNekY2bNggZDKZeO+998Thw4fFnDlzRHBwcJO7aZ577jkRFhYm3nvvPVFQUCB27Nhx2btJLqewsFDodDrx5JNPioMHD4olS5YIhUIhNmzY4FrnjTfeEDfffLPrz2VlZeLtt98WBw8eFLm5ueIvf/mL0Gq14scff3Rrvxs2bBBff/21KCwsFJs2bRIDBw4UaWlprruLLBaLGDNmjIiPjxd5eXmiqKjI9TKbza79PPbYYyIuLk5s3LhRHDp0SEydOlUYDAZRUVEhhBDi4MGDQqPRiD/96U+unrD7779fhISEuHpk7Ha7GDRokPjNb34j9u7dK3bv3i3S0tLEb3/72ybt1fg7kJqaKiZMmCByc3PFL7/80mL7Nnd31Mcffyy2bNkijh49KtauXSuSkpLE3Xff7XrfYrGIHj16iBEjRogff/xRFBQUiAULFgiZTCbWrVvn+juIiIgQd999t8jLyxP5+fniiSeeECqVSuTl5bX6uL/99luh0+nEjBkzmrTvuXPnXPW8/PLLYtOmTeLo0aPiwIEDYsGCBUKpVDa5++di3ribpkOGEZPJJKqqqlyvU6dOeSSM/OuH4+KVrw+KIyXGdt0vkSf4chiprq4W999/v9DpdCIqKkq8+uqrl3SNf/jhhyI5OVloNBqRnp4uvvzyyyZhRAghnn/+eREdHS1kMpnr1t5vvvlG9O3bV2g0GjFgwACRk5NzxTDSeMnn4lfjPoVw/qOopW7pRk8++aSIiIgQQUFBYty4ceL1119vEkaEcF4yiIqKEiEhIWL69Oniz3/+8yW39r744osiKSlJqFQqkZiYKF5++eXLN+hlbNmyRaSkpAi1Wi26desm3n333SbvX3xcZWVl4vrrrxeBgYFCp9OJW265Rfzwww9u73fNmjWiW7duQq1Wi+joaDFt2jRRWVnper+527MbX1u2bHGtZ7FYxP/93/8Jg8Eg9Hq9yMjIuOQSx6ZNm1y3c4eFhYmbb75Z7Nixo8k6Z86cEXfffbfrVvEHHnigyUlZCNFsLZf7O28ujCxevFjEx8e7/u5mzZrVJFwJIcThw4fF3XffLQwGg9DpdGLAgAGX3Oq7a9cuceutt4rw8HCh1+vF9ddfL9avX+/WcU+ePLnZY7rw923mzJmiR48eQqvVirCwMJGeni5Wr17d4jEL4Z0wIhOi9ffoWSwW6HQ6fPrpp01moZs8eTIqKyvxxRdfXLJNYmIisrKymsxuN3fuXKxdu/aSGfNaYjQaERISgqqqKteoayJ/YzKZcOzYMXTt2vWSWyip/U2ePBkymaxVM5gS+bPLfTe19vzt1t00arUaqampyM7Odi1zOBzIzs5u8Zpkenp6k/WB1l/DJCKSghACOTk5ruv+RORZbo98y8rKwuTJkzF48GAMHToUixYtQm1trWtgy6RJkxAXF4d58+YBcE7Je+ONN+K1117DqFGjsHr1auzevRv/+Mc/2vdIiIjaiUwmu2QwKhF5jtthZNy4cSgrK8OcOXNQXFyMlJQUbNiwwTUhzsmTJ5uMCB42bBg+/PBDzJo1C88++yx69uyJtWvXenwyIiIiIvINbo0ZkQrHjBBxzAgRdUxeHzNCRERE1N4YRoh8jDceAEdE1Frt8Z3UsaduJCIXtVoNuVyOs2fPokuXLlCr1d59dgQR0QWEELBYLCgrK4NcLnfNktwWDCNEPkIul6Nr164oKirC2bNnpS6HiAgAoNPpkJiY6NaDFS/GMELkQ9RqNRITE2Gz2Zo8o4WISAoKhQJKpfKqe2kZRoh8jEwmg0qlcj1sjYjI13EAKxEREUmKYYSIiIgkxTBCREREkvKJMSONk8QajUaJKyEiIqLWajxvX2myd58II9XV1QCAhIQEiSshIiIid1VXVyMkJKTF933i2TQOhwNnz56FXq9v10mejEYjEhIScOrUKT7zxoPYzt7DtvYOtrN3sJ29w5PtLIRAdXU1YmNjLzsPiU/0jMjlcsTHx3ts/8HBwfxF9wK2s/ewrb2D7ewdbGfv8FQ7X65HpBEHsBIREZGkGEaIiIhIUn4dRjQaDebOnQuNRiN1KZ0a29l72NbewXb2Drazd3SEdvaJAaxERETUefl1zwgRERFJj2GEiIiIJMUwQkRERJJiGCEiIiJJdfowsmTJEiQnJ0Or1SItLQ07d+687PqffPIJ+vTpA61Wi/79+2P9+vVeqtS3udPOy5Ytw4gRIxAWFoawsDBkZGRc8e+FfuXu73Sj1atXQyaTYezYsZ4tsJNwt50rKysxbdo0xMTEQKPRoFevXvz+aAV323nRokXo3bs3AgICkJCQgOnTp8NkMnmpWt+0detWjB49GrGxsZDJZFi7du0Vt8nJycGgQYOg0WjQo0cPrFq1yrNFik5s9erVQq1Wi5UrV4pffvlFPPTQQyI0NFSUlJQ0u/73338vFAqFePXVV8WBAwfErFmzhEqlEvv27fNy5b7F3XaeMGGCWLJkicjNzRUHDx4UDzzwgAgJCRGnT5/2cuW+x922bnTs2DERFxcnRowYIe666y7vFOvD3G1ns9ksBg8eLO644w6xbds2cezYMZGTkyPy8vK8XLlvcbedP/jgA6HRaMQHH3wgjh07JjZu3ChiYmLE9OnTvVy5b1m/fr2YOXOm+OyzzwQA8fnnn192/cLCQqHT6URWVpY4cOCAeOONN4RCoRAbNmzwWI2dOowMHTpUTJs2zfVnu90uYmNjxbx585pd/7777hOjRo1qsiwtLU388Y9/9Gidvs7ddr6YzWYTer1evPfee54qsdNoS1vbbDYxbNgwsXz5cjF58mSGkVZwt53ffvtt0a1bN2GxWLxVYqfgbjtPmzZN3HzzzU2WZWVlieHDh3u0zs6kNWHkqaeeEv369WuybNy4cSIzM9NjdXXayzQWiwV79uxBRkaGa5lcLkdGRgZ27NjR7DY7duxosj4AZGZmtrg+ta2dL1ZXVwer1Yrw8HBPldkptLWtn3/+eRgMBkydOtUbZfq8trTzl19+ifT0dEybNg1RUVG49tpr8fLLL8Nut3urbJ/TlnYeNmwY9uzZ47qUU1hYiPXr1+OOO+7wSs3+QopzoU88KK8tysvLYbfbERUV1WR5VFQUDh061Ow2xcXFza5fXFzssTp9XVva+WJPP/00YmNjL/nlp6ba0tbbtm3DihUrkJeX54UKO4e2tHNhYSG+/fZb/P73v8f69etRUFCARx99FFarFXPnzvVG2T6nLe08YcIElJeX44YbboAQAjabDY888gieffZZb5TsN1o6FxqNRtTX1yMgIKDdP7PT9oyQb5g/fz5Wr16Nzz//HFqtVupyOpXq6mpMnDgRy5YtQ2RkpNTldGoOhwMGgwH/+Mc/kJqainHjxmHmzJlYunSp1KV1Kjk5OXj55Zfx1ltvYe/evfjss8+wbt06vPDCC1KXRlep0/aMREZGQqFQoKSkpMnykpISREdHN7tNdHS0W+tT29q50YIFCzB//nxs3rwZAwYM8GSZnYK7bX306FEcP34co0ePdi1zOBwAAKVSifz8fHTv3t2zRfugtvxOx8TEQKVSQaFQuJb17dsXxcXFsFgsUKvVHq3ZF7WlnWfPno2JEyfiwQcfBAD0798ftbW1ePjhhzFz5kzI5fz3dXto6VwYHBzskV4RoBP3jKjVaqSmpiI7O9u1zOFwIDs7G+np6c1uk56e3mR9APjmm29aXJ/a1s4A8Oqrr+KFF17Ahg0bMHjwYG+U6vPcbes+ffpg3759yMvLc73GjBmDkSNHIi8vDwkJCd4s32e05Xd6+PDhKCgocIU9ADh8+DBiYmIYRFrQlnauq6u7JHA0BkDBx6y1G0nOhR4bGtsBrF69Wmg0GrFq1Spx4MAB8fDDD4vQ0FBRXFwshBBi4sSJ4plnnnGt//333wulUikWLFggDh48KObOnctbe1vB3XaeP3++UKvV4tNPPxVFRUWuV3V1tVSH4DPcbeuL8W6a1nG3nU+ePCn0er3485//LPLz88VXX30lDAaDePHFF6U6BJ/gbjvPnTtX6PV68dFHH4nCwkKxadMm0b17d3HfffdJdQg+obq6WuTm5orc3FwBQCxcuFDk5uaKEydOCCGEeOaZZ8TEiRNd6zfe2vvkk0+KgwcPiiVLlvDW3qv1xhtviMTERKFWq8XQoUPFDz/84HrvxhtvFJMnT26y/scffyx69eol1Gq16Nevn1i3bp2XK/ZN7rRzUlKSAHDJa+7cud4v3Ae5+zt9IYaR1nO3nbdv3y7S0tKERqMR3bp1Ey+99JKw2Wxertr3uNPOVqtVPPfcc6J79+5Cq9WKhIQE8eijj4rz5897v3AfsmXLlma/cxvbdvLkyeLGG2+8ZJuUlBShVqtFt27dxLvvvuvRGmVCsG+LiIiIpNNpx4wQERGRb2AYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFL/H+vc2VH22UuUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot AUC for external test set\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_test, pred)\n",
    "# metrics.auc(fpr, tpr)\n",
    "auc = metrics.roc_auc_score(labels_test, pred)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08c40325-3287-42b2-a75e-3b3972f37bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082954461668751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "# Compute precision, recall, and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, pred)\n",
    "\n",
    "# Calculate AUPR\n",
    "aupr = auc(recall, precision)\n",
    "print(aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec1d1653-9598-412c-93f3-b424472aaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 0.5 as a cut off for predictions of model\n",
    "for i in range(len(pred)):\n",
    "  if (pred[i]>0.5):\n",
    "    pred[i]=1\n",
    "  else:\n",
    "    pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94fe064e-b1e1-4f4e-8ce3-ccdb2b948a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6702332519536219)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the MCC of model\n",
    "metrics.matthews_corrcoef(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0744ebba-b9b8-4e3e-a7a2-f17c4cf8030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348144225677562"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28cb9604-3049-47aa-8709-1af87801fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83     51881\n",
      "           1       0.82      0.86      0.84     51984\n",
      "\n",
      "    accuracy                           0.83    103865\n",
      "   macro avg       0.84      0.83      0.83    103865\n",
      "weighted avg       0.84      0.83      0.83    103865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report for external test set with esm\n",
    "print(classification_report(labels_test, pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bc765d9-bf64-417d-a723-e30dbd5df259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (Recall): 0.8564\n",
      "Specificity: 0.8131\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(labels_test, pred).ravel()\n",
    "\n",
    "# Calculate Sensitivity (Recall) and Specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abf9075b-86ca-4410-a336-c4c2a9faf288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45592 6289 8000 43984\n"
     ]
    }
   ],
   "source": [
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c49c1d88-c512-4ebf-a8f1-75c151ffb7ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating HLA: HLA-A01:01\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "AUC: 0.9551\n",
      "Confusion Matrix:\n",
      " [[4060  257]\n",
      " [ 731 3596]]\n",
      "Sensitivity: 0.9405\n",
      "Specificity: 0.8311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      4317\n",
      "           1       0.93      0.83      0.88      4327\n",
      "\n",
      "    accuracy                           0.89      8644\n",
      "   macro avg       0.89      0.89      0.89      8644\n",
      "weighted avg       0.89      0.89      0.89      8644\n",
      "\n",
      "\n",
      "Evaluating HLA: HLA-A02:01\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "AUC: 0.9173\n",
      "Confusion Matrix:\n",
      " [[3979  479]\n",
      " [1005 3474]]\n",
      "Sensitivity: 0.8926\n",
      "Specificity: 0.7756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84      4458\n",
      "           1       0.88      0.78      0.82      4479\n",
      "\n",
      "    accuracy                           0.83      8937\n",
      "   macro avg       0.84      0.83      0.83      8937\n",
      "weighted avg       0.84      0.83      0.83      8937\n",
      "\n",
      "\n",
      "Evaluating HLA: HLA-A24:02\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "AUC: 0.9478\n",
      "Confusion Matrix:\n",
      " [[3713  420]\n",
      " [ 587 3559]]\n",
      "Sensitivity: 0.8984\n",
      "Specificity: 0.8584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      4133\n",
      "           1       0.89      0.86      0.88      4146\n",
      "\n",
      "    accuracy                           0.88      8279\n",
      "   macro avg       0.88      0.88      0.88      8279\n",
      "weighted avg       0.88      0.88      0.88      8279\n",
      "\n",
      "\n",
      "Evaluating HLA: HLA-B08:01\n",
      "\u001b[1m1224/1224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "AUC: 0.9429\n",
      "Confusion Matrix:\n",
      " [[17134  2421]\n",
      " [ 2556 17039]]\n",
      "Sensitivity: 0.8762\n",
      "Specificity: 0.8696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87     19555\n",
      "           1       0.88      0.87      0.87     19595\n",
      "\n",
      "    accuracy                           0.87     39150\n",
      "   macro avg       0.87      0.87      0.87     39150\n",
      "weighted avg       0.87      0.87      0.87     39150\n",
      "\n",
      "\n",
      "Evaluating HLA: HLA-B18:01\n",
      "\u001b[1m1215/1215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "AUC: 0.9437\n",
      "Confusion Matrix:\n",
      " [[17642  1776]\n",
      " [ 3398 16039]]\n",
      "Sensitivity: 0.9085\n",
      "Specificity: 0.8252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87     19418\n",
      "           1       0.90      0.83      0.86     19437\n",
      "\n",
      "    accuracy                           0.87     38855\n",
      "   macro avg       0.87      0.87      0.87     38855\n",
      "weighted avg       0.87      0.87      0.87     38855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#external / HLA\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load peptide embeddings for test set\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\external_set.csv\")\n",
    "\n",
    "# Preprocess HLA names (remove \"*\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "\n",
    "# Extract unique HLAs for evaluation\n",
    "new_hla_list = test[\"HLA\"].unique()\n",
    "\n",
    "# Iterate over each unique HLA in the test set\n",
    "for HLA in new_hla_list:\n",
    "    print(f\"\\nEvaluating HLA: {HLA}\")\n",
    "    \n",
    "    # Subset test data for the current HLA\n",
    "    test_subset = test[test[\"HLA\"] == HLA]\n",
    "\n",
    "    # Get peptide and HLA embeddings\n",
    "    peptides_test = np.array([peptide_embeddings_test[p] for p in test_subset[\"peptide\"]])\n",
    "    hlas_test = np.array([hla_df.loc[h].values for h in test_subset[\"HLA\"]])\n",
    "    labels_test = test_subset[\"label\"].values\n",
    "\n",
    "    # Reshape for model input\n",
    "    peptides_test = peptides_test.reshape(-1, 320)\n",
    "    hlas_test = hlas_test.reshape(-1, 180)\n",
    "\n",
    "    # Model prediction\n",
    "    pred_probs = siamese_net.predict([peptides_test, hlas_test])\n",
    "    auc = roc_auc_score(labels_test, pred_probs)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels_test, pred_labels)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    try:\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"], output_dict=True)\n",
    "        print(classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "        # Plot classification report heatmap\n",
    "        # sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.title(f\"Classification Report for {HLA}\")\n",
    "        # plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping HLA {HLA} due to error: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f8c0c51-f3e3-45c9-9d20-8a02f0ff8aed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating length: 8\n",
      "\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "AUC: 0.9618\n",
      "Confusion Matrix:\n",
      " [[14560  1551]\n",
      " [ 1642 14478]]\n",
      "Sensitivity: 0.9037\n",
      "Specificity: 0.8981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     16111\n",
      "           1       0.90      0.90      0.90     16120\n",
      "\n",
      "    accuracy                           0.90     32231\n",
      "   macro avg       0.90      0.90      0.90     32231\n",
      "weighted avg       0.90      0.90      0.90     32231\n",
      "\n",
      "\n",
      "Evaluating length: 9\n",
      "\u001b[1m1448/1448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "AUC: 0.9478\n",
      "Confusion Matrix:\n",
      " [[20798  2327]\n",
      " [ 3541 19665]]\n",
      "Sensitivity: 0.8994\n",
      "Specificity: 0.8474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88     23125\n",
      "           1       0.89      0.85      0.87     23206\n",
      "\n",
      "    accuracy                           0.87     46331\n",
      "   macro avg       0.87      0.87      0.87     46331\n",
      "weighted avg       0.87      0.87      0.87     46331\n",
      "\n",
      "\n",
      "Evaluating length: 10\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "AUC: 0.9385\n",
      "Confusion Matrix:\n",
      " [[4601  580]\n",
      " [ 810 4375]]\n",
      "Sensitivity: 0.8881\n",
      "Specificity: 0.8438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      5181\n",
      "           1       0.88      0.84      0.86      5185\n",
      "\n",
      "    accuracy                           0.87     10366\n",
      "   macro avg       0.87      0.87      0.87     10366\n",
      "weighted avg       0.87      0.87      0.87     10366\n",
      "\n",
      "\n",
      "Evaluating length: 11\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "AUC: 0.9254\n",
      "Confusion Matrix:\n",
      " [[3130  396]\n",
      " [ 599 2937]]\n",
      "Sensitivity: 0.8877\n",
      "Specificity: 0.8306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86      3526\n",
      "           1       0.88      0.83      0.86      3536\n",
      "\n",
      "    accuracy                           0.86      7062\n",
      "   macro avg       0.86      0.86      0.86      7062\n",
      "weighted avg       0.86      0.86      0.86      7062\n",
      "\n",
      "\n",
      "Evaluating length: 12\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "AUC: 0.8623\n",
      "Confusion Matrix:\n",
      " [[1457  184]\n",
      " [ 527 1114]]\n",
      "Sensitivity: 0.8879\n",
      "Specificity: 0.6789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80      1641\n",
      "           1       0.86      0.68      0.76      1641\n",
      "\n",
      "    accuracy                           0.78      3282\n",
      "   macro avg       0.80      0.78      0.78      3282\n",
      "weighted avg       0.80      0.78      0.78      3282\n",
      "\n",
      "\n",
      "Evaluating length: 13\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "AUC: 0.7860\n",
      "Confusion Matrix:\n",
      " [[1166  178]\n",
      " [ 606  737]]\n",
      "Sensitivity: 0.8676\n",
      "Specificity: 0.5488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.87      0.75      1344\n",
      "           1       0.81      0.55      0.65      1343\n",
      "\n",
      "    accuracy                           0.71      2687\n",
      "   macro avg       0.73      0.71      0.70      2687\n",
      "weighted avg       0.73      0.71      0.70      2687\n",
      "\n",
      "\n",
      "Evaluating length: 14\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "AUC: 0.7117\n",
      "Confusion Matrix:\n",
      " [[816 137]\n",
      " [552 401]]\n",
      "Sensitivity: 0.8562\n",
      "Specificity: 0.4208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.86      0.70       953\n",
      "           1       0.75      0.42      0.54       953\n",
      "\n",
      "    accuracy                           0.64      1906\n",
      "   macro avg       0.67      0.64      0.62      1906\n",
      "weighted avg       0.67      0.64      0.62      1906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# external / length\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load peptide embeddings for test set\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\external_set.csv\")\n",
    "\n",
    "# Preprocess HLA names (remove \"*\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "\n",
    "# Extract unique HLAs for evaluation\n",
    "length_list = test[\"length\"].unique()\n",
    "\n",
    "# Iterate over each unique HLA in the test set\n",
    "for length in length_list:\n",
    "    print(f\"\\nEvaluating length: {length}\")\n",
    "    \n",
    "    # Subset test data for the current HLA\n",
    "    test_subset = test[test[\"length\"] == length]\n",
    "\n",
    "    # Get peptide and HLA embeddings\n",
    "    peptides_test = np.array([peptide_embeddings_test[p] for p in test_subset[\"peptide\"]])\n",
    "    hlas_test = np.array([hla_df.loc[h].values for h in test_subset[\"HLA\"]])\n",
    "    labels_test = test_subset[\"label\"].values\n",
    "\n",
    "    # Reshape for model input\n",
    "    peptides_test = peptides_test.reshape(-1, 320)\n",
    "    hlas_test = hlas_test.reshape(-1, 180)\n",
    "\n",
    "    # Model prediction\n",
    "    pred_probs = siamese_net.predict([peptides_test, hlas_test])\n",
    "    auc = roc_auc_score(labels_test, pred_probs)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels_test, pred_labels)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    try:\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"], output_dict=True)\n",
    "        print(classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "        # Plot classification report heatmap\n",
    "        # sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.title(f\"Classification Report for {HLA}\")\n",
    "        # plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping HLA {HLA} due to error: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "252d86af-2c41-460d-bd48-fcd9876fe48d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating length: 8\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "AUC: 0.9751\n",
      "Confusion Matrix:\n",
      " [[3414  339]\n",
      " [ 231 3524]]\n",
      "Sensitivity: 0.9097\n",
      "Specificity: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      3753\n",
      "           1       0.91      0.94      0.93      3755\n",
      "\n",
      "    accuracy                           0.92      7508\n",
      "   macro avg       0.92      0.92      0.92      7508\n",
      "weighted avg       0.92      0.92      0.92      7508\n",
      "\n",
      "\n",
      "Evaluating length: 9\n",
      "\u001b[1m3612/3612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n",
      "AUC: 0.9777\n",
      "Confusion Matrix:\n",
      " [[52051  5595]\n",
      " [ 2715 55203]]\n",
      "Sensitivity: 0.9029\n",
      "Specificity: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     57646\n",
      "           1       0.91      0.95      0.93     57918\n",
      "\n",
      "    accuracy                           0.93    115564\n",
      "   macro avg       0.93      0.93      0.93    115564\n",
      "weighted avg       0.93      0.93      0.93    115564\n",
      "\n",
      "\n",
      "Evaluating length: 10\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "AUC: 0.9719\n",
      "Confusion Matrix:\n",
      " [[12149  1337]\n",
      " [  888 12625]]\n",
      "Sensitivity: 0.9009\n",
      "Specificity: 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92     13486\n",
      "           1       0.90      0.93      0.92     13513\n",
      "\n",
      "    accuracy                           0.92     26999\n",
      "   macro avg       0.92      0.92      0.92     26999\n",
      "weighted avg       0.92      0.92      0.92     26999\n",
      "\n",
      "\n",
      "Evaluating length: 11\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "AUC: 0.9673\n",
      "Confusion Matrix:\n",
      " [[5152  634]\n",
      " [ 476 5319]]\n",
      "Sensitivity: 0.8904\n",
      "Specificity: 0.9179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90      5786\n",
      "           1       0.89      0.92      0.91      5795\n",
      "\n",
      "    accuracy                           0.90     11581\n",
      "   macro avg       0.90      0.90      0.90     11581\n",
      "weighted avg       0.90      0.90      0.90     11581\n",
      "\n",
      "\n",
      "Evaluating length: 12\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "AUC: 0.9392\n",
      "Confusion Matrix:\n",
      " [[2359  354]\n",
      " [ 372 2345]]\n",
      "Sensitivity: 0.8695\n",
      "Specificity: 0.8631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      2713\n",
      "           1       0.87      0.86      0.87      2717\n",
      "\n",
      "    accuracy                           0.87      5430\n",
      "   macro avg       0.87      0.87      0.87      5430\n",
      "weighted avg       0.87      0.87      0.87      5430\n",
      "\n",
      "\n",
      "Evaluating length: 13\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "AUC: 0.9158\n",
      "Confusion Matrix:\n",
      " [[1220  176]\n",
      " [ 300 1096]]\n",
      "Sensitivity: 0.8739\n",
      "Specificity: 0.7851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84      1396\n",
      "           1       0.86      0.79      0.82      1396\n",
      "\n",
      "    accuracy                           0.83      2792\n",
      "   macro avg       0.83      0.83      0.83      2792\n",
      "weighted avg       0.83      0.83      0.83      2792\n",
      "\n",
      "\n",
      "Evaluating length: 14\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "AUC: 0.8564\n",
      "Confusion Matrix:\n",
      " [[635 147]\n",
      " [203 579]]\n",
      "Sensitivity: 0.8120\n",
      "Specificity: 0.7404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78       782\n",
      "           1       0.80      0.74      0.77       782\n",
      "\n",
      "    accuracy                           0.78      1564\n",
      "   macro avg       0.78      0.78      0.78      1564\n",
      "weighted avg       0.78      0.78      0.78      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# independent/length\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load peptide embeddings for test set\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\independent_set.csv\")\n",
    "\n",
    "# Preprocess HLA names (remove \"*\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "\n",
    "# Extract unique HLAs for evaluation\n",
    "length_list = test[\"length\"].unique()\n",
    "\n",
    "# Iterate over each unique HLA in the test set\n",
    "for length in length_list:\n",
    "    print(f\"\\nEvaluating length: {length}\")\n",
    "    \n",
    "    # Subset test data for the current HLA\n",
    "    test_subset = test[test[\"length\"] == length]\n",
    "\n",
    "    # Get peptide and HLA embeddings\n",
    "    peptides_test = np.array([peptide_embeddings_test[p] for p in test_subset[\"peptide\"]])\n",
    "    hlas_test = np.array([hla_df.loc[h].values for h in test_subset[\"HLA\"]])\n",
    "    labels_test = test_subset[\"label\"].values\n",
    "\n",
    "    # Reshape for model input\n",
    "    peptides_test = peptides_test.reshape(-1, 320)\n",
    "    hlas_test = hlas_test.reshape(-1, 180)\n",
    "\n",
    "    # Model prediction\n",
    "    pred_probs = siamese_net.predict([peptides_test, hlas_test])\n",
    "    auc = roc_auc_score(labels_test, pred_probs)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels_test, pred_labels)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    try:\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"], output_dict=True)\n",
    "        print(classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "        # Plot classification report heatmap\n",
    "        # sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.title(f\"Classification Report for {HLA}\")\n",
    "        # plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping HLA {HLA} due to error: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c47a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6da896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HLA IC feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2258b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_feature():\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    alldata0 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold0.csv\")\n",
    "    alldata1 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold1.csv\")\n",
    "    alldata2 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold2.csv\")\n",
    "    alldata3 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold3.csv\")\n",
    "    alldata4 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold4.csv\")\n",
    "    # Correct concatenation syntax:\n",
    "    alldata = pd.concat([alldata0, alldata1, alldata2, alldata3, alldata4])\n",
    "    alldata['HLA'] = alldata['HLA'].str.replace('*', '')\n",
    "    # data_train, data_val = train_test_split(alldata, test_size=0.2, random_state=42)\n",
    "    # data_train.to_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\train_data.csv\")\n",
    "    # data_val.to_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\validation_data.csv\")\n",
    "    \n",
    "    \n",
    "    HLA_list = alldata['HLA'].unique()\n",
    "    test_keys = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    test_values = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "    res = {}\n",
    "    for key in test_keys:\n",
    "        for value in test_values:\n",
    "            res[key] = value\n",
    "            test_values.remove(value)\n",
    "            break\n",
    "    column = [0]*181\n",
    "    column[0] = 'HLA'\n",
    "    for i in range(1, 181):\n",
    "      column[i] = (str(i+319))\n",
    "    \n",
    "    \n",
    "    antropy_data = pd.DataFrame(columns = column)\n",
    "    for HLA in tqdm(HLA_list):\n",
    "      data = alldata[alldata.HLA == HLA]\n",
    "      data_0 = data[data.label == 0]\n",
    "      data_1 = data[data.label == 1]\n",
    "\n",
    "      data_0 = data_0[data_0.length == 9]\n",
    "      data_1 = data_1[data_1.length == 9]\n",
    "\n",
    "      #character profile\n",
    "      arr0 = [[0 for _ in range(9)] for _ in range(len(data_0))]\n",
    "      j=0\n",
    "      for _, row in data_0.iterrows():\n",
    "        for i in range(9):\n",
    "          arr0[j][i] = row['peptide'][i]\n",
    "        j=j+1\n",
    "\n",
    "      arr1 = [[0 for _ in range(9)] for _ in range(len(data_1))]\n",
    "      j=0\n",
    "      for _, row in data_1.iterrows():\n",
    "        for i in range(9):\n",
    "          arr1[j][i] = row['peptide'][i]\n",
    "        j=j+1\n",
    "\n",
    "      arr_0 = [[pow(10, -10) for _ in range(9)] for _ in range(20)]\n",
    "      arr_1 = [[pow(10, -10) for _ in range(9)] for _ in range(20)]\n",
    "\n",
    "      #probability profile\n",
    "      for i in range(len(arr0)):\n",
    "        for j in range(9):\n",
    "          char = res[arr0[i][j]]\n",
    "          arr_0[char][j] = arr_0[char][j] + (1/len(data_0))\n",
    "\n",
    "      for i in range(len(arr1)):\n",
    "        for j in range(9):\n",
    "          char = res[arr1[i][j]]\n",
    "          arr_1[char][j] = arr_1[char][j] + (1/len(data_1))\n",
    "\n",
    "      #antropy matrix\n",
    "      antropy = [[0 for _ in range(9)] for _ in range(20)]\n",
    "\n",
    "      for i in range(20):\n",
    "        for j in range(9):\n",
    "          antropy[i][j] = arr_1[i][j]*(math.log((arr_1[i][j]/arr_0[i][j]),20))\n",
    "      \n",
    "      flattened_array = [element for column in zip(*antropy) for element in column]\n",
    "  \n",
    "      # flattened_array = [element for row in antropy for element in row]\n",
    "      feature_antropy = [0]*181\n",
    "      feature_antropy[0] = HLA\n",
    "      feature_antropy[1:] = flattened_array\n",
    "      new_df = pd.DataFrame(np.array(feature_antropy).reshape(1, -1), columns=column)\n",
    "      antropy_data = pd.concat([antropy_data, new_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    antropy_data.to_csv(r\"C:\\Users\\asus\\Desktop\\final_entropy_column.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa482684-2e44-4870-9f1a-8563ff2d9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112/112 [02:59<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "IC_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "372dc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [07:41<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "def calculate_similarity(seq1, seq2, matrix):\n",
    "    alignments = pairwise2.align.globalds(seq1, seq2, matrix, -10, -0.5)\n",
    "    score = alignments[0][2]\n",
    "    max_score = min(len(seq1), len(seq2)) * matrix[('A', 'A')]\n",
    "    return score / max_score\n",
    "\n",
    "def calculate_percentage_similarities(sequences):\n",
    "    matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "    n = len(sequences)\n",
    "    similarity_matrix = [[0] * n for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = calculate_similarity(sequences[i], sequences[j], matrix)\n",
    "\n",
    "    percentages = []\n",
    "    for i in range(n):\n",
    "        total_similarity = sum(similarity_matrix[i])\n",
    "        percentages.append([similarity_matrix[i][j] / total_similarity for j in range(n)])\n",
    "\n",
    "    return percentages\n",
    "\n",
    "# Example usage\n",
    "header_hlaESM = [0]*180\n",
    "for i in range(1, 181):\n",
    "  header_hlaESM[i-1] = (str(i+319))\n",
    "\n",
    "new_column = [0]*181\n",
    "new_column[1:181] = header_hlaESM\n",
    "new_column[0] = 'HLA'\n",
    "antropy = pd.read_csv(r'C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv', names = new_column, header=0)\n",
    "unique_HLA = pd.read_csv(r'C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\unique_HLA.csv')\n",
    "antropy = pd.merge(antropy, unique_HLA, on=['HLA'])\n",
    "\n",
    "sequences = antropy['HLA_sequence'].tolist()\n",
    "percentages = calculate_percentage_similarities(sequences)\n",
    "temp = antropy.copy()\n",
    "antropy_data = pd.DataFrame(columns = new_column)\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    r=0\n",
    "    new_feature=[0]*181\n",
    "    new_feature[0] = sequences[i]\n",
    "    for j in range(320,500):\n",
    "        temp[str(j)] = antropy[str(j)]*percentages[i]\n",
    "        for z in range(len(sequences)):\n",
    "            try:\n",
    "                new_feature[j-319] = new_feature[j-319]+(float(temp[temp.HLA_sequence==sequences[z]][str(j)])/(len(sequences)-1))\n",
    "            except:\n",
    "                if r==0:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[0])/(len(sequences)-1))\n",
    "                    r=1\n",
    "                else:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[1])/(len(sequences)-1))\n",
    "                    \n",
    "\n",
    "    new_df = pd.DataFrame(np.array(new_feature).reshape(1, -1), columns=new_column)\n",
    "    antropy_data = antropy_data.append(new_df, ignore_index=True)\n",
    "    \n",
    "antropy_data['HLA']=antropy['HLA']\n",
    "antropy_data.to_csv(r\"C:\\Users\\asus\\Desktop\\Maryam\\esm320\\final_entropy_removed_HLA.csv\")\n",
    "\n",
    "    # for i, seq_percentages in enumerate(percentages):\n",
    "#     print(f\"Sequence {i+1} similarities: {seq_percentages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635f2a0-5b5f-41b5-a49c-c8f968177cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biopython\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "def calculate_similarity(seq1, seq2, matrix):\n",
    "    alignments = pairwise2.align.globalds(seq1, seq2, matrix, -10, -0.5)\n",
    "    score = alignments[0][2]\n",
    "    max_score = min(len(seq1), len(seq2)) * matrix[('A', 'A')]\n",
    "    return score / max_score\n",
    "\n",
    "def calculate_percentage_similarities(sequences):\n",
    "    matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "    n = len(sequences)\n",
    "    similarity_matrix = [[0] * n for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = calculate_similarity(sequences[i], sequences[j], matrix)\n",
    "\n",
    "    percentages = []\n",
    "    for i in range(n):\n",
    "        total_similarity = sum(similarity_matrix[i])\n",
    "        percentages.append([similarity_matrix[i][j] / total_similarity for j in range(n)])\n",
    "\n",
    "    return percentages\n",
    "\n",
    "header_hlaESM = [0]*180\n",
    "for i in range(1, 181):\n",
    "  header_hlaESM[i-1] = (str(i+319))\n",
    "\n",
    "new_column = [0]*181\n",
    "new_column[1:181] = header_hlaESM\n",
    "new_column[0] = 'HLA'\n",
    "antropy = pd.read_csv(r'C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv', names = new_column, header=0)\n",
    "\n",
    "sequences = antropy['HLA_sequence'].tolist() ## should added a newly HLA sequence to this list\n",
    "percentages = calculate_percentage_similarities(sequences)\n",
    "temp = antropy.copy()\n",
    "antropy_data = pd.DataFrame(columns = new_column)\n",
    "\n",
    "## and just calculate new_feature just for our new HLA and returne this feature for new HLA\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    r=0\n",
    "    new_feature=[0]*181\n",
    "    new_feature[0] = sequences[i]\n",
    "    for j in range(320,500):\n",
    "        temp[str(j)] = antropy[str(j)]*percentages[i]\n",
    "        for z in range(len(sequences)):\n",
    "            try:\n",
    "                new_feature[j-319] = new_feature[j-319]+(float(temp[temp.HLA_sequence==sequences[z]][str(j)])/(len(sequences)-1))\n",
    "            except:\n",
    "                if r==0:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[0])/(len(sequences)-1))\n",
    "                    r=1\n",
    "                else:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[1])/(len(sequences)-1))\n",
    "                    \n",
    "\n",
    "    new_df = pd.DataFrame(np.array(new_feature).reshape(1, -1), columns=new_column)\n",
    "    antropy_data = antropy_data.append(new_df, ignore_index=True)\n",
    "    \n",
    "antropy_data['HLA']=antropy['HLA']\n",
    "antropy_data.to_csv(r\"C:\\Users\\asus\\Desktop\\Maryam\\esm320\\final_entropy_removed_HLA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca1adc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ESM embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1068292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fair-esm\n",
      "  Using cached fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
      "Installing collected packages: fair-esm\n",
      "Successfully installed fair-esm-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asus\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca0ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESM\n",
    "# !pip install fair-esm #if it's needed\n",
    "import esm # pip install fair-esm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def make_esm_representations(df,col, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #df = pd.read_excel(input_file_name)\n",
    "    peptides_list = df[col].tolist()\n",
    "    esm_model, alphabet = model\n",
    "    esm_model = esm_model.to(device)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    peptides_list = [(\"\", peptides_list[i]) for i in range(len(peptides_list))]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(peptides_list)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Extract per-residue representations (on CPU or CUDA)\n",
    "    results = esm_model(batch_tokens.to(device), repr_layers=[6], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "    sequence_representations = torch.stack(sequence_representations)\n",
    "    df = pd.DataFrame(sequence_representations.cpu().detach().numpy())\n",
    "    return df\n",
    "    #df.to_excel(output_file_name, index=False)\n",
    "    #print(f'{output_file_name} file made')\n",
    "\n",
    "def pretrained_model(dim):\n",
    "    assert dim in [5120,2560,1280,640,480,320]\n",
    "    if dim==5120:\n",
    "        return esm.pretrained.esm2_t48_15B_UR50D()\n",
    "    elif dim==2560:\n",
    "        return esm.pretrained.esm2_t36_3B_UR50D()\n",
    "    elif dim==1280:\n",
    "        return esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    elif dim==640:\n",
    "        return esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    elif dim==480:\n",
    "        return esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    elif dim==320:\n",
    "        return esm.pretrained.esm2_t6_8M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46fd18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     \"HLA\": ['HLA-A01:01', 'HLA-A02:01', 'HLA-A24:02', 'HLA-B08:01', 'HLA-B18:01'],\n",
    "#     \"peptide\": ['KVYLRVRPLL', 'KVYLRVRPLL', 'KVYLRVRPLL', 'KVYLRVRPLL', 'KVYLRVRPLL']\n",
    "# }\n",
    "my_set = pd.read_csv(r'C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\merged_neoantigen_data.csv')\n",
    "# my_test = pd.DataFrame(data)\n",
    "my_set['peptide'] = my_set['peptide'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284019a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample:\n",
    "feature = make_esm_representations(my_set, 'peptide', model=pretrained_model(320))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
