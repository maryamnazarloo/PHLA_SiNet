{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68ad709",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2565c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix, matthews_corrcoef\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU, Concatenate, Multiply, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "# import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import log\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "import csv\n",
    "from urllib.request import urlretrieve\n",
    "import pickle as pickle\n",
    "import os\n",
    "import gzip\n",
    "from collections.abc import MutableMapping\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469c180-ff05-4cc7-93e3-9e1f6c8b3006",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# all 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e03d11-071f-4de5-944b-1c4369a53214",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bd4df-fc94-4bae-a449-d706123e15e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define Swish activation function\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load HLA features (drop non-feature columns)\n",
    "hla_df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\final_entropy_column.csv\")\n",
    "hla_df = hla_df.drop(columns=[\"Unnamed: 0\"])  # Correct way to drop a column\n",
    "hla_df = hla_df.set_index(\"HLA\")  # Set HLA column as index\n",
    "\n",
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\train_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings = pickle.load(f)\n",
    "\n",
    "# Load all folds\n",
    "folds = [\n",
    "    pd.read_csv(rf\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold{i}.csv\")\n",
    "    for i in range(5)\n",
    "]\n",
    "# Remove '*' from HLA names\n",
    "for fold in folds:\n",
    "    fold[\"HLA\"] = fold[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "# Assume df1, df2, df3, df4, df5 are your data frames\n",
    "# merged_df = pd.concat([folds[0], folds[1], folds[2], folds[3], folds[4]], axis=0)  # axis=0 for row-wise\n",
    "merged_df = folds[0]  # axis=0 for row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988ff9a-5e9f-4387-940f-ef9b0b4553cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input shapes\n",
    "input_shape1 = (320,)  # Peptide\n",
    "input_shape2 = (180,)  # HLA\n",
    "\n",
    "# Define model\n",
    "left_input = Input(shape=input_shape1)\n",
    "right_input = Input(shape=input_shape2)\n",
    "\n",
    "encoded_l = Dense(256, activation=swish)(left_input)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "encoded_l = Dense(128, activation=swish)(encoded_l)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "\n",
    "encoded_r = Dense(128, activation=swish)(right_input)\n",
    "encoded_r = Dropout(0.2)(encoded_r)\n",
    "\n",
    "L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "concatenated = Dense(64, activation=swish)(L1_distance)\n",
    "concatenated = Dropout(0.2)(concatenated)\n",
    "concatenated = Dense(32, activation=swish)(concatenated)\n",
    "\n",
    "prediction = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f17c5-68b1-42fd-9531-cc6216653971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size = 512  # Fixed batch size\n",
    "initial_lr = 0.001  # Fixed learning rate\n",
    "epochs = 10  # Total epochs\n",
    "\n",
    "# Assume `fold` is your single fold of data\n",
    "# print(\"Training on the single fold...\")\n",
    "\n",
    "# Prepare the data\n",
    "peptides = np.array([peptide_embeddings[p] for p in merged_df[\"peptide\"]])\n",
    "hlas = np.array([hla_df.loc[h].values for h in merged_df[\"HLA\"]])\n",
    "labels = merged_df[\"label\"].values\n",
    "\n",
    "# Training Loop\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "#     # Train the model for one epoch\n",
    "#     history = siamese_net.fit(\n",
    "#         [peptides, hlas], labels,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=1,  # Train for one epoch at a time\n",
    "#         verbose=1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d707a-99c9-4194-a6dc-7e687562f211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"loss\",  # Monitor training loss\n",
    "    patience=10,  # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True,  # Restore model to best epoch\n",
    "    min_delta=0.0001,  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "history = siamese_net.fit(\n",
    "        [peptides, hlas], labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50,  \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ed472-b716-449a-a151-964877f17f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "siamese_net.save(r\"C:\\Users\\asus\\Desktop\\siamese_all_5_fold_new_feature.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4cbdd-d22a-4df5-bf4f-de5adbe566fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting training loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], 'r', label='train_loss')\n",
    "plt.plot(history.history['accuracy'], 'y', label='train_accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss and accuracy checking')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611280b2-1a70-440e-a91c-9ba872d28cb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d383c-f74c-49e2-8c08-0f8f1b958cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define Swish activation function\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load HLA features (drop non-feature columns)\n",
    "hla_df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv\")\n",
    "hla_df = hla_df.drop(columns=[\"Unnamed: 0\"])  # Correct way to drop a column\n",
    "hla_df = hla_df.set_index(\"HLA\")  # Set HLA column as index\n",
    "\n",
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\train_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings = pickle.load(f)\n",
    "\n",
    "# Load all folds\n",
    "folds = [\n",
    "    pd.read_csv(rf\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold{i}.csv\")\n",
    "    for i in range(5)\n",
    "]\n",
    "# Remove '*' from HLA names\n",
    "for fold in folds:\n",
    "    fold[\"HLA\"] = fold[\"HLA\"].str.replace(\"*\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57587f-0798-4029-943a-113efd72d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input shapes\n",
    "input_shape1 = (320,)  # Peptide\n",
    "input_shape2 = (180,)  # HLA\n",
    "\n",
    "# Define model\n",
    "left_input = Input(shape=input_shape1)\n",
    "right_input = Input(shape=input_shape2)\n",
    "\n",
    "encoded_l = Dense(256, activation=swish)(left_input)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "encoded_l = Dense(128, activation=swish)(encoded_l)\n",
    "encoded_l = Dropout(0.2)(encoded_l)\n",
    "\n",
    "encoded_r = Dense(128, activation=swish)(right_input)\n",
    "encoded_r = Dropout(0.2)(encoded_r)\n",
    "\n",
    "L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "concatenated = Dense(64, activation=swish)(L1_distance)\n",
    "concatenated = Dropout(0.2)(concatenated)\n",
    "concatenated = Dense(32, activation=swish)(concatenated)\n",
    "\n",
    "prediction = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0dc6d-80d9-44c1-8a3c-bb40ade51415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, \n",
    "                            matthews_corrcoef, accuracy_score, \n",
    "                            confusion_matrix, classification_report)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Initialize metric storage\n",
    "metrics = {\n",
    "    'AUC': [],\n",
    "    'AUPR': [],\n",
    "    'MCC': [],\n",
    "    'ACC': [],\n",
    "    'Specificity': [],\n",
    "    'Sensitivity': []\n",
    "}\n",
    "\n",
    "for test_fold_idx in range(5):\n",
    "    print(f\"\\n\\n{'='*50}\")\n",
    "    print(f\"=== Training on 4 folds, Testing on Fold {test_fold_idx + 1} ===\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Combine 4 folds for training\n",
    "    train_peptides = []\n",
    "    train_hlas = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for fold_idx in range(5):\n",
    "        if fold_idx != test_fold_idx:\n",
    "            fold = folds[fold_idx]\n",
    "            train_peptides.extend(fold['peptide'])\n",
    "            train_hlas.extend(fold['HLA'])\n",
    "            train_labels.extend(fold['label'])\n",
    "    \n",
    "    # Use current fold for testing\n",
    "    test_fold = folds[test_fold_idx]\n",
    "    test_peptides = test_fold['peptide']\n",
    "    test_hlas = test_fold['HLA']\n",
    "    test_labels = test_fold['label']\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    X_train_pep = np.array([peptide_embeddings[p] for p in train_peptides])\n",
    "    X_train_hla = np.array([hla_df.loc[h].values for h in train_hlas])\n",
    "    y_train = np.array(train_labels)\n",
    "    \n",
    "    X_test_pep = np.array([peptide_embeddings[p] for p in test_peptides])\n",
    "    X_test_hla = np.array([hla_df.loc[h].values for h in test_hlas])\n",
    "    y_test = np.array(test_labels)\n",
    "    \n",
    "    # Recompile model for fresh training\n",
    "    siamese_net.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training samples: {len(X_train_pep)}, Test samples: {len(X_test_pep)}\")\n",
    "    siamese_net.fit(\n",
    "        [X_train_pep, X_train_hla], y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict probabilities and classes\n",
    "    y_pred_probs = siamese_net.predict([X_test_pep, X_test_hla]).flatten()\n",
    "    y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel()\n",
    "    \n",
    "    metrics['AUC'].append(roc_auc_score(y_test, y_pred_probs))\n",
    "    metrics['AUPR'].append(average_precision_score(y_test, y_pred_probs))\n",
    "    metrics['MCC'].append(matthews_corrcoef(y_test, y_pred_classes))\n",
    "    metrics['ACC'].append(accuracy_score(y_test, y_pred_classes))\n",
    "    metrics['Specificity'].append(tn / (tn + fp))\n",
    "    metrics['Sensitivity'].append(tp / (tp + fn))\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"Fold {test_fold_idx + 1} Detailed Results:\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes, digits=4))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    # Additional Metrics\n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"AUC: {metrics['AUC'][-1]:.4f}\")\n",
    "    print(f\"AUPR: {metrics['AUPR'][-1]:.4f}\")\n",
    "    print(f\"MCC: {metrics['MCC'][-1]:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['ACC'][-1]:.4f}\")\n",
    "    print(f\"Specificity: {metrics['Specificity'][-1]:.4f}\")\n",
    "    print(f\"Sensitivity: {metrics['Sensitivity'][-1]:.4f}\")\n",
    "\n",
    "# Calculate and display final results\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"=== Final 5-Fold Cross-Validation Results ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nIndividual Fold Results:\")\n",
    "print(\"Fold\\tAUC\\tAUPR\\tMCC\\tACC\\tSpec\\tSens\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}\\t\" + \"\\t\".join(f\"{metrics[metric][i]:.4f}\" \n",
    "          for metric in ['AUC', 'AUPR', 'MCC', 'ACC', 'Specificity', 'Sensitivity']))\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(\"Metric\\tMean\\tStd\")\n",
    "for metric in metrics:\n",
    "    print(f\"{metric}\\t{np.mean(metrics[metric]):.4f}\\t{np.std(metrics[metric]):.4f}\")\n",
    "\n",
    "# Save all results to a dictionary for further analysis\n",
    "results = {\n",
    "    'per_fold_metrics': metrics,\n",
    "    'average_metrics': {metric: np.mean(values) for metric, values in metrics.items()},\n",
    "    'std_metrics': {metric: np.std(values) for metric, values in metrics.items()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9bbb1-73b9-47d7-a698-f582aec230d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['average_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404fe4a-951d-4c41-91ba-1b4422b9a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['std_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edc84a-ed65-48a0-b57f-adfc958d0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['per_fold_metrics']['AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679f05f-c71b-4ac0-8e27-ff34a6906c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['per_fold_metrics']['AUPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88426ccd-eff6-4c5b-881f-2482419f0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['per_fold_metrics']['MCC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b53b9b-ecf9-41f5-b991-a4a2edb160ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['per_fold_metrics']['ACC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a2db0-dd4e-49ac-bc80-346aabf5f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['per_fold_metrics']['Specificity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bd122-a952-423f-9dc9-d46c2dfd69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['per_fold_metrics']['Sensitivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91450b-2537-4a2a-9380-985880db4941",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d73c67-ca92-4362-9d99-7da78ea16b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\external_set.csv\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "peptides_test = np.array([peptide_embeddings_test[p] for p in test[\"peptide\"]])\n",
    "hlas_test = np.array([hla_df.loc[h].values for h in test[\"HLA\"]])\n",
    "labels_test = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34a1e2-3c85-487b-abee-091e93638099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peptide embeddings\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\independent_set.csv\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "peptides_test = np.array([peptide_embeddings_test[p] for p in test[\"peptide\"]])\n",
    "hlas_test = np.array([hla_df.loc[h].values for h in test[\"HLA\"]])\n",
    "labels_test = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efc1aa-4431-48be-8992-8b0fa5b9d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hlas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290ff66-97b1-4402-bb74-f82192c68f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last dimension (1) since your model expects 2D inputs\n",
    "l_test = np.array(peptides_test).reshape(-1, 320)  # Shape: (103865, 320)\n",
    "r_test = np.array(hlas_test).reshape(-1, 180)      # Shape: (103865, 180)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Peptides shape:\", l_test.shape)  # Should be (103865, 320)\n",
    "print(\"HLAs shape:\", r_test.shape)      # Should be (103865, 180)\n",
    "\n",
    "# Now predict\n",
    "pred = siamese_net.predict([l_test, r_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e83676-b467-4977-a16b-87cfc30b7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot AUC for external test set\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_test, pred)\n",
    "# metrics.auc(fpr, tpr)\n",
    "auc = metrics.roc_auc_score(labels_test, pred)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c40325-3287-42b2-a75e-3b3972f37bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "# Compute precision, recall, and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, pred)\n",
    "\n",
    "# Calculate AUPR\n",
    "aupr = auc(recall, precision)\n",
    "print(aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d1653-9598-412c-93f3-b424472aaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 0.5 as a cut off for predictions of model\n",
    "for i in range(len(pred)):\n",
    "  if (pred[i]>0.5):\n",
    "    pred[i]=1\n",
    "  else:\n",
    "    pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe064e-b1e1-4f4e-8ce3-ccdb2b948a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the MCC of model\n",
    "metrics.matthews_corrcoef(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744ebba-b9b8-4e3e-a7a2-f17c4cf8030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb9604-3049-47aa-8709-1af87801fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report for external test set with esm\n",
    "print(classification_report(labels_test, pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc765d9-bf64-417d-a723-e30dbd5df259",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(labels_test, pred).ravel()\n",
    "\n",
    "# Calculate Sensitivity (Recall) and Specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9075b-86ca-4410-a336-c4c2a9faf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c1d88-c512-4ebf-a8f1-75c151ffb7ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#external / HLA\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load peptide embeddings for test set\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\external_set.csv\")\n",
    "\n",
    "# Preprocess HLA names (remove \"*\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "\n",
    "# Extract unique HLAs for evaluation\n",
    "new_hla_list = test[\"HLA\"].unique()\n",
    "\n",
    "# Iterate over each unique HLA in the test set\n",
    "for HLA in new_hla_list:\n",
    "    print(f\"\\nEvaluating HLA: {HLA}\")\n",
    "    \n",
    "    # Subset test data for the current HLA\n",
    "    test_subset = test[test[\"HLA\"] == HLA]\n",
    "\n",
    "    # Get peptide and HLA embeddings\n",
    "    peptides_test = np.array([peptide_embeddings_test[p] for p in test_subset[\"peptide\"]])\n",
    "    hlas_test = np.array([hla_df.loc[h].values for h in test_subset[\"HLA\"]])\n",
    "    labels_test = test_subset[\"label\"].values\n",
    "\n",
    "    # Reshape for model input\n",
    "    peptides_test = peptides_test.reshape(-1, 320)\n",
    "    hlas_test = hlas_test.reshape(-1, 180)\n",
    "\n",
    "    # Model prediction\n",
    "    pred_probs = siamese_net.predict([peptides_test, hlas_test])\n",
    "    auc = roc_auc_score(labels_test, pred_probs)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels_test, pred_labels)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    try:\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"], output_dict=True)\n",
    "        print(classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "        # Plot classification report heatmap\n",
    "        # sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.title(f\"Classification Report for {HLA}\")\n",
    "        # plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping HLA {HLA} due to error: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c0c51-f3e3-45c9-9d20-8a02f0ff8aed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# external / length\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load peptide embeddings for test set\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\external_set.csv\")\n",
    "\n",
    "# Preprocess HLA names (remove \"*\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "\n",
    "# Extract unique HLAs for evaluation\n",
    "length_list = test[\"length\"].unique()\n",
    "\n",
    "# Iterate over each unique HLA in the test set\n",
    "for length in length_list:\n",
    "    print(f\"\\nEvaluating length: {length}\")\n",
    "    \n",
    "    # Subset test data for the current HLA\n",
    "    test_subset = test[test[\"length\"] == length]\n",
    "\n",
    "    # Get peptide and HLA embeddings\n",
    "    peptides_test = np.array([peptide_embeddings_test[p] for p in test_subset[\"peptide\"]])\n",
    "    hlas_test = np.array([hla_df.loc[h].values for h in test_subset[\"HLA\"]])\n",
    "    labels_test = test_subset[\"label\"].values\n",
    "\n",
    "    # Reshape for model input\n",
    "    peptides_test = peptides_test.reshape(-1, 320)\n",
    "    hlas_test = hlas_test.reshape(-1, 180)\n",
    "\n",
    "    # Model prediction\n",
    "    pred_probs = siamese_net.predict([peptides_test, hlas_test])\n",
    "    auc = roc_auc_score(labels_test, pred_probs)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels_test, pred_labels)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    try:\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"], output_dict=True)\n",
    "        print(classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "        # Plot classification report heatmap\n",
    "        # sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.title(f\"Classification Report for {HLA}\")\n",
    "        # plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping HLA {HLA} due to error: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d86af-2c41-460d-bd48-fcd9876fe48d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# independent/length\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load peptide embeddings for test set\n",
    "with open(r\"C:\\Users\\asus\\Desktop\\revised things\\tests_ESM320_embeddings.pkl\", \"rb\") as f:\n",
    "    peptide_embeddings_test = pickle.load(f)\n",
    "\n",
    "# Load test dataset\n",
    "test = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\independent_set.csv\")\n",
    "\n",
    "# Preprocess HLA names (remove \"*\")\n",
    "test[\"HLA\"] = test[\"HLA\"].str.replace(\"*\", \"\", regex=False)\n",
    "\n",
    "# Extract unique HLAs for evaluation\n",
    "length_list = test[\"length\"].unique()\n",
    "\n",
    "# Iterate over each unique HLA in the test set\n",
    "for length in length_list:\n",
    "    print(f\"\\nEvaluating length: {length}\")\n",
    "    \n",
    "    # Subset test data for the current HLA\n",
    "    test_subset = test[test[\"length\"] == length]\n",
    "\n",
    "    # Get peptide and HLA embeddings\n",
    "    peptides_test = np.array([peptide_embeddings_test[p] for p in test_subset[\"peptide\"]])\n",
    "    hlas_test = np.array([hla_df.loc[h].values for h in test_subset[\"HLA\"]])\n",
    "    labels_test = test_subset[\"label\"].values\n",
    "\n",
    "    # Reshape for model input\n",
    "    peptides_test = peptides_test.reshape(-1, 320)\n",
    "    hlas_test = hlas_test.reshape(-1, 180)\n",
    "\n",
    "    # Model prediction\n",
    "    pred_probs = siamese_net.predict([peptides_test, hlas_test])\n",
    "    auc = roc_auc_score(labels_test, pred_probs)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions (Threshold = 0.5)\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels_test, pred_labels)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    try:\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"], output_dict=True)\n",
    "        print(classification_report(labels_test, pred_labels, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "        # Plot classification report heatmap\n",
    "        # sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.title(f\"Classification Report for {HLA}\")\n",
    "        # plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping HLA {HLA} due to error: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c47a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6da896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HLA IC feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_feature():\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    alldata0 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold0.csv\")\n",
    "    alldata1 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold1.csv\")\n",
    "    alldata2 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold2.csv\")\n",
    "    alldata3 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold3.csv\")\n",
    "    alldata4 = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\train_data_fold4.csv\")\n",
    "    # Correct concatenation syntax:\n",
    "    alldata = pd.concat([alldata0, alldata1, alldata2, alldata3, alldata4])\n",
    "    alldata['HLA'] = alldata['HLA'].str.replace('*', '')\n",
    "    # data_train, data_val = train_test_split(alldata, test_size=0.2, random_state=42)\n",
    "    # data_train.to_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\train_data.csv\")\n",
    "    # data_val.to_csv(r\"C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\validation_data.csv\")\n",
    "    \n",
    "    \n",
    "    HLA_list = alldata['HLA'].unique()\n",
    "    test_keys = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    test_values = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "    res = {}\n",
    "    for key in test_keys:\n",
    "        for value in test_values:\n",
    "            res[key] = value\n",
    "            test_values.remove(value)\n",
    "            break\n",
    "    column = [0]*181\n",
    "    column[0] = 'HLA'\n",
    "    for i in range(1, 181):\n",
    "      column[i] = (str(i+319))\n",
    "    \n",
    "    \n",
    "    antropy_data = pd.DataFrame(columns = column)\n",
    "    for HLA in tqdm(HLA_list):\n",
    "      data = alldata[alldata.HLA == HLA]\n",
    "      data_0 = data[data.label == 0]\n",
    "      data_1 = data[data.label == 1]\n",
    "\n",
    "      data_0 = data_0[data_0.length == 9]\n",
    "      data_1 = data_1[data_1.length == 9]\n",
    "\n",
    "      #character profile\n",
    "      arr0 = [[0 for _ in range(9)] for _ in range(len(data_0))]\n",
    "      j=0\n",
    "      for _, row in data_0.iterrows():\n",
    "        for i in range(9):\n",
    "          arr0[j][i] = row['peptide'][i]\n",
    "        j=j+1\n",
    "\n",
    "      arr1 = [[0 for _ in range(9)] for _ in range(len(data_1))]\n",
    "      j=0\n",
    "      for _, row in data_1.iterrows():\n",
    "        for i in range(9):\n",
    "          arr1[j][i] = row['peptide'][i]\n",
    "        j=j+1\n",
    "\n",
    "      arr_0 = [[pow(10, -10) for _ in range(9)] for _ in range(20)]\n",
    "      arr_1 = [[pow(10, -10) for _ in range(9)] for _ in range(20)]\n",
    "\n",
    "      #probability profile\n",
    "      for i in range(len(arr0)):\n",
    "        for j in range(9):\n",
    "          char = res[arr0[i][j]]\n",
    "          arr_0[char][j] = arr_0[char][j] + (1/len(data_0))\n",
    "\n",
    "      for i in range(len(arr1)):\n",
    "        for j in range(9):\n",
    "          char = res[arr1[i][j]]\n",
    "          arr_1[char][j] = arr_1[char][j] + (1/len(data_1))\n",
    "\n",
    "      #antropy matrix\n",
    "      antropy = [[0 for _ in range(9)] for _ in range(20)]\n",
    "\n",
    "      for i in range(20):\n",
    "        for j in range(9):\n",
    "          antropy[i][j] = arr_1[i][j]*(math.log((arr_1[i][j]/arr_0[i][j]),20))\n",
    "      \n",
    "      flattened_array = [element for column in zip(*antropy) for element in column]\n",
    "  \n",
    "      # flattened_array = [element for row in antropy for element in row]\n",
    "      feature_antropy = [0]*181\n",
    "      feature_antropy[0] = HLA\n",
    "      feature_antropy[1:] = flattened_array\n",
    "      new_df = pd.DataFrame(np.array(feature_antropy).reshape(1, -1), columns=column)\n",
    "      antropy_data = pd.concat([antropy_data, new_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    antropy_data.to_csv(r\"C:\\Users\\asus\\Desktop\\final_entropy_column.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa482684-2e44-4870-9f1a-8563ff2d9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IC_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biopython\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "def calculate_similarity(seq1, seq2, matrix):\n",
    "    alignments = pairwise2.align.globalds(seq1, seq2, matrix, -10, -0.5)\n",
    "    score = alignments[0][2]\n",
    "    max_score = min(len(seq1), len(seq2)) * matrix[('A', 'A')]\n",
    "    return score / max_score\n",
    "\n",
    "def calculate_percentage_similarities(sequences):\n",
    "    matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "    n = len(sequences)\n",
    "    similarity_matrix = [[0] * n for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = calculate_similarity(sequences[i], sequences[j], matrix)\n",
    "\n",
    "    percentages = []\n",
    "    for i in range(n):\n",
    "        total_similarity = sum(similarity_matrix[i])\n",
    "        percentages.append([similarity_matrix[i][j] / total_similarity for j in range(n)])\n",
    "\n",
    "    return percentages\n",
    "\n",
    "# Example usage\n",
    "header_hlaESM = [0]*180\n",
    "for i in range(1, 181):\n",
    "  header_hlaESM[i-1] = (str(i+319))\n",
    "\n",
    "new_column = [0]*181\n",
    "new_column[1:181] = header_hlaESM\n",
    "new_column[0] = 'HLA'\n",
    "antropy = pd.read_csv(r'C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv', names = new_column, header=0)\n",
    "unique_HLA = pd.read_csv(r'C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\unique_HLA.csv')\n",
    "antropy = pd.merge(antropy, unique_HLA, on=['HLA'])\n",
    "\n",
    "sequences = antropy['HLA_sequence'].tolist()\n",
    "percentages = calculate_percentage_similarities(sequences)\n",
    "temp = antropy.copy()\n",
    "antropy_data = pd.DataFrame(columns = new_column)\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    r=0\n",
    "    new_feature=[0]*181\n",
    "    new_feature[0] = sequences[i]\n",
    "    for j in range(320,500):\n",
    "        temp[str(j)] = antropy[str(j)]*percentages[i]\n",
    "        for z in range(len(sequences)):\n",
    "            try:\n",
    "                new_feature[j-319] = new_feature[j-319]+(float(temp[temp.HLA_sequence==sequences[z]][str(j)])/(len(sequences)-1))\n",
    "            except:\n",
    "                if r==0:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[0])/(len(sequences)-1))\n",
    "                    r=1\n",
    "                else:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[1])/(len(sequences)-1))\n",
    "                    \n",
    "\n",
    "    new_df = pd.DataFrame(np.array(new_feature).reshape(1, -1), columns=new_column)\n",
    "    antropy_data = antropy_data.append(new_df, ignore_index=True)\n",
    "    \n",
    "antropy_data['HLA']=antropy['HLA']\n",
    "antropy_data.to_csv(r\"C:\\Users\\asus\\Desktop\\Maryam\\esm320\\final_entropy_removed_HLA.csv\")\n",
    "\n",
    "    # for i, seq_percentages in enumerate(percentages):\n",
    "#     print(f\"Sequence {i+1} similarities: {seq_percentages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635f2a0-5b5f-41b5-a49c-c8f968177cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biopython\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "def calculate_similarity(seq1, seq2, matrix):\n",
    "    alignments = pairwise2.align.globalds(seq1, seq2, matrix, -10, -0.5)\n",
    "    score = alignments[0][2]\n",
    "    max_score = min(len(seq1), len(seq2)) * matrix[('A', 'A')]\n",
    "    return score / max_score\n",
    "\n",
    "def calculate_percentage_similarities(sequences):\n",
    "    matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "    n = len(sequences)\n",
    "    similarity_matrix = [[0] * n for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = calculate_similarity(sequences[i], sequences[j], matrix)\n",
    "\n",
    "    percentages = []\n",
    "    for i in range(n):\n",
    "        total_similarity = sum(similarity_matrix[i])\n",
    "        percentages.append([similarity_matrix[i][j] / total_similarity for j in range(n)])\n",
    "\n",
    "    return percentages\n",
    "\n",
    "header_hlaESM = [0]*180\n",
    "for i in range(1, 181):\n",
    "  header_hlaESM[i-1] = (str(i+319))\n",
    "\n",
    "new_column = [0]*181\n",
    "new_column[1:181] = header_hlaESM\n",
    "new_column[0] = 'HLA'\n",
    "antropy = pd.read_csv(r'C:\\Users\\asus\\Desktop\\master article\\Maryam\\esm320\\final_entropy.csv', names = new_column, header=0)\n",
    "\n",
    "sequences = antropy['HLA_sequence'].tolist() ## should added a newly HLA sequence to this list\n",
    "percentages = calculate_percentage_similarities(sequences)\n",
    "temp = antropy.copy()\n",
    "antropy_data = pd.DataFrame(columns = new_column)\n",
    "\n",
    "## and just calculate new_feature just for our new HLA and returne this feature for new HLA\n",
    "for i in tqdm(range(len(sequences))):\n",
    "    r=0\n",
    "    new_feature=[0]*181\n",
    "    new_feature[0] = sequences[i]\n",
    "    for j in range(320,500):\n",
    "        temp[str(j)] = antropy[str(j)]*percentages[i]\n",
    "        for z in range(len(sequences)):\n",
    "            try:\n",
    "                new_feature[j-319] = new_feature[j-319]+(float(temp[temp.HLA_sequence==sequences[z]][str(j)])/(len(sequences)-1))\n",
    "            except:\n",
    "                if r==0:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[0])/(len(sequences)-1))\n",
    "                    r=1\n",
    "                else:\n",
    "                    new_feature[j-319] = new_feature[j-319]+(float((temp[temp.HLA_sequence==sequences[z]][str(j)]).tolist()[1])/(len(sequences)-1))\n",
    "                    \n",
    "\n",
    "    new_df = pd.DataFrame(np.array(new_feature).reshape(1, -1), columns=new_column)\n",
    "    antropy_data = antropy_data.append(new_df, ignore_index=True)\n",
    "    \n",
    "antropy_data['HLA']=antropy['HLA']\n",
    "antropy_data.to_csv(r\"C:\\Users\\asus\\Desktop\\Maryam\\esm320\\final_entropy_removed_HLA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca1adc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ESM embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1068292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESM\n",
    "# !pip install fair-esm #if it's needed\n",
    "import esm # pip install fair-esm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def make_esm_representations(df,col, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #df = pd.read_excel(input_file_name)\n",
    "    peptides_list = df[col].tolist()\n",
    "    esm_model, alphabet = model\n",
    "    esm_model = esm_model.to(device)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    peptides_list = [(\"\", peptides_list[i]) for i in range(len(peptides_list))]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(peptides_list)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Extract per-residue representations (on CPU or CUDA)\n",
    "    results = esm_model(batch_tokens.to(device), repr_layers=[6], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "    sequence_representations = torch.stack(sequence_representations)\n",
    "    df = pd.DataFrame(sequence_representations.cpu().detach().numpy())\n",
    "    return df\n",
    "    #df.to_excel(output_file_name, index=False)\n",
    "    #print(f'{output_file_name} file made')\n",
    "\n",
    "def pretrained_model(dim):\n",
    "    assert dim in [5120,2560,1280,640,480,320]\n",
    "    if dim==5120:\n",
    "        return esm.pretrained.esm2_t48_15B_UR50D()\n",
    "    elif dim==2560:\n",
    "        return esm.pretrained.esm2_t36_3B_UR50D()\n",
    "    elif dim==1280:\n",
    "        return esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    elif dim==640:\n",
    "        return esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    elif dim==480:\n",
    "        return esm.pretrained.esm2_t12_35M_UR50D()\n",
    "    elif dim==320:\n",
    "        return esm.pretrained.esm2_t6_8M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fd18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     \"HLA\": ['HLA-A01:01', 'HLA-A02:01', 'HLA-A24:02', 'HLA-B08:01', 'HLA-B18:01'],\n",
    "#     \"peptide\": ['KVYLRVRPLL', 'KVYLRVRPLL', 'KVYLRVRPLL', 'KVYLRVRPLL', 'KVYLRVRPLL']\n",
    "# }\n",
    "my_set = pd.read_csv(r'C:\\Users\\asus\\Desktop\\revised things\\TransPHLA-AOMP-master\\TransPHLA-AOMP-master\\Dataset\\merged_neoantigen_data.csv')\n",
    "# my_test = pd.DataFrame(data)\n",
    "my_set['peptide'] = my_set['peptide'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284019a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample:\n",
    "feature = make_esm_representations(my_set, 'peptide', model=pretrained_model(320))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
